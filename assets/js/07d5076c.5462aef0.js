"use strict";(globalThis.webpackChunkphysical_humanoid_ai_book=globalThis.webpackChunkphysical_humanoid_ai_book||[]).push([[143],{519(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module4-vla/chapter2-voice-to-action","title":"Chapter 2: Voice-to-Action - Speech Recognition with OpenAI Whisper","description":"Learning Objectives","source":"@site/docs/module4-vla/chapter2-voice-to-action.md","sourceDirName":"module4-vla","slug":"/module4-vla/chapter2-voice-to-action","permalink":"/physical-humanoid-ai-book/docs/module4-vla/chapter2-voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/alizah-fatima/physical-humanoid-ai-book/tree/main/docs/module4-vla/chapter2-voice-to-action.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: Introduction to Vision-Language-Action Models","permalink":"/physical-humanoid-ai-book/docs/module4-vla/chapter1-vla-intro"},"next":{"title":"Chapter 3: Cognitive Planning - Natural Language to ROS 2 Actions","permalink":"/physical-humanoid-ai-book/docs/module4-vla/chapter3-cognitive-planning"}}');var t=i(4848),r=i(8453);const s={sidebar_position:2},a="Chapter 2: Voice-to-Action - Speech Recognition with OpenAI Whisper",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Speech Recognition for Robotics",id:"introduction-to-speech-recognition-for-robotics",level:2},{value:"The Role of Speech Recognition in VLA Systems",id:"the-role-of-speech-recognition-in-vla-systems",level:3},{value:"Why OpenAI Whisper for Robotics?",id:"why-openai-whisper-for-robotics",level:3},{value:"OpenAI Whisper Setup and Configuration",id:"openai-whisper-setup-and-configuration",level:2},{value:"Installation and Dependencies",id:"installation-and-dependencies",level:3},{value:"Basic Whisper Usage",id:"basic-whisper-usage",level:3},{value:"Whisper Configuration for Robotics",id:"whisper-configuration-for-robotics",level:3},{value:"Real-Time Speech Recognition for Robotics",id:"real-time-speech-recognition-for-robotics",level:2},{value:"Continuous Audio Processing",id:"continuous-audio-processing",level:3},{value:"Wake Word Detection",id:"wake-word-detection",level:3},{value:"Alternative Speech Recognition Systems",id:"alternative-speech-recognition-systems",level:2},{value:"Vosk Integration",id:"vosk-integration",level:3},{value:"Coqui STT Integration",id:"coqui-stt-integration",level:3},{value:"Audio Preprocessing for Robotics",id:"audio-preprocessing-for-robotics",level:2},{value:"Noise Reduction and Enhancement",id:"noise-reduction-and-enhancement",level:3},{value:"Real-Time Audio Processing Pipeline",id:"real-time-audio-processing-pipeline",level:3},{value:"ROS 2 Integration for Audio Processing",id:"ros-2-integration-for-audio-processing",level:2},{value:"Audio Message Processing",id:"audio-message-processing",level:3},{value:"Voice Command Processing Pipeline",id:"voice-command-processing-pipeline",level:2},{value:"Command Validation and Filtering",id:"command-validation-and-filtering",level:3},{value:"Command Queue and Processing Manager",id:"command-queue-and-processing-manager",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Model Optimization Techniques",id:"model-optimization-techniques",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"Audio Quality Issues",id:"audio-quality-issues",level:3},{value:"Common Error Solutions",id:"common-error-solutions",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-2-voice-to-action---speech-recognition-with-openai-whisper",children:"Chapter 2: Voice-to-Action - Speech Recognition with OpenAI Whisper"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement speech recognition systems using OpenAI Whisper"}),"\n",(0,t.jsx)(n.li,{children:"Configure and optimize Whisper models for robotics applications"}),"\n",(0,t.jsx)(n.li,{children:"Process voice commands in real-time for robot control"}),"\n",(0,t.jsx)(n.li,{children:"Integrate speech recognition with ROS 2 audio systems"}),"\n",(0,t.jsx)(n.li,{children:"Implement alternative speech recognition systems (Vosk, Coqui STT)"}),"\n",(0,t.jsx)(n.li,{children:"Design robust voice command processing pipelines"}),"\n",(0,t.jsx)(n.li,{children:"Apply audio preprocessing techniques for improved recognition"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-speech-recognition-for-robotics",children:"Introduction to Speech Recognition for Robotics"}),"\n",(0,t.jsx)(n.h3,{id:"the-role-of-speech-recognition-in-vla-systems",children:"The Role of Speech Recognition in VLA Systems"}),"\n",(0,t.jsx)(n.p,{children:"Speech recognition serves as the primary interface between human operators and VLA-enabled robots. It enables natural, hands-free interaction that feels intuitive to users. In robotics applications, speech recognition faces unique challenges:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Environmental Noise"}),": Robots operate in various acoustic environments"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time Processing"}),": Commands need to be processed quickly for responsive interaction"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Domain Specificity"}),": Robot commands follow specific patterns and vocabularies"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safety Criticality"}),": Misunderstood commands could lead to unsafe robot behavior"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"why-openai-whisper-for-robotics",children:"Why OpenAI Whisper for Robotics?"}),"\n",(0,t.jsx)(n.p,{children:"OpenAI Whisper has emerged as a leading choice for robotics applications due to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multilingual Support"}),": Works with 99+ languages out of the box"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robustness"}),": Performs well in noisy environments"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Open Source"}),": Freely available and modifiable"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"High Accuracy"}),": State-of-the-art performance across domains"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Flexibility"}),": Available in multiple model sizes for different hardware"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"openai-whisper-setup-and-configuration",children:"OpenAI Whisper Setup and Configuration"}),"\n",(0,t.jsx)(n.h3,{id:"installation-and-dependencies",children:"Installation and Dependencies"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Install Whisper with pip\npip install openai-whisper\n\n# On Ubuntu, install additional dependencies\nsudo apt update\nsudo apt install ffmpeg python3-pyaudio\n\n# Install additional audio processing libraries\npip install soundfile librosa pyaudio\n"})}),"\n",(0,t.jsx)(n.h3,{id:"basic-whisper-usage",children:"Basic Whisper Usage"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import whisper\nimport torch\n\n# Check if CUDA is available for GPU acceleration\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\nprint(f"Using device: {device}")\n\n# Load different model sizes based on your needs:\n# - "tiny": Fastest, least accurate (75MB)\n# - "base": Good balance (145MB)\n# - "small": Better accuracy (484MB)\n# - "medium": High quality (1.5GB)\n# - "large": Highest quality (3.0GB)\nmodel = whisper.load_model("base").to(device)\n\n# Load audio file\naudio_path = "path/to/your/audio.wav"\naudio = whisper.load_audio(audio_path)\naudio = whisper.pad_or_trim(audio)\n\n# Convert to log-Mel spectrogram\nmel = whisper.log_mel_spectrogram(audio).to(model.device)\n\n# Detect language (optional)\n_, probs = model.detect_language(mel)\ndetected_language = max(probs, key=probs.get)\nprint(f"Detected language: {detected_language}")\n\n# Decode audio\noptions = whisper.DecodingOptions(fp16=torch.cuda.is_available())\nresult = whisper.decode(model, mel, options)\n\nprint(f"Transcribed text: {result.text}")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"whisper-configuration-for-robotics",children:"Whisper Configuration for Robotics"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Advanced Whisper configuration for robotics applications\nimport whisper\nimport numpy as np\n\nclass WhisperRobotConfig:\n    def __init__(self):\n        # Model configuration\n        self.model_size = "base"  # Balance between speed and accuracy\n        self.device = "cuda" if torch.cuda.is_available() else "cpu"\n\n        # Audio processing parameters\n        self.sample_rate = 16000  # Standard for speech recognition\n        self.audio_chunk_size = 16000  # 1 second of audio at 16kHz\n\n        # Recognition parameters\n        self.language = "en"  # Set to specific language for better accuracy\n        self.temperature = 0.0  # Deterministic decoding\n        self.best_of = 5  # Number of options to generate\n        self.beam_size = 5  # Beam search size\n        self.patience = 1.0  # Patience factor for beam search\n\n        # Thresholds and filters\n        self.confidence_threshold = 0.7  # Minimum confidence for acceptance\n        self.min_silence_duration = 0.5  # Minimum silence before processing\n        self.max_audio_duration = 10.0  # Maximum audio duration to process\n\nclass WhisperRobotInterface:\n    def __init__(self, config: WhisperRobotConfig):\n        self.config = config\n        self.model = whisper.load_model(config.model_size).to(config.device)\n        self.is_initialized = False\n\n        # Initialize audio processing components\n        self.audio_buffer = np.array([], dtype=np.float32)\n        self.last_transcription = ""\n        self.confidence_score = 0.0\n\n        self.is_initialized = True\n        print(f"Whisper Robot Interface initialized on {config.device}")\n\n    def preprocess_audio(self, audio_data: np.ndarray) -> np.ndarray:\n        """Preprocess audio data for Whisper"""\n        # Normalize audio to [-1, 1] range\n        audio_data = audio_data.astype(np.float32)\n        if audio_data.max() != 0:\n            audio_data = audio_data / np.abs(audio_data).max()\n\n        # Pad or trim to appropriate length\n        target_length = self.config.audio_chunk_size\n        if len(audio_data) < target_length:\n            # Pad with zeros\n            audio_data = np.pad(audio_data, (0, target_length - len(audio_data)), \'constant\')\n        elif len(audio_data) > target_length:\n            # Trim to target length\n            audio_data = audio_data[:target_length]\n\n        return audio_data\n\n    def transcribe_audio(self, audio_data: np.ndarray) -> tuple[str, float]:\n        """Transcribe audio using Whisper with confidence scoring"""\n        try:\n            # Preprocess audio\n            processed_audio = self.preprocess_audio(audio_data)\n\n            # Convert to tensor and move to device\n            audio_tensor = torch.from_numpy(processed_audio).to(self.config.device)\n\n            # Pad or trim audio to required length\n            audio_tensor = whisper.pad_or_trim(audio_tensor)\n\n            # Compute log-Mel spectrogram\n            mel = whisper.log_mel_spectrogram(audio_tensor).to(self.config.device)\n\n            # Decode with specific options for robotics\n            options = whisper.DecodingOptions(\n                language=self.config.language,\n                temperature=self.config.temperature,\n                best_of=self.config.best_of,\n                beam_size=self.config.beam_size,\n                patience=self.config.patience,\n                fp16=torch.cuda.is_available() if self.config.device == "cuda" else False\n            )\n\n            result = whisper.decode(self.model, mel, options)\n\n            # Calculate confidence (simplified approach)\n            # In practice, you might use more sophisticated confidence measures\n            confidence = self.calculate_confidence(result)\n\n            return result.text.strip(), confidence\n\n        except Exception as e:\n            print(f"Error in transcription: {e}")\n            return "", 0.0\n\n    def calculate_confidence(self, result) -> float:\n        """Calculate confidence score for transcription"""\n        # This is a simplified confidence calculation\n        # In practice, you might use log probabilities or other metrics\n        if hasattr(result, \'avg_logprob\') and result.avg_logprob is not None:\n            # Convert log probability to confidence (simple normalization)\n            # Higher logprob = more confidence\n            logprob = result.avg_logprob\n            # Normalize logprob to 0-1 range (empirical scaling)\n            confidence = max(0.0, min(1.0, (logprob + 2.0) / 2.0))\n            return confidence\n        else:\n            # Fallback confidence calculation\n            return 0.5  # Neutral confidence\n\n    def is_command_valid(self, text: str, confidence: float) -> bool:\n        """Validate if transcription is a valid command"""\n        if not text.strip():\n            return False\n\n        if confidence < self.config.confidence_threshold:\n            return False\n\n        # Additional validation could include:\n        # - Checking if text contains robot command keywords\n        # - Verifying text length is reasonable\n        # - Checking for profanity or inappropriate content\n\n        return True\n'})}),"\n",(0,t.jsx)(n.h2,{id:"real-time-speech-recognition-for-robotics",children:"Real-Time Speech Recognition for Robotics"}),"\n",(0,t.jsx)(n.h3,{id:"continuous-audio-processing",children:"Continuous Audio Processing"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import pyaudio\nimport threading\nimport queue\nimport time\nfrom dataclasses import dataclass\nfrom typing import Callable, Optional\n\n@dataclass\nclass SpeechRecognitionResult:\n    """Result from speech recognition"""\n    text: str\n    confidence: float\n    is_valid: bool\n    timestamp: float\n    processing_time: float\n\nclass RealTimeWhisperRecognizer:\n    def __init__(self, whisper_interface: WhisperRobotInterface):\n        self.whisper_interface = whisper_interface\n        self.audio_queue = queue.Queue()\n        self.result_queue = queue.Queue()\n\n        # Audio stream parameters\n        self.chunk_size = 1024  # Frames per buffer\n        self.sample_rate = 16000\n        self.channels = 1\n        self.format = pyaudio.paInt16\n\n        # Processing state\n        self.is_listening = False\n        self.audio_thread = None\n        self.processing_thread = None\n\n        # Callback for processing results\n        self.result_callback: Optional[Callable[[SpeechRecognitionResult], None]] = None\n\n    def start_listening(self, result_callback: Callable[[SpeechRecognitionResult], None] = None):\n        """Start real-time listening"""\n        self.result_callback = result_callback\n        self.is_listening = True\n\n        # Start audio recording thread\n        self.audio_thread = threading.Thread(target=self._record_audio, daemon=True)\n        self.audio_thread.start()\n\n        # Start processing thread\n        self.processing_thread = threading.Thread(target=self._process_audio, daemon=True)\n        self.processing_thread.start()\n\n        print("Started real-time listening")\n\n    def stop_listening(self):\n        """Stop real-time listening"""\n        self.is_listening = False\n        if self.audio_thread:\n            self.audio_thread.join(timeout=1.0)\n        if self.processing_thread:\n            self.processing_thread.join(timeout=1.0)\n        print("Stopped real-time listening")\n\n    def _record_audio(self):\n        """Record audio in a separate thread"""\n        p = pyaudio.PyAudio()\n\n        stream = p.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.sample_rate,\n            input=True,\n            frames_per_buffer=self.chunk_size\n        )\n\n        print("Recording started...")\n\n        while self.is_listening:\n            try:\n                data = stream.read(self.chunk_size, exception_on_overflow=False)\n\n                # Convert to numpy array\n                audio_data = np.frombuffer(data, dtype=np.int16).astype(np.float32)\n                audio_data = audio_data / 32768.0  # Normalize to [-1, 1]\n\n                # Add to processing queue\n                self.audio_queue.put(audio_data)\n\n            except Exception as e:\n                print(f"Error recording audio: {e}")\n                break\n\n        stream.stop_stream()\n        stream.close()\n        p.terminate()\n\n    def _process_audio(self):\n        """Process audio chunks in a separate thread"""\n        audio_buffer = np.array([], dtype=np.float32)\n        buffer_duration = 0.0\n        min_speech_duration = 1.0  # Minimum speech duration to process\n        silence_threshold = 0.01  # Threshold for detecting silence\n\n        while self.is_listening:\n            try:\n                # Get audio data from queue\n                audio_chunk = self.audio_queue.get(timeout=0.1)\n\n                # Add to buffer\n                audio_buffer = np.concatenate([audio_buffer, audio_chunk])\n                buffer_duration = len(audio_buffer) / self.sample_rate\n\n                # Check if buffer has enough data and contains speech\n                if buffer_duration >= min_speech_duration:\n                    # Check if this is likely speech (not just silence)\n                    if np.max(np.abs(audio_buffer)) > silence_threshold:\n                        # Process this chunk\n                        start_time = time.time()\n\n                        # Transcribe audio\n                        text, confidence = self.whisper_interface.transcribe_audio(audio_buffer)\n\n                        # Check if it\'s a valid command\n                        is_valid = self.whisper_interface.is_command_valid(text, confidence)\n\n                        # Create result\n                        result = SpeechRecognitionResult(\n                            text=text,\n                            confidence=confidence,\n                            is_valid=is_valid,\n                            timestamp=time.time(),\n                            processing_time=time.time() - start_time\n                        )\n\n                        # Call result callback if provided\n                        if self.result_callback:\n                            self.result_callback(result)\n\n                    # Clear buffer after processing\n                    audio_buffer = np.array([], dtype=np.float32)\n                    buffer_duration = 0.0\n                else:\n                    # Continue accumulating audio\n                    continue\n\n            except queue.Empty:\n                # No audio data available, continue\n                continue\n            except Exception as e:\n                print(f"Error processing audio: {e}")\n                break\n'})}),"\n",(0,t.jsx)(n.h3,{id:"wake-word-detection",children:"Wake Word Detection"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import speech_recognition as sr\nimport threading\n\nclass WakeWordDetector:\n    def __init__(self, wake_word="robot"):\n        self.wake_word = wake_word.lower()\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        self.is_awake = False\n        self.wake_callback = None\n\n        # Adjust for ambient noise\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n    def set_wake_callback(self, callback: Callable[[], None]):\n        """Set callback for when wake word is detected"""\n        self.wake_callback = callback\n\n    def start_wake_word_detection(self):\n        """Start wake word detection in background"""\n        def detect_wake_word():\n            while True:\n                try:\n                    with self.microphone as source:\n                        # Listen for audio with timeout\n                        audio = self.recognizer.listen(source, timeout=1.0, phrase_time_limit=2.0)\n\n                    # Recognize speech\n                    text = self.recognizer.recognize_google(audio).lower()\n\n                    # Check for wake word\n                    if self.wake_word in text:\n                        print(f"Wake word \'{self.wake_word}\' detected!")\n                        self.is_awake = True\n\n                        # Call wake callback if set\n                        if self.wake_callback:\n                            self.wake_callback()\n\n                        # Reset awake state after a delay\n                        threading.Timer(5.0, self._reset_awake_state).start()\n\n                except sr.WaitTimeoutError:\n                    # No speech detected, continue listening\n                    continue\n                except sr.UnknownValueError:\n                    # Could not understand audio, continue\n                    continue\n                except sr.RequestError:\n                    # Error with speech recognition service\n                    print("Speech recognition service error")\n                    time.sleep(1)\n                    continue\n                except Exception as e:\n                    print(f"Error in wake word detection: {e}")\n                    time.sleep(1)\n                    continue\n\n        # Start wake word detection in background thread\n        thread = threading.Thread(target=detect_wake_word, daemon=True)\n        thread.start()\n\n    def _reset_awake_state(self):\n        """Reset awake state after timeout"""\n        self.is_awake = False\n'})}),"\n",(0,t.jsx)(n.h2,{id:"alternative-speech-recognition-systems",children:"Alternative Speech Recognition Systems"}),"\n",(0,t.jsx)(n.h3,{id:"vosk-integration",children:"Vosk Integration"}),"\n",(0,t.jsx)(n.p,{children:"Vosk is an excellent alternative for robotics applications due to its lightweight nature and offline capabilities:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Install Vosk: pip install vosk\nfrom vosk import Model, KaldiRecognizer\nimport json\nimport pyaudio\n\nclass VoskRecognizer:\n    def __init__(self, model_path: str = "model"):\n        """\n        Initialize Vosk recognizer\n        Download model from: https://alphacephei.com/vosk/models\n        """\n        self.model = Model(model_path)\n        self.recognizer = KaldiRecognizer(self.model, 16000)\n        self.is_initialized = True\n\n        # Audio stream parameters\n        self.chunk_size = 8192\n        self.sample_rate = 16000\n        self.channels = 1\n        self.format = pyaudio.paInt16\n\n    def recognize_continuous(self, audio_callback: Callable[[str], None]):\n        """Continuous speech recognition"""\n        p = pyaudio.PyAudio()\n\n        stream = p.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.sample_rate,\n            input=True,\n            frames_per_buffer=self.chunk_size\n        )\n\n        print("Vosk listening... Press Ctrl+C to stop")\n\n        try:\n            while True:\n                data = stream.read(self.chunk_size)\n                if len(data) == 0:\n                    break\n\n                if self.recognizer.AcceptWaveform(data):\n                    result = self.recognizer.Result()\n                    result_dict = json.loads(result)\n\n                    if \'text\' in result_dict and result_dict[\'text\']:\n                        recognized_text = result_dict[\'text\']\n                        audio_callback(recognized_text)\n\n        except KeyboardInterrupt:\n            print("\\nStopping Vosk recognition...")\n        finally:\n            stream.stop_stream()\n            stream.close()\n            p.terminate()\n\n    def recognize_single_phrase(self, timeout: float = 5.0) -> str:\n        """Recognize a single phrase with timeout"""\n        p = pyaudio.PyAudio()\n\n        stream = p.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.sample_rate,\n            input=True,\n            frames_per_buffer=self.chunk_size\n        )\n\n        start_time = time.time()\n        result_text = ""\n\n        try:\n            while time.time() - start_time < timeout:\n                data = stream.read(self.chunk_size)\n\n                if self.recognizer.AcceptWaveform(data):\n                    result = self.recognizer.Result()\n                    result_dict = json.loads(result)\n\n                    if \'text\' in result_dict and result_dict[\'text\']:\n                        result_text = result_dict[\'text\']\n                        break\n\n        except Exception as e:\n            print(f"Error in Vosk recognition: {e}")\n        finally:\n            stream.stop_stream()\n            stream.close()\n            p.terminate()\n\n        return result_text\n'})}),"\n",(0,t.jsx)(n.h3,{id:"coqui-stt-integration",children:"Coqui STT Integration"}),"\n",(0,t.jsx)(n.p,{children:"Coqui STT (formerly Mozilla DeepSpeech) offers another open-source option:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Install Coqui STT: pip install coqui-stt\nfrom stt import Model\nimport numpy as np\nimport wave\nimport subprocess\n\nclass CoquiRecognizer:\n    def __init__(self, model_path: str, scorer_path: str = None):\n        """\n        Initialize Coqui STT recognizer\n        Download model from: https://coqui.ai/models\n        """\n        self.model = Model(model_path)\n\n        if scorer_path:\n            self.model.enableExternalScorer(scorer_path)\n\n        # Set beam width for better accuracy\n        self.model.setBeamWidth(1024)\n\n        self.is_initialized = True\n\n    def transcribe_audio_file(self, audio_file_path: str) -> str:\n        """Transcribe audio file using Coqui STT"""\n        # Load audio file\n        fin = wave.open(audio_file_path, \'rb\')\n        audio = np.frombuffer(fin.readframes(fin.getnframes()), np.int16)\n        fin.close()\n\n        # Transcribe\n        text = self.model.stt(audio)\n        return text\n\n    def transcribe_audio_data(self, audio_data: np.ndarray) -> str:\n        """Transcribe audio data (numpy array)"""\n        # Ensure audio data is in the right format (16-bit integers)\n        if audio_data.dtype != np.int16:\n            # Convert to 16-bit if needed\n            audio_data = (audio_data * 32767).astype(np.int16)\n\n        # Transcribe\n        text = self.model.stt(audio_data)\n        return text\n\n    def transcribe_stream(self, audio_generator):\n        """Transcribe audio in streaming mode"""\n        # Initialize streaming\n        stream = self.model.createStream()\n\n        for audio_chunk in audio_generator:\n            # Feed audio chunk to stream\n            stream.feedAudioContent(audio_chunk)\n\n        # Finish and get result\n        text = stream.finishStream()\n        return text\n'})}),"\n",(0,t.jsx)(n.h2,{id:"audio-preprocessing-for-robotics",children:"Audio Preprocessing for Robotics"}),"\n",(0,t.jsx)(n.h3,{id:"noise-reduction-and-enhancement",children:"Noise Reduction and Enhancement"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import librosa\nimport numpy as np\nfrom scipy import signal\n\nclass AudioPreprocessor:\n    def __init__(self):\n        self.sample_rate = 16000\n        self.frame_length = 2048\n        self.hop_length = 512\n\n    def denoise_audio(self, audio_data: np.ndarray) -> np.ndarray:\n        """Reduce noise in audio signal"""\n        # Apply spectral gating for noise reduction\n        # This is a simplified approach - more sophisticated methods exist\n\n        # Compute STFT\n        stft = librosa.stft(audio_data, n_fft=self.frame_length, hop_length=self.hop_length)\n\n        # Compute magnitude and phase\n        magnitude = np.abs(stft)\n        phase = np.angle(stft)\n\n        # Estimate noise profile (using first portion of audio as noise reference)\n        noise_profile = np.mean(magnitude[:, :50], axis=1, keepdims=True)  # First 50 frames\n\n        # Apply spectral subtraction\n        enhanced_magnitude = np.maximum(magnitude - noise_profile * 0.3, 0)  # 30% noise reduction\n\n        # Reconstruct signal\n        enhanced_stft = enhanced_magnitude * np.exp(1j * phase)\n        enhanced_audio = librosa.istft(enhanced_stft, hop_length=self.hop_length)\n\n        return enhanced_audio.astype(np.float32)\n\n    def enhance_speech(self, audio_data: np.ndarray) -> np.ndarray:\n        """Enhance speech signal using various techniques"""\n        # Apply pre-emphasis filter\n        enhanced = self.pre_emphasis_filter(audio_data)\n\n        # Normalize amplitude\n        enhanced = self.normalize_audio(enhanced)\n\n        # Apply noise reduction\n        enhanced = self.denoise_audio(enhanced)\n\n        return enhanced\n\n    def pre_emphasis_filter(self, audio_data: np.ndarray, coeff: float = 0.97) -> np.ndarray:\n        """Apply pre-emphasis filter to enhance high frequencies"""\n        return np.append(audio_data[0], audio_data[1:] - coeff * audio_data[:-1])\n\n    def normalize_audio(self, audio_data: np.ndarray) -> np.ndarray:\n        """Normalize audio to unit amplitude"""\n        max_val = np.max(np.abs(audio_data))\n        if max_val > 0:\n            return audio_data / max_val\n        return audio_data\n\n    def vad_simple(self, audio_data: np.ndarray, threshold_db: float = -30) -> np.ndarray:\n        """Simple voice activity detection"""\n        # Convert to dB\n        rms = np.sqrt(np.mean(audio_data**2, axis=0))\n        db = 20 * np.log10(rms + 1e-10)  # Add small value to avoid log(0)\n\n        # Create mask for speech regions\n        speech_mask = db > threshold_db\n\n        # Apply mask to audio\n        return audio_data * speech_mask\n\n    def bandpass_filter(self, audio_data: np.ndarray, low_freq: float = 300, high_freq: float = 3400) -> np.ndarray:\n        """Apply bandpass filter to focus on speech frequencies"""\n        nyquist = self.sample_rate / 2\n        low = low_freq / nyquist\n        high = high_freq / nyquist\n\n        # Design Butterworth bandpass filter\n        b, a = signal.butter(4, [low, high], btype=\'band\', analog=False)\n\n        # Apply filter\n        filtered_audio = signal.filtfilt(b, a, audio_data)\n\n        return filtered_audio.astype(np.float32)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"real-time-audio-processing-pipeline",children:"Real-Time Audio Processing Pipeline"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class RealTimeAudioProcessor:\n    def __init__(self):\n        self.preprocessor = AudioPreprocessor()\n        self.buffer_size = 4096  # Samples\n        self.overlap_ratio = 0.5  # 50% overlap\n        self.min_speech_energy = 0.001  # Minimum energy threshold\n\n        # Circular buffer for real-time processing\n        self.audio_buffer = np.zeros(self.buffer_size * 2, dtype=np.float32)\n        self.buffer_fill_level = 0\n\n    def process_audio_chunk(self, audio_chunk: np.ndarray) -> np.ndarray:\n        """Process a chunk of audio in real-time"""\n        # Add new audio to buffer\n        chunk_size = len(audio_chunk)\n\n        if self.buffer_fill_level + chunk_size > len(self.audio_buffer):\n            # Shift buffer to make room\n            shift_amount = self.buffer_fill_level + chunk_size - len(self.audio_buffer)\n            self.audio_buffer[:-shift_amount] = self.audio_buffer[shift_amount:]\n            self.buffer_fill_level -= shift_amount\n\n        # Add new chunk to buffer\n        self.audio_buffer[self.buffer_fill_level:self.buffer_fill_level + chunk_size] = audio_chunk\n        self.buffer_fill_level += chunk_size\n\n        # Process if we have enough audio\n        if self.buffer_fill_level >= self.buffer_size:\n            # Process the oldest complete buffer\n            processing_buffer = self.audio_buffer[:self.buffer_size].copy()\n\n            # Apply preprocessing\n            processed_audio = self.preprocessor.enhance_speech(processing_buffer)\n\n            # Remove processed portion from buffer\n            self.audio_buffer[:-self.buffer_size] = self.audio_buffer[self.buffer_size:]\n            self.buffer_fill_level -= self.buffer_size\n\n            return processed_audio\n\n        return np.array([])  # Not enough audio to process yet\n\n    def detect_speech_activity(self, audio_data: np.ndarray) -> bool:\n        """Detect if speech is present in audio"""\n        # Calculate energy\n        energy = np.mean(audio_data**2)\n\n        # Check if energy exceeds threshold\n        return energy > self.min_speech_energy\n'})}),"\n",(0,t.jsx)(n.h2,{id:"ros-2-integration-for-audio-processing",children:"ROS 2 Integration for Audio Processing"}),"\n",(0,t.jsx)(n.h3,{id:"audio-message-processing",children:"Audio Message Processing"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# ROS 2 node for speech recognition integration\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import AudioData\nfrom audio_common_msgs.msg import AudioData as AudioDataMsg\nimport numpy as np\nfrom typing import Optional\n\nclass SpeechRecognitionROSNode(Node):\n    def __init__(self):\n        super().__init__(\'speech_recognition_node\')\n\n        # Initialize Whisper interface\n        config = WhisperRobotConfig()\n        self.whisper_interface = WhisperRobotInterface(config)\n\n        # Initialize audio preprocessor\n        self.audio_preprocessor = RealTimeAudioProcessor()\n\n        # Initialize wake word detector\n        self.wake_word_detector = WakeWordDetector(wake_word="hey robot")\n\n        # ROS 2 publishers and subscribers\n        self.audio_sub = self.create_subscription(\n            AudioData, \'/audio_input\', self.audio_callback, 10)\n\n        self.command_pub = self.create_publisher(\n            String, \'/robot_voice_commands\', 10)\n\n        self.transcription_pub = self.create_publisher(\n            String, \'/transcriptions\', 10)\n\n        # State variables\n        self.is_listening_for_command = False\n        self.command_timeout = self.create_timer(5.0, self.command_timeout_callback)\n\n        # Wake word detection\n        self.wake_word_detector.set_wake_callback(self.wake_detected_callback)\n        self.wake_word_detector.start_wake_word_detection()\n\n        self.get_logger().info(\'Speech Recognition ROS Node initialized\')\n\n    def audio_callback(self, msg: AudioData):\n        """Process incoming audio messages"""\n        try:\n            # Convert ROS AudioData to numpy array\n            audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32)\n            audio_data = audio_data / 32768.0  # Normalize to [-1, 1]\n\n            # Preprocess audio\n            processed_audio = self.audio_preprocessor.process_audio_chunk(audio_data)\n\n            if len(processed_audio) > 0:\n                # Check for speech activity\n                has_speech = self.audio_preprocessor.detect_speech_activity(processed_audio)\n\n                if has_speech:\n                    if self.is_listening_for_command:\n                        # Transcribe the audio\n                        text, confidence = self.whisper_interface.transcribe_audio(processed_audio)\n\n                        if text and confidence > 0.7:\n                            # Publish transcription\n                            transcription_msg = String()\n                            transcription_msg.data = text\n                            self.transcription_pub.publish(transcription_msg)\n\n                            # Publish as robot command\n                            command_msg = String()\n                            command_msg.data = text\n                            self.command_pub.publish(command_msg)\n\n                            self.get_logger().info(f\'Transcribed: "{text}" (conf: {confidence:.2f})\')\n\n                            # Reset listening state\n                            self.is_listening_for_command = False\n                        else:\n                            self.get_logger().info(f\'Low confidence transcription: "{text}" (conf: {confidence:.2f})\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing audio: {e}\')\n\n    def wake_detected_callback(self):\n        """Callback when wake word is detected"""\n        self.get_logger().info(\'Wake word detected - ready for command\')\n        self.is_listening_for_command = True\n\n        # Reset command timeout\n        self.reset_command_timeout()\n\n    def command_timeout_callback(self):\n        """Timer callback for command timeout"""\n        if self.is_listening_for_command:\n            self.get_logger().info(\'Command timeout - stopped listening\')\n            self.is_listening_for_command = False\n\n    def reset_command_timeout(self):\n        """Reset the command timeout timer"""\n        # Cancel current timer and start new one\n        self.command_timeout.reset()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    speech_node = SpeechRecognitionROSNode()\n\n    try:\n        rclpy.spin(speech_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        speech_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"voice-command-processing-pipeline",children:"Voice Command Processing Pipeline"}),"\n",(0,t.jsx)(n.h3,{id:"command-validation-and-filtering",children:"Command Validation and Filtering"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import re\nfrom typing import List, Dict, Tuple\n\nclass VoiceCommandValidator:\n    def __init__(self):\n        # Define valid command patterns\n        self.valid_patterns = [\n            # Navigation commands\n            r'go to (the )?\\w+',\n            r'move to (the )?\\w+',\n            r'go (straight|left|right)',\n            r'turn (left|right)',\n\n            # Manipulation commands\n            r'(pick|grasp|take) (the )?\\w+',\n            r'(place|put) (the )?\\w+',\n            r'pick up (the )?\\w+',\n\n            # Interaction commands\n            r'greet (the )?\\w+',\n            r'say hello to (the )?\\w+',\n            r'tell (the )?\\w+',\n\n            # Query commands\n            r'where is (the )?\\w+',\n            r'find (the )?\\w+',\n            r'look for (the )?\\w+'\n        ]\n\n        # Define prohibited content patterns\n        self.prohibited_patterns = [\n            r'\\b(hate|kill|destroy|break|attack)\\b',\n            r'\\b(inappropriate|offensive|profanity)\\b'\n        ]\n\n        # Confidence thresholds\n        self.min_confidence = 0.6\n        self.command_length_min = 2\n        self.command_length_max = 50\n\n    def validate_command(self, text: str, confidence: float) -> Tuple[bool, str]:\n        \"\"\"Validate voice command\"\"\"\n        if not text or not text.strip():\n            return False, \"Empty command\"\n\n        if confidence < self.min_confidence:\n            return False, f\"Low confidence: {confidence:.2f}\"\n\n        text_clean = text.strip().lower()\n\n        if len(text_clean) < self.command_length_min:\n            return False, \"Command too short\"\n\n        if len(text_clean) > self.command_length_max:\n            return False, \"Command too long\"\n\n        # Check for prohibited content\n        for pattern in self.prohibited_patterns:\n            if re.search(pattern, text_clean):\n                return False, \"Command contains prohibited content\"\n\n        # Check if command matches valid patterns\n        is_valid = any(re.match(pattern, text_clean) for pattern in self.valid_patterns)\n\n        if not is_valid:\n            return False, \"Command doesn't match expected patterns\"\n\n        return True, \"Valid command\"\n\n    def preprocess_command(self, text: str) -> str:\n        \"\"\"Preprocess command text\"\"\"\n        # Remove punctuation and normalize\n        text = re.sub(r'[^\\w\\s]', ' ', text)\n        text = ' '.join(text.split())  # Remove extra whitespace\n        text = text.strip()\n\n        return text\n"})}),"\n",(0,t.jsx)(n.h3,{id:"command-queue-and-processing-manager",children:"Command Queue and Processing Manager"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport queue\nfrom dataclasses import dataclass\nfrom typing import Optional, Callable\nimport time\n\n@dataclass\nclass VoiceCommand:\n    """Represents a voice command with metadata"""\n    text: str\n    confidence: float\n    timestamp: float\n    source: str  # \'voice\', \'text\', etc.\n    priority: int = 0  # Higher number = higher priority\n\nclass VoiceCommandManager:\n    def __init__(self, max_queue_size: int = 10):\n        self.command_queue = asyncio.Queue(maxsize=max_queue_size)\n        self.processed_commands = queue.Queue(maxsize=100)  # For history\n        self.is_processing = False\n        self.validator = VoiceCommandValidator()\n\n        # Callbacks\n        self.command_processed_callback: Optional[Callable[[VoiceCommand], None]] = None\n        self.command_rejected_callback: Optional[Callable[[str, str], None]] = None\n\n    async def add_command(self, text: str, confidence: float, source: str = \'voice\') -> bool:\n        """Add a command to the processing queue"""\n        try:\n            # Validate command\n            is_valid, reason = self.validator.validate_command(text, confidence)\n\n            if not is_valid:\n                if self.command_rejected_callback:\n                    self.command_rejected_callback(text, reason)\n                return False\n\n            # Preprocess command\n            processed_text = self.validator.preprocess_command(text)\n\n            # Create command object\n            command = VoiceCommand(\n                text=processed_text,\n                confidence=confidence,\n                timestamp=time.time(),\n                source=source\n            )\n\n            # Add to queue\n            await self.command_queue.put(command)\n            return True\n\n        except asyncio.QueueFull:\n            print("Command queue is full")\n            return False\n\n    async def start_processing(self):\n        """Start processing commands from the queue"""\n        self.is_processing = True\n\n        while self.is_processing:\n            try:\n                # Get command from queue (with timeout)\n                command = await asyncio.wait_for(self.command_queue.get(), timeout=1.0)\n\n                # Process command\n                await self.process_command(command)\n\n                # Add to history\n                if not self.processed_commands.full():\n                    self.processed_commands.put(command)\n\n                # Mark as processed\n                self.command_queue.task_done()\n\n            except asyncio.TimeoutError:\n                # No command available, continue\n                continue\n            except Exception as e:\n                print(f"Error processing command: {e}")\n\n    async def process_command(self, command: VoiceCommand):\n        """Process a single command"""\n        print(f"Processing command: \'{command.text}\' (confidence: {command.confidence:.2f})")\n\n        # Call processing callback if available\n        if self.command_processed_callback:\n            self.command_processed_callback(command)\n\n    def stop_processing(self):\n        """Stop processing commands"""\n        self.is_processing = False\n\n    def get_command_history(self) -> List[VoiceCommand]:\n        """Get command processing history"""\n        history = []\n        temp_queue = queue.Queue()\n\n        # Copy queue contents\n        while not self.processed_commands.empty():\n            cmd = self.processed_commands.get()\n            history.append(cmd)\n            temp_queue.put(cmd)\n\n        # Restore queue\n        while not temp_queue.empty():\n            self.processed_commands.put(temp_queue.get())\n\n        return history\n'})}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"model-optimization-techniques",children:"Model Optimization Techniques"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\nfrom transformers import WhisperForConditionalGeneration, WhisperProcessor\nimport onnxruntime as ort\n\nclass OptimizedWhisperInference:\n    def __init__(self, model_path: str, optimize_for_device: str = "gpu"):\n        """\n        Initialize optimized Whisper inference\n        """\n        self.optimize_for_device = optimize_for_device\n\n        if optimize_for_device == "cpu":\n            # For CPU optimization, consider using ONNX runtime\n            self.use_onnx = True\n            self.onnx_session = self._load_onnx_model(model_path)\n        else:\n            # For GPU, use PyTorch with optimizations\n            self.use_onnx = False\n            self.model = WhisperForConditionalGeneration.from_pretrained(model_path)\n            self.processor = WhisperProcessor.from_pretrained(model_path)\n\n            # Move to device and optimize\n            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n            self.model = self.model.to(device)\n\n            # Apply optimizations\n            if torch.cuda.is_available():\n                self.model = torch.compile(self.model)  # PyTorch 2.0+ optimization\n\n    def _load_onnx_model(self, model_path: str):\n        """Load ONNX model for CPU optimization"""\n        # This would typically involve converting the model to ONNX format first\n        # For demonstration, assuming ONNX model exists\n        session_options = ort.SessionOptions()\n        session_options.intra_op_num_threads = 4  # Limit CPU threads\n        session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n\n        if self.optimize_for_device == "gpu" and "CUDAExecutionProvider" in ort.get_available_providers():\n            providers = ["CUDAExecutionProvider", "CPUExecutionProvider"]\n        else:\n            providers = ["CPUExecutionProvider"]\n\n        return ort.InferenceSession(f"{model_path}/model.onnx", sess_options=session_options, providers=providers)\n\n    def transcribe(self, audio_input) -> str:\n        """Transcribe audio with optimized inference"""\n        if self.use_onnx:\n            return self._transcribe_onnx(audio_input)\n        else:\n            return self._transcribe_pytorch(audio_input)\n\n    def _transcribe_pytorch(self, audio_input):\n        """PyTorch-based transcription"""\n        # Process audio\n        inputs = self.processor(audio_input, sampling_rate=16000, return_tensors="pt")\n        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n\n        # Generate\n        with torch.no_grad():\n            predicted_ids = self.model.generate(**inputs)\n\n        # Decode\n        transcription = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n        return transcription\n\n    def _transcribe_onnx(self, audio_input):\n        """ONNX-based transcription"""\n        # Process audio for ONNX model\n        # This is a simplified example - actual implementation would depend on model structure\n        inputs = self.processor(audio_input, sampling_rate=16000, return_tensors="np")\n\n        # Run inference\n        outputs = self.onnx_session.run(None, {"input_features": inputs["input_features"]})\n\n        # Decode outputs\n        transcription = self.processor.batch_decode(outputs[0], skip_special_tokens=True)[0]\n        return transcription\n\n    def warm_up(self):\n        """Warm up the model with dummy input"""\n        dummy_audio = np.random.randn(16000)  # 1 second of dummy audio\n        try:\n            self.transcribe(dummy_audio)\n            print("Model warmed up successfully")\n        except Exception as e:\n            print(f"Error warming up model: {e}")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,t.jsx)(n.h3,{id:"audio-quality-issues",children:"Audio Quality Issues"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Poor Recognition Accuracy"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Check microphone quality and positioning"}),"\n",(0,t.jsx)(n.li,{children:"Ensure proper audio preprocessing"}),"\n",(0,t.jsx)(n.li,{children:"Verify sample rate matches model expectations (16kHz)"}),"\n",(0,t.jsx)(n.li,{children:"Consider using noise reduction techniques"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"High Latency"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use smaller Whisper models for faster inference"}),"\n",(0,t.jsx)(n.li,{children:"Optimize audio chunk sizes"}),"\n",(0,t.jsx)(n.li,{children:"Consider using GPU acceleration"}),"\n",(0,t.jsx)(n.li,{children:"Implement streaming recognition"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Wake Word False Positives"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Adjust sensitivity thresholds"}),"\n",(0,t.jsx)(n.li,{children:"Use more distinctive wake words"}),"\n",(0,t.jsx)(n.li,{children:"Implement acoustic modeling for wake word detection"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"common-error-solutions",children:"Common Error Solutions"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Error handling and recovery strategies\nclass SpeechRecognitionErrorHandler:\n    def __init__(self):\n        self.error_counts = {}\n        self.recovery_strategies = {\n            \'MicrophoneError\': self.handle_microphone_error,\n            \'ModelLoadError\': self.handle_model_load_error,\n            \'RecognitionError\': self.handle_recognition_error,\n            \'MemoryError\': self.handle_memory_error\n        }\n\n    def handle_error(self, error_type: str, error_details: str):\n        """Handle specific error types with appropriate recovery"""\n        self.error_counts[error_type] = self.error_counts.get(error_type, 0) + 1\n\n        if error_type in self.recovery_strategies:\n            return self.recovery_strategies[error_type](error_details)\n        else:\n            print(f"Unknown error type: {error_type}")\n            return False\n\n    def handle_microphone_error(self, details: str):\n        """Handle microphone-related errors"""\n        print(f"Microphone error: {details}")\n        # Try to reinitialize audio input\n        # Switch to alternative audio source if available\n        return True\n\n    def handle_model_load_error(self, details: str):\n        """Handle model loading errors"""\n        print(f"Model load error: {details}")\n        # Try to load alternative model\n        # Use fallback recognition system\n        return True\n\n    def handle_recognition_error(self, details: str):\n        """Handle recognition errors"""\n        print(f"Recognition error: {details}")\n        # Continue with next audio chunk\n        return True\n\n    def handle_memory_error(self, details: str):\n        """Handle memory errors"""\n        print(f"Memory error: {details}")\n        # Clear audio buffers\n        # Use smaller model temporarily\n        return True\n'})}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"This chapter covered speech recognition for VLA systems:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"OpenAI Whisper setup"})," and configuration for robotics applications"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time processing"})," techniques for continuous speech recognition"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Alternative systems"})," like Vosk and Coqui STT for different use cases"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Audio preprocessing"})," for improved recognition quality"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 integration"})," for robotics applications"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Voice command processing"})," pipelines with validation and queuing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Performance optimization"})," techniques for efficient operation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Troubleshooting strategies"})," for common issues"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The ability to process voice commands in real-time is crucial for natural human-robot interaction. With proper implementation of speech recognition systems, robots can respond to natural language commands and integrate seamlessly into human environments. The next chapter will explore cognitive planning, which translates these voice commands into sequences of robot actions."})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>s,x:()=>a});var o=i(6540);const t={},r=o.createContext(t);function s(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);