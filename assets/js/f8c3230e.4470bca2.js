"use strict";(globalThis.webpackChunkphysical_humanoid_ai_book=globalThis.webpackChunkphysical_humanoid_ai_book||[]).push([[687],{7930(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module2-digital-twin/chapter3-sensor-unity-integration","title":"Chapter 3: Sensor Simulation and Unity Integration for High-Fidelity Rendering","description":"Learning Objectives","source":"@site/docs/module2-digital-twin/chapter3-sensor-unity-integration.md","sourceDirName":"module2-digital-twin","slug":"/module2-digital-twin/chapter3-sensor-unity-integration","permalink":"/physical-humanoid-ai-book/docs/module2-digital-twin/chapter3-sensor-unity-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/alizah-fatima/physical-humanoid-ai-book/tree/main/docs/module2-digital-twin/chapter3-sensor-unity-integration.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Robot Description Formats - URDF and SDF for Humanoid Modeling","permalink":"/physical-humanoid-ai-book/docs/module2-digital-twin/chapter2-urdf-sdf-modeling"},"next":{"title":"Chapter 1: Introduction to NVIDIA Isaac Sim","permalink":"/physical-humanoid-ai-book/docs/module3-nvidia-isaac/chapter1-isaac-sim-intro"}}');var o=i(4848),t=i(8453);const r={sidebar_position:3},s="Chapter 3: Sensor Simulation and Unity Integration for High-Fidelity Rendering",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Sensor Simulation",id:"introduction-to-sensor-simulation",level:2},{value:"LiDAR Sensor Simulation",id:"lidar-sensor-simulation",level:2},{value:"2D LiDAR (Laser Scanner)",id:"2d-lidar-laser-scanner",level:3},{value:"3D LiDAR (Velodyne-style)",id:"3d-lidar-velodyne-style",level:3},{value:"LiDAR Configuration for Humanoid Robots",id:"lidar-configuration-for-humanoid-robots",level:3},{value:"Depth Camera Simulation",id:"depth-camera-simulation",level:2},{value:"RGB-D Camera Configuration",id:"rgb-d-camera-configuration",level:3},{value:"Stereo Camera Setup",id:"stereo-camera-setup",level:3},{value:"Depth Camera for Humanoid Applications",id:"depth-camera-for-humanoid-applications",level:3},{value:"IMU Sensor Simulation",id:"imu-sensor-simulation",level:2},{value:"IMU Configuration",id:"imu-configuration",level:3},{value:"Multiple IMU Placement for Humanoid Balance",id:"multiple-imu-placement-for-humanoid-balance",level:3},{value:"Advanced Sensor Simulation",id:"advanced-sensor-simulation",level:2},{value:"Force/Torque Sensors",id:"forcetorque-sensors",level:3},{value:"GPS Simulation",id:"gps-simulation",level:3},{value:"Unity Integration for High-Fidelity Rendering",id:"unity-integration-for-high-fidelity-rendering",level:2},{value:"Unity as a Visualization Tool",id:"unity-as-a-visualization-tool",level:3},{value:"Gazebo-Unity Bridge Architecture",id:"gazebo-unity-bridge-architecture",level:3},{value:"Setting Up the Bridge",id:"setting-up-the-bridge",level:3},{value:"1. ROS 2 Bridge Package",id:"1-ros-2-bridge-package",level:4},{value:"2. Unity ROS TCP Connector",id:"2-unity-ros-tcp-connector",level:4},{value:"Unity Asset Setup for Humanoid Robots",id:"unity-asset-setup-for-humanoid-robots",level:3},{value:"1. Creating Humanoid Robot Models in Unity",id:"1-creating-humanoid-robot-models-in-unity",level:4},{value:"2. High-Fidelity Rendering Materials",id:"2-high-fidelity-rendering-materials",level:4},{value:"Sensor Fusion in Simulation",id:"sensor-fusion-in-simulation",level:2},{value:"Combining Multiple Sensor Inputs",id:"combining-multiple-sensor-inputs",level:3},{value:"Extended Kalman Filter for Sensor Fusion",id:"extended-kalman-filter-for-sensor-fusion",level:3},{value:"Human-Robot Interaction in Unity",id:"human-robot-interaction-in-unity",level:2},{value:"Creating Interactive Interfaces",id:"creating-interactive-interfaces",level:3},{value:"Virtual Reality Integration",id:"virtual-reality-integration",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Sensor Simulation Optimization",id:"sensor-simulation-optimization",level:3},{value:"Unity Rendering Optimization",id:"unity-rendering-optimization",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"Sensor Data Quality Issues",id:"sensor-data-quality-issues",level:3},{value:"Unity Integration Issues",id:"unity-integration-issues",level:3},{value:"Summary",id:"summary",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"chapter-3-sensor-simulation-and-unity-integration-for-high-fidelity-rendering",children:"Chapter 3: Sensor Simulation and Unity Integration for High-Fidelity Rendering"})}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Implement various sensor types in Gazebo simulation (LiDAR, Depth Cameras, IMUs)"}),"\n",(0,o.jsx)(e.li,{children:"Configure sensor parameters for realistic humanoid robot perception"}),"\n",(0,o.jsx)(e.li,{children:"Understand the integration between Gazebo and Unity for enhanced visualization"}),"\n",(0,o.jsx)(e.li,{children:"Set up high-fidelity rendering pipelines for human-robot interaction"}),"\n",(0,o.jsx)(e.li,{children:"Apply sensor fusion techniques in simulated environments"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"introduction-to-sensor-simulation",children:"Introduction to Sensor Simulation"}),"\n",(0,o.jsx)(e.p,{children:"Sensor simulation is crucial for developing and testing humanoid robots in virtual environments. It allows developers to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Test perception algorithms without physical hardware"}),"\n",(0,o.jsx)(e.li,{children:"Generate large datasets for AI training"}),"\n",(0,o.jsx)(e.li,{children:"Validate sensor fusion approaches"}),"\n",(0,o.jsx)(e.li,{children:"Evaluate human-robot interaction scenarios safely"}),"\n",(0,o.jsx)(e.li,{children:"Reduce development costs and time"}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"In the context of humanoid robotics, sensor simulation must accurately represent how robots perceive their environment through various modalities."}),"\n",(0,o.jsx)(e.h2,{id:"lidar-sensor-simulation",children:"LiDAR Sensor Simulation"}),"\n",(0,o.jsx)(e.h3,{id:"2d-lidar-laser-scanner",children:"2D LiDAR (Laser Scanner)"}),"\n",(0,o.jsx)(e.p,{children:"2D LiDAR sensors provide distance measurements in a 2D plane and are commonly used for navigation and mapping:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'\x3c!-- In URDF/SDF --\x3e\n<gazebo reference="laser_link">\n  <sensor name="laser_scan" type="ray">\n    <pose>0 0 0 0 0 0</pose>\n    <visualize>true</visualize>\n    <update_rate>10</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>720</samples>\n          <resolution>1</resolution>\n          <min_angle>-1.570796</min_angle>  \x3c!-- -90 degrees --\x3e\n          <max_angle>1.570796</max_angle>   \x3c!-- 90 degrees --\x3e\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.1</min>\n        <max>30.0</max>\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    <plugin name="laser_controller" filename="libgazebo_ros_ray_sensor.so">\n      <ros>\n        <namespace>/laser</namespace>\n        <remapping>~/out:=scan</remapping>\n      </ros>\n      <output_type>sensor_msgs/LaserScan</output_type>\n      <frame_name>laser_link</frame_name>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,o.jsx)(e.h3,{id:"3d-lidar-velodyne-style",children:"3D LiDAR (Velodyne-style)"}),"\n",(0,o.jsx)(e.p,{children:"3D LiDAR sensors provide full 3D point cloud data for complex environment mapping:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'<gazebo reference="velodyne_link">\n  <sensor name="velodyne_sensor" type="ray">\n    <pose>0 0 0 0 0 0</pose>\n    <visualize>false</visualize>\n    <update_rate>10</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>800</samples>\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle>\n          <max_angle>3.14159</max_angle>\n        </horizontal>\n        <vertical>\n          <samples>32</samples>\n          <resolution>1</resolution>\n          <min_angle>-0.261799</min_angle>  \x3c!-- -15 degrees --\x3e\n          <max_angle>0.261799</max_angle>   \x3c!-- 15 degrees --\x3e\n        </vertical>\n      </scan>\n      <range>\n        <min>0.2</min>\n        <max>100.0</max>\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    <plugin name="velodyne_controller" filename="libgazebo_ros_velodyne_gpu.so">\n      <ros>\n        <namespace>/velodyne</namespace>\n        <remapping>~/out:=/velodyne_points</remapping>\n      </ros>\n      <topic_name>/velodyne_points</topic_name>\n      <frame_name>velodyne_link</frame_name>\n      <min_range>0.9</min_range>\n      <max_range>130.0</max_range>\n      <gaussian_noise>0.008</gaussian_noise>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,o.jsx)(e.h3,{id:"lidar-configuration-for-humanoid-robots",children:"LiDAR Configuration for Humanoid Robots"}),"\n",(0,o.jsx)(e.p,{children:"For humanoid robots, LiDAR placement is critical for navigation and obstacle detection:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Head-mounted LiDAR for environment mapping --\x3e\n<joint name="head_laser_joint" type="fixed">\n  <parent link="head"/>\n  <child link="head_laser_link"/>\n  <origin xyz="0.05 0 0.05" rpy="0 0 0"/>\n</joint>\n\n<link name="head_laser_link">\n  <inertial>\n    <mass value="0.1"/>\n    <origin xyz="0 0 0"/>\n    <inertia ixx="0.0001" ixy="0" ixz="0" iyy="0.0001" iyz="0" izz="0.0001"/>\n  </inertial>\n</link>\n\n\x3c!-- Chest-mounted LiDAR for navigation --\x3e\n<joint name="chest_laser_joint" type="fixed">\n  <parent link="torso"/>\n  <child link="chest_laser_link"/>\n  <origin xyz="0.1 0 0.1" rpy="0 0 0"/>\n</joint>\n\n<link name="chest_laser_link">\n  <inertial>\n    <mass value="0.1"/>\n    <origin xyz="0 0 0"/>\n    <inertia ixx="0.0001" ixy="0" ixz="0" iyy="0.0001" iyz="0" izz="0.0001"/>\n  </inertial>\n</link>\n'})}),"\n",(0,o.jsx)(e.h2,{id:"depth-camera-simulation",children:"Depth Camera Simulation"}),"\n",(0,o.jsx)(e.p,{children:"Depth cameras provide both visual and depth information, essential for humanoid perception:"}),"\n",(0,o.jsx)(e.h3,{id:"rgb-d-camera-configuration",children:"RGB-D Camera Configuration"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'<gazebo reference="camera_link">\n  <sensor name="camera" type="depth">\n    <always_on>true</always_on>\n    <update_rate>30</update_rate>\n    <camera name="head">\n      <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60 degrees --\x3e\n      <image>\n        <format>R8G8B8</format>\n        <width>640</width>\n        <height>480</height>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>10.0</far>\n      </clip>\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.007</stddev>\n      </noise>\n    </camera>\n    <visualize>true</visualize>\n    <plugin name="camera_controller" filename="libgazebo_ros_openni_kinect.so">\n      <alwaysOn>true</alwaysOn>\n      <updateRate>30.0</updateRate>\n      <cameraName>camera</cameraName>\n      <frameName>camera_optical_frame</frameName>\n      <baseline>0.2</baseline>\n      <distortion_k1>0.0</distortion_k1>\n      <distortion_k2>0.0</distortion_k2>\n      <distortion_k3>0.0</distortion_k3>\n      <distortion_t1>0.0</distortion_t1>\n      <distortion_t2>0.0</distortion_t2>\n      <pointCloudCutoff>0.1</pointCloudCutoff>\n      <pointCloudCutoffMax>3.0</pointCloudCutoffMax>\n      <CxPrime>0.0</CxPrime>\n      <Cx>0.0</Cx>\n      <Cy>0.0</Cy>\n      <focalLength>0.0</focalLength>\n      <hackBaseline>0.0</hackBaseline>\n      <disableDistortionCorrection>false</disableDistortionCorrection>\n      <projection_parameters>\n        <px>0.0</px>\n        <py>0.0</py>\n      </projection_parameters>\n      <ros>\n        <namespace>/camera</namespace>\n        <remapping>rgb/image_raw:=/camera/rgb/image_raw</remapping>\n        <remapping>depth/image_raw:=/camera/depth/image_raw</remapping>\n        <remapping>depth/points:=/camera/depth/points</remapping>\n        <remapping>rgb/camera_info:=/camera/rgb/camera_info</remapping>\n      </ros>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,o.jsx)(e.h3,{id:"stereo-camera-setup",children:"Stereo Camera Setup"}),"\n",(0,o.jsx)(e.p,{children:"Stereo cameras provide depth perception through triangulation:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Left camera --\x3e\n<joint name="left_camera_joint" type="fixed">\n  <parent link="head"/>\n  <child link="left_camera_frame"/>\n  <origin xyz="0.05 0.06 0.05" rpy="0 0 0"/>\n</joint>\n\n<link name="left_camera_frame">\n  <inertial>\n    <mass value="0.01"/>\n    <origin xyz="0 0 0"/>\n    <inertia ixx="0.000001" ixy="0" ixz="0" iyy="0.000001" iyz="0" izz="0.000001"/>\n  </inertial>\n</link>\n\n\x3c!-- Right camera --\x3e\n<joint name="right_camera_joint" type="fixed">\n  <parent link="head"/>\n  <child link="right_camera_frame"/>\n  <origin xyz="0.05 -0.06 0.05" rpy="0 0 0"/>\n</joint>\n\n<link name="right_camera_frame">\n  <inertial>\n    <mass value="0.01"/>\n    <origin xyz="0 0 0"/>\n    <inertia ixx="0.000001" ixy="0" ixz="0" iyy="0.000001" iyz="0" izz="0.000001"/>\n  </inertial>\n</link>\n\n\x3c!-- Configure both cameras with appropriate plugins --\x3e\n<gazebo reference="left_camera_frame">\n  <sensor name="left_camera" type="camera">\n    \x3c!-- Camera configuration --\x3e\n  </sensor>\n</gazebo>\n\n<gazebo reference="right_camera_frame">\n  <sensor name="right_camera" type="camera">\n    \x3c!-- Camera configuration --\x3e\n  </sensor>\n</gazebo>\n'})}),"\n",(0,o.jsx)(e.h3,{id:"depth-camera-for-humanoid-applications",children:"Depth Camera for Humanoid Applications"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Face detection camera --\x3e\n<joint name="face_camera_joint" type="fixed">\n  <parent link="head"/>\n  <child link="face_camera_link"/>\n  <origin xyz="0.08 0 0.02" rpy="0 0 0"/>  \x3c!-- Positioned at face level --\x3e\n</joint>\n\n<link name="face_camera_link">\n  <inertial>\n    <mass value="0.01"/>\n    <origin xyz="0 0 0"/>\n    <inertia ixx="0.000001" ixy="0" ixz="0" iyy="0.000001" iyz="0" izz="0.000001"/>\n  </inertial>\n</link>\n\n\x3c!-- Navigation camera --\x3e\n<joint name="nav_camera_joint" type="fixed">\n  <parent link="torso"/>\n  <child link="nav_camera_link"/>\n  <origin xyz="0.15 0 0.1" rpy="0 -0.1 0"/>  \x3c!-- Slightly downward facing --\x3e\n</joint>\n\n<link name="nav_camera_link">\n  <inertial>\n    <mass value="0.01"/>\n    <origin xyz="0 0 0"/>\n    <inertia ixx="0.000001" ixy="0" ixz="0" iyy="0.000001" iyz="0" izz="0.000001"/>\n  </inertial>\n</link>\n'})}),"\n",(0,o.jsx)(e.h2,{id:"imu-sensor-simulation",children:"IMU Sensor Simulation"}),"\n",(0,o.jsx)(e.p,{children:"IMU (Inertial Measurement Unit) sensors provide crucial information about robot orientation and acceleration:"}),"\n",(0,o.jsx)(e.h3,{id:"imu-configuration",children:"IMU Configuration"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'\x3c!-- IMU in the torso for balance estimation --\x3e\n<gazebo reference="torso">\n  <sensor name="torso_imu" type="imu">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <visualize>false</visualize>\n    <imu>\n      <angular_velocity>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0017</stddev>  \x3c!-- ~0.1 deg/s --\x3e\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0017</stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0017</stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>  \x3c!-- ~0.017 m/s\xb2 --\x3e\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n    <plugin name="imu_controller" filename="libgazebo_ros_imu.so">\n      <ros>\n        <namespace>/imu</namespace>\n        <remapping>~/out:=data</remapping>\n      </ros>\n      <frame_name>torso</frame_name>\n      <body_name>torso</body_name>\n      <update_rate>100</update_rate>\n      <gaussian_noise>0.017</gaussian_noise>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,o.jsx)(e.h3,{id:"multiple-imu-placement-for-humanoid-balance",children:"Multiple IMU Placement for Humanoid Balance"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Head IMU for orientation tracking --\x3e\n<joint name="head_imu_joint" type="fixed">\n  <parent link="head"/>\n  <child link="head_imu_link"/>\n  <origin xyz="0 0 0.05" rpy="0 0 0"/>\n</joint>\n\n<link name="head_imu_link">\n  <inertial>\n    <mass value="0.001"/>\n    <origin xyz="0 0 0"/>\n    <inertia ixx="0.000000001" ixy="0" ixz="0" iyy="0.000000001" iyz="0" izz="0.000000001"/>\n  </inertial>\n</link>\n\n\x3c!-- Foot IMUs for contact detection --\x3e\n<joint name="left_foot_imu_joint" type="fixed">\n  <parent link="left_foot"/>\n  <child link="left_foot_imu_link"/>\n  <origin xyz="0 0 0" rpy="0 0 0"/>\n</joint>\n\n<link name="left_foot_imu_link">\n  <inertial>\n    <mass value="0.001"/>\n    <origin xyz="0 0 0"/>\n    <inertia ixx="0.000000001" ixy="0" ixz="0" iyy="0.000000001" iyz="0" izz="0.000000001"/>\n  </inertial>\n</link>\n'})}),"\n",(0,o.jsx)(e.h2,{id:"advanced-sensor-simulation",children:"Advanced Sensor Simulation"}),"\n",(0,o.jsx)(e.h3,{id:"forcetorque-sensors",children:"Force/Torque Sensors"}),"\n",(0,o.jsx)(e.p,{children:"Force/torque sensors are crucial for humanoid manipulation and balance:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Force/Torque sensor at ankle for balance control --\x3e\n<gazebo reference="left_ankle">\n  <sensor name="left_ankle_ft" type="force_torque">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <force_torque>\n      <frame>child</frame>\n      <measure_direction>child_to_parent</measure_direction>\n    </force_torque>\n    <plugin name="ft_sensor_plugin" filename="libgazebo_ros_ft_sensor.so">\n      <ros>\n        <namespace>/ft_sensor</namespace>\n        <remapping>~/wrench:=left_ankle_wrench</remapping>\n      </ros>\n      <frame_name>left_ankle</frame_name>\n      <topic_name>left_ankle_wrench</topic_name>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,o.jsx)(e.h3,{id:"gps-simulation",children:"GPS Simulation"}),"\n",(0,o.jsx)(e.p,{children:"For outdoor humanoid applications:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'<gazebo reference="base_link">\n  <sensor name="gps_sensor" type="gps">\n    <always_on>true</always_on>\n    <update_rate>1</update_rate>\n    <visualize>false</visualize>\n    <plugin name="gps_plugin" filename="libgazebo_ros_gps.so">\n      <ros>\n        <namespace>/gps</namespace>\n        <remapping>~/out:=fix</remapping>\n      </ros>\n      <frame_name>base_link</frame_name>\n      <update_rate>1.0</update_rate>\n      <gaussian_noise>0.1</gaussian_noise>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,o.jsx)(e.h2,{id:"unity-integration-for-high-fidelity-rendering",children:"Unity Integration for High-Fidelity Rendering"}),"\n",(0,o.jsx)(e.h3,{id:"unity-as-a-visualization-tool",children:"Unity as a Visualization Tool"}),"\n",(0,o.jsx)(e.p,{children:"While Gazebo provides accurate physics simulation, Unity excels in high-fidelity rendering and human-robot interaction. The two can be integrated for the best of both worlds:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Physics Simulation"}),": Gazebo handles accurate physics"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Visual Rendering"}),": Unity provides photorealistic visualization"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Human Interaction"}),": Unity enables intuitive human-robot interaction interfaces"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"gazebo-unity-bridge-architecture",children:"Gazebo-Unity Bridge Architecture"}),"\n",(0,o.jsx)(e.mermaid,{value:"graph LR\n    A[Gazebo Physics Engine] --\x3e B(Robot State Publisher)\n    B --\x3e C(Data Bridge)\n    C --\x3e D[Unity Visualization Engine]\n    D --\x3e E(High-Fidelity Rendering)\n    E --\x3e F(Human-Computer Interface)\n    F --\x3e G[Interaction Commands]\n    G --\x3e C\n    C --\x3e A"}),"\n",(0,o.jsx)(e.h3,{id:"setting-up-the-bridge",children:"Setting Up the Bridge"}),"\n",(0,o.jsx)(e.h4,{id:"1-ros-2-bridge-package",children:"1. ROS 2 Bridge Package"}),"\n",(0,o.jsxs)(e.p,{children:["Use the ",(0,o.jsx)(e.code,{children:"ros_gz_bridge"})," package to connect ROS 2, Gazebo, and external visualization tools:"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"# Install the bridge package\nsudo apt install ros-humble-ros-gz-bridge\n\n# Launch the bridge\nros2 run ros_gz_bridge parameter_bridge\n"})}),"\n",(0,o.jsx)(e.h4,{id:"2-unity-ros-tcp-connector",children:"2. Unity ROS TCP Connector"}),"\n",(0,o.jsx)(e.p,{children:"In Unity, use the Unity ROS TCP connector to receive simulation data:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'// Unity C# script for receiving robot state\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing ROS2;\nusing std_msgs.msg;\n\npublic class RobotStateReceiver : MonoBehaviour\n{\n    private ROS2UnityComponent ros2Unity;\n    private ISubscription<JointState> jointStateSub;\n\n    void Start()\n    {\n        ros2Unity = GetComponent<ROS2UnityComponent>();\n        ros2Unity.Init();\n        ros2Unity.CreateNode("unity_robot_visualizer");\n\n        jointStateSub = ros2Unity.node.CreateSubscription<JointState>\n            ("/joint_states", JointStateCallback);\n    }\n\n    void JointStateCallback(JointState jointState)\n    {\n        // Update Unity robot model based on received joint states\n        UpdateRobotModel(jointState);\n    }\n\n    void UpdateRobotModel(JointState jointState)\n    {\n        for (int i = 0; i < jointState.name.Count; i++)\n        {\n            string jointName = jointState.name[i];\n            float jointPosition = jointState.position[i];\n\n            Transform jointTransform = FindJointTransform(jointName);\n            if (jointTransform != null)\n            {\n                // Apply rotation based on joint position\n                jointTransform.localRotation =\n                    Quaternion.AngleAxis(jointPosition * Mathf.Rad2Deg, Vector3.right);\n            }\n        }\n    }\n\n    Transform FindJointTransform(string jointName)\n    {\n        // Find the corresponding joint in Unity hierarchy\n        Transform[] allChildren = GetComponentsInChildren<Transform>();\n        foreach (Transform child in allChildren)\n        {\n            if (child.name == jointName)\n                return child;\n        }\n        return null;\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"unity-asset-setup-for-humanoid-robots",children:"Unity Asset Setup for Humanoid Robots"}),"\n",(0,o.jsx)(e.h4,{id:"1-creating-humanoid-robot-models-in-unity",children:"1. Creating Humanoid Robot Models in Unity"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'// Example script for humanoid robot in Unity\nusing UnityEngine;\n\npublic class HumanoidRobotController : MonoBehaviour\n{\n    [Header("Joint Configuration")]\n    public Transform head;\n    public Transform leftArm;\n    public Transform rightArm;\n    public Transform leftLeg;\n    public Transform rightLeg;\n\n    [Header("Sensor Visualization")]\n    public GameObject[] lidarPoints;\n    public GameObject cameraFrustum;\n\n    void Update()\n    {\n        // Update sensor visualizations based on simulation data\n        UpdateLidarVisualization();\n        UpdateCameraVisualization();\n    }\n\n    void UpdateLidarVisualization()\n    {\n        // Visualize LiDAR points received from simulation\n        // This would typically be updated from ROS 2 messages\n    }\n\n    void UpdateCameraVisualization()\n    {\n        // Show camera frustum for depth camera visualization\n        // Update based on camera data from simulation\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h4,{id:"2-high-fidelity-rendering-materials",children:"2. High-Fidelity Rendering Materials"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'// Shader for realistic robot materials in Unity\nShader "Custom/RobotMaterial"\n{\n    Properties\n    {\n        _Color ("Color", Color) = (0.8, 0.8, 0.8, 1)\n        _Metallic ("Metallic", Range(0, 1)) = 0.5\n        _Smoothness ("Smoothness", Range(0, 1)) = 0.5\n        _MainTex ("Albedo", 2D) = "white" {}\n    }\n    SubShader\n    {\n        Tags { "RenderType"="Opaque" }\n        LOD 200\n\n        CGPROGRAM\n        #pragma surface surf Standard fullforwardshadows\n        #pragma target 3.0\n\n        struct Input\n        {\n            float2 uv_MainTex;\n        };\n\n        sampler2D _MainTex;\n        fixed4 _Color;\n        half _Metallic;\n        half _Smoothness;\n\n        void surf (Input IN, inout SurfaceOutputStandard o)\n        {\n            fixed4 c = tex2D(_MainTex, IN.uv_MainTex) * _Color;\n            o.Albedo = c.rgb;\n            o.Metallic = _Metallic;\n            o.Smoothness = _Smoothness;\n            o.Alpha = c.a;\n        }\n        ENDCG\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"sensor-fusion-in-simulation",children:"Sensor Fusion in Simulation"}),"\n",(0,o.jsx)(e.h3,{id:"combining-multiple-sensor-inputs",children:"Combining Multiple Sensor Inputs"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image, Imu, PointCloud2\nfrom geometry_msgs.msg import PoseWithCovarianceStamped\nfrom nav_msgs.msg import Odometry\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass SensorFusionNode(Node):\n    def __init__(self):\n        super().__init__('sensor_fusion_node')\n\n        # Subscriptions for different sensors\n        self.lidar_sub = self.create_subscription(\n            LaserScan, '/laser/scan', self.lidar_callback, 10)\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10)\n        self.odom_sub = self.create_subscription(\n            Odometry, '/odom', self.odom_callback, 10)\n        self.camera_sub = self.create_subscription(\n            Image, '/camera/rgb/image_raw', self.camera_callback, 10)\n\n        # Publisher for fused state\n        self.fused_state_pub = self.create_publisher(\n            PoseWithCovarianceStamped, '/fused_pose', 10)\n\n        # Initialize state variables\n        self.robot_pose = np.zeros(6)  # x, y, z, roll, pitch, yaw\n        self.robot_velocity = np.zeros(6)  # linear and angular velocities\n\n    def lidar_callback(self, msg):\n        # Process LiDAR data for environment mapping\n        ranges = np.array(msg.ranges)\n        # Implement obstacle detection and mapping\n        self.process_lidar_data(ranges, msg.angle_min, msg.angle_increment)\n\n    def imu_callback(self, msg):\n        # Process IMU data for orientation and acceleration\n        orientation = [msg.orientation.x, msg.orientation.y,\n                      msg.orientation.z, msg.orientation.w]\n        angular_velocity = [msg.angular_velocity.x, msg.angular_velocity.y,\n                           msg.angular_velocity.z]\n        linear_acceleration = [msg.linear_acceleration.x,\n                              msg.linear_acceleration.y,\n                              msg.linear_acceleration.z]\n\n        # Update pose estimate using IMU integration\n        self.update_pose_from_imu(orientation, angular_velocity, linear_acceleration)\n\n    def odom_callback(self, msg):\n        # Process odometry data\n        position = [msg.pose.pose.position.x, msg.pose.pose.position.y,\n                   msg.pose.pose.position.z]\n        orientation = [msg.pose.pose.orientation.x, msg.pose.pose.orientation.y,\n                      msg.pose.pose.orientation.z, msg.pose.pose.orientation.w]\n\n        # Update pose estimate from odometry\n        self.update_pose_from_odom(position, orientation)\n\n    def camera_callback(self, msg):\n        # Process camera data for visual features\n        # This could include face detection, object recognition, etc.\n        self.process_camera_data(msg)\n\n    def sensor_fusion_update(self):\n        # Implement Kalman filter or other fusion algorithm\n        # Combine all sensor inputs to get optimal state estimate\n        pass\n\n    def publish_fused_state(self):\n        # Publish the fused state estimate\n        fused_pose_msg = PoseWithCovarianceStamped()\n        # Fill in the message with fused state data\n        self.fused_state_pub.publish(fused_pose_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    sensor_fusion_node = SensorFusionNode()\n\n    try:\n        rclpy.spin(sensor_fusion_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        sensor_fusion_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(e.h3,{id:"extended-kalman-filter-for-sensor-fusion",children:"Extended Kalman Filter for Sensor Fusion"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom scipy.linalg import block_diag\n\nclass ExtendedKalmanFilter:\n    def __init__(self):\n        # State vector: [x, y, z, roll, pitch, yaw, vx, vy, vz, wx, wy, wz]\n        self.state_dim = 12\n        self.state = np.zeros(self.state_dim)\n\n        # Covariance matrix\n        self.covariance = np.eye(self.state_dim) * 0.1\n\n        # Process noise\n        self.process_noise = np.eye(self.state_dim) * 0.01\n\n    def predict(self, dt):\n        """Prediction step of EKF"""\n        # State transition model (simplified)\n        F = self.compute_jacobian_F(dt)\n\n        # Predict state\n        self.state = self.state_transition(self.state, dt)\n\n        # Predict covariance\n        self.covariance = F @ self.covariance @ F.T + self.process_noise\n\n    def update_lidar(self, lidar_data):\n        """Update from LiDAR measurements"""\n        # Measurement model for LiDAR\n        H = self.compute_jacobian_H_lidar()\n        z = self.extract_lidar_features(lidar_data)\n\n        # Innovation\n        innovation = z - self.lidar_measurement_model(self.state)\n\n        # Innovation covariance\n        S = H @ self.covariance @ H.T + self.lidar_noise\n\n        # Kalman gain\n        K = self.covariance @ H.T @ np.linalg.inv(S)\n\n        # Update state and covariance\n        self.state = self.state + K @ innovation\n        self.covariance = (np.eye(self.state_dim) - K @ H) @ self.covariance\n\n    def update_imu(self, imu_data):\n        """Update from IMU measurements"""\n        # Similar process for IMU data\n        pass\n\n    def update_camera(self, camera_data):\n        """Update from camera measurements"""\n        # Similar process for camera data\n        pass\n\n    def state_transition(self, state, dt):\n        """Nonlinear state transition model"""\n        # Implement the dynamics model\n        return state\n'})}),"\n",(0,o.jsx)(e.h2,{id:"human-robot-interaction-in-unity",children:"Human-Robot Interaction in Unity"}),"\n",(0,o.jsx)(e.h3,{id:"creating-interactive-interfaces",children:"Creating Interactive Interfaces"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'// Unity script for human-robot interaction\nusing UnityEngine;\nusing UnityEngine.UI;\nusing TMPro;\n\npublic class HumanRobotInterface : MonoBehaviour\n{\n    [Header("UI Elements")]\n    public Slider robotSpeedSlider;\n    public Button walkButton;\n    public Button stopButton;\n    public Button gestureButton;\n    public TextMeshProUGUI statusText;\n\n    [Header("Robot Control")]\n    public HumanoidRobotController robotController;\n\n    void Start()\n    {\n        SetupUI();\n    }\n\n    void SetupUI()\n    {\n        robotSpeedSlider.onValueChanged.AddListener(OnSpeedChanged);\n        walkButton.onClick.AddListener(OnWalkClicked);\n        stopButton.onClick.AddListener(OnStopClicked);\n        gestureButton.onClick.AddListener(OnGestureClicked);\n    }\n\n    void OnSpeedChanged(float speed)\n    {\n        // Send speed command to robot through ROS 2\n        robotController.SetSpeed(speed);\n        statusText.text = $"Speed: {speed:F2} m/s";\n    }\n\n    void OnWalkClicked()\n    {\n        // Send walk command to robot\n        robotController.StartWalking();\n        statusText.text = "Walking...";\n    }\n\n    void OnStopClicked()\n    {\n        // Send stop command to robot\n        robotController.Stop();\n        statusText.text = "Stopped";\n    }\n\n    void OnGestureClicked()\n    {\n        // Send gesture command to robot\n        robotController.PerformGesture();\n        statusText.text = "Performing gesture...";\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"virtual-reality-integration",children:"Virtual Reality Integration"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'// VR interaction for humanoid robot control\nusing UnityEngine;\nusing UnityEngine.XR;\n\npublic class VRHumanoidControl : MonoBehaviour\n{\n    [Header("VR Controllers")]\n    public Transform leftController;\n    public Transform rightController;\n\n    [Header("Robot Control")]\n    public HumanoidRobotController robot;\n\n    void Update()\n    {\n        HandleVRInput();\n    }\n\n    void HandleVRInput()\n    {\n        // Get VR controller inputs\n        if (leftController != null)\n        {\n            // Use left controller for navigation commands\n            Vector3 leftPosition = leftController.position;\n            Quaternion leftRotation = leftController.rotation;\n\n            // Send navigation commands based on controller position/orientation\n            SendNavigationCommand(leftPosition, leftRotation);\n        }\n\n        if (rightController != null)\n        {\n            // Use right controller for manipulation commands\n            Vector3 rightPosition = rightController.position;\n            Quaternion rightRotation = rightController.rotation;\n\n            // Send manipulation commands\n            SendManipulationCommand(rightPosition, rightRotation);\n        }\n    }\n\n    void SendNavigationCommand(Vector3 position, Quaternion rotation)\n    {\n        // Convert VR input to robot navigation commands\n        // This would typically send ROS 2 messages\n    }\n\n    void SendManipulationCommand(Vector3 position, Quaternion rotation)\n    {\n        // Convert VR input to robot manipulation commands\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsx)(e.h3,{id:"sensor-simulation-optimization",children:"Sensor Simulation Optimization"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Optimize sensor update rates based on application needs --\x3e\n\x3c!-- For navigation: 10-20 Hz LiDAR is often sufficient --\x3e\n<gazebo reference="navigation_lidar">\n  <sensor name="nav_lidar" type="ray">\n    <update_rate>15</update_rate>  \x3c!-- Lower update rate for better performance --\x3e\n    \x3c!-- ... other configurations ... --\x3e\n  </sensor>\n</gazebo>\n\n\x3c!-- For precise manipulation: Higher update rate IMU --\x3e\n<gazebo reference="balance_imu">\n  <sensor name="balance_imu" type="imu">\n    <update_rate>200</update_rate>  \x3c!-- Higher update rate for balance control --\x3e\n    \x3c!-- ... other configurations ... --\x3e\n  </sensor>\n</gazebo>\n'})}),"\n",(0,o.jsx)(e.h3,{id:"unity-rendering-optimization",children:"Unity Rendering Optimization"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'// LOD (Level of Detail) system for robot models\nusing UnityEngine;\n\npublic class RobotLODSystem : MonoBehaviour\n{\n    [Header("LOD Configuration")]\n    public Transform[] lodLevels;\n    public float[] lodDistances;\n\n    [Header("Performance Settings")]\n    public bool enableLOD = true;\n    public float lodUpdateInterval = 0.5f;\n\n    private float lastLODUpdate;\n    private Transform cameraTransform;\n\n    void Start()\n    {\n        // Find main camera\n        cameraTransform = Camera.main.transform;\n    }\n\n    void Update()\n    {\n        if (enableLOD && Time.time - lastLODUpdate > lodUpdateInterval)\n        {\n            UpdateLOD();\n            lastLODUpdate = Time.time;\n        }\n    }\n\n    void UpdateLOD()\n    {\n        if (cameraTransform != null)\n        {\n            float distance = Vector3.Distance(transform.position, cameraTransform.position);\n\n            for (int i = 0; i < lodLevels.Length; i++)\n            {\n                if (distance < lodDistances[i])\n                {\n                    // Activate this LOD level\n                    ActivateLOD(i);\n                    break;\n                }\n            }\n        }\n    }\n\n    void ActivateLOD(int level)\n    {\n        for (int i = 0; i < lodLevels.Length; i++)\n        {\n            lodLevels[i].gameObject.SetActive(i == level);\n        }\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,o.jsx)(e.h3,{id:"sensor-data-quality-issues",children:"Sensor Data Quality Issues"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Noisy Sensor Data"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Check noise parameters in sensor configuration"}),"\n",(0,o.jsx)(e.li,{children:"Verify update rates are appropriate"}),"\n",(0,o.jsx)(e.li,{children:"Ensure proper coordinate frame alignment"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Delayed Sensor Updates"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Increase update rates if needed"}),"\n",(0,o.jsx)(e.li,{children:"Check network bandwidth for remote visualization"}),"\n",(0,o.jsx)(e.li,{children:"Optimize ROS 2 QoS settings"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Incorrect Sensor Readings"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Verify sensor placement in URDF"}),"\n",(0,o.jsx)(e.li,{children:"Check coordinate frame conventions"}),"\n",(0,o.jsx)(e.li,{children:"Ensure proper calibration parameters"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"unity-integration-issues",children:"Unity Integration Issues"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Synchronization Problems"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Use appropriate time synchronization"}),"\n",(0,o.jsx)(e.li,{children:"Implement interpolation for smooth visualization"}),"\n",(0,o.jsx)(e.li,{children:"Check message rates between systems"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Performance Issues"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Reduce Unity rendering quality during development"}),"\n",(0,o.jsx)(e.li,{children:"Use Level of Detail (LOD) systems"}),"\n",(0,o.jsx)(e.li,{children:"Optimize sensor update rates"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"This chapter covered the comprehensive setup of sensor simulation for humanoid robots:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"LiDAR, depth camera, and IMU configurations in Gazebo"}),"\n",(0,o.jsx)(e.li,{children:"Proper sensor placement and parameter tuning for humanoid applications"}),"\n",(0,o.jsx)(e.li,{children:"Unity integration for high-fidelity rendering and human-robot interaction"}),"\n",(0,o.jsx)(e.li,{children:"Sensor fusion techniques for combining multiple sensor inputs"}),"\n",(0,o.jsx)(e.li,{children:"Performance optimization strategies for both simulation and visualization"}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"The integration of accurate sensor simulation with high-fidelity visualization in Unity provides a powerful platform for developing and testing humanoid robots in complex scenarios. This combination enables realistic testing of perception algorithms, human-robot interaction interfaces, and control systems before deployment on physical hardware."})]})}function m(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(c,{...n})}):c(n)}},8453(n,e,i){i.d(e,{R:()=>r,x:()=>s});var a=i(6540);const o={},t=a.createContext(o);function r(n){const e=a.useContext(t);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:r(n.components),a.createElement(t.Provider,{value:e},n.children)}}}]);