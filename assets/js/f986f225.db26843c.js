"use strict";(globalThis.webpackChunkphysical_humanoid_ai_book=globalThis.webpackChunkphysical_humanoid_ai_book||[]).push([[437],{1311(e,n,a){a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module3-nvidia-isaac/chapter2-isaac-ros-tools","title":"Chapter 2: Isaac ROS - Hardware-Accelerated Tools","description":"Learning Objectives","source":"@site/docs/module3-nvidia-isaac/chapter2-isaac-ros-tools.md","sourceDirName":"module3-nvidia-isaac","slug":"/module3-nvidia-isaac/chapter2-isaac-ros-tools","permalink":"/docs/module3-nvidia-isaac/chapter2-isaac-ros-tools","draft":false,"unlisted":false,"editUrl":"https://github.com/alizah-fatima/physical-humanoid-ai-book/tree/main/docs/module3-nvidia-isaac/chapter2-isaac-ros-tools.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: Introduction to NVIDIA Isaac Sim","permalink":"/docs/module3-nvidia-isaac/chapter1-isaac-sim-intro"},"next":{"title":"Chapter 3: Nav2 and Path Planning for Bipedal Humanoid Movement","permalink":"/docs/module3-nvidia-isaac/chapter3-nav2-bipedal-planning"}}');var s=a(4848),t=a(8453);const r={sidebar_position:2},o="Chapter 2: Isaac ROS - Hardware-Accelerated Tools",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Isaac ROS Framework",id:"introduction-to-isaac-ros-framework",level:2},{value:"Isaac ROS Architecture",id:"isaac-ros-architecture",level:3},{value:"Isaac ROS Installation and Setup",id:"isaac-ros-installation-and-setup",level:2},{value:"System Requirements",id:"system-requirements",level:3},{value:"Installation Process",id:"installation-process",level:3},{value:"Verification and Testing",id:"verification-and-testing",level:3},{value:"Visual SLAM (VSLAM) with Isaac ROS",id:"visual-slam-vslam-with-isaac-ros",level:2},{value:"Overview of Isaac ROS Visual SLAM",id:"overview-of-isaac-ros-visual-slam",level:3},{value:"Basic VSLAM Setup",id:"basic-vslam-setup",level:3},{value:"Isaac ROS VSLAM Launch Configuration",id:"isaac-ros-vslam-launch-configuration",level:3},{value:"Isaac ROS Navigation and Perception Pipelines",id:"isaac-ros-navigation-and-perception-pipelines",level:2},{value:"GPU-Accelerated Navigation Stack",id:"gpu-accelerated-navigation-stack",level:3},{value:"Isaac ROS Perception Pipeline",id:"isaac-ros-perception-pipeline",level:3},{value:"CUDA and TensorRT Optimization",id:"cuda-and-tensorrt-optimization",level:2},{value:"GPU Memory Management",id:"gpu-memory-management",level:3},{value:"TensorRT Integration Example",id:"tensorrt-integration-example",level:3},{value:"Isaac ROS Hardware Acceleration Techniques",id:"isaac-ros-hardware-acceleration-techniques",level:2},{value:"CUDA Kernel Optimization",id:"cuda-kernel-optimization",level:3},{value:"Multi-Stream Processing",id:"multi-stream-processing",level:3},{value:"Isaac ROS for Humanoid Robotics Applications",id:"isaac-ros-for-humanoid-robotics-applications",level:2},{value:"Humanoid-Specific Perception",id:"humanoid-specific-perception",level:3},{value:"Humanoid Navigation with Isaac ROS",id:"humanoid-navigation-with-isaac-ros",level:3},{value:"Performance Optimization and Best Practices",id:"performance-optimization-and-best-practices",level:2},{value:"GPU Utilization Monitoring",id:"gpu-utilization-monitoring",level:3},{value:"Isaac ROS Best Practices",id:"isaac-ros-best-practices",level:3},{value:"Troubleshooting and Debugging",id:"troubleshooting-and-debugging",level:2},{value:"Common Isaac ROS Issues",id:"common-isaac-ros-issues",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-2-isaac-ros---hardware-accelerated-tools",children:"Chapter 2: Isaac ROS - Hardware-Accelerated Tools"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand the Isaac ROS framework and its hardware acceleration capabilities"}),"\n",(0,s.jsx)(n.li,{children:"Implement Visual SLAM (VSLAM) using Isaac ROS packages"}),"\n",(0,s.jsx)(n.li,{children:"Configure navigation and perception pipelines with GPU acceleration"}),"\n",(0,s.jsx)(n.li,{children:"Optimize perception algorithms using CUDA and TensorRT"}),"\n",(0,s.jsx)(n.li,{children:"Integrate Isaac ROS tools with humanoid robotics applications"}),"\n",(0,s.jsx)(n.li,{children:"Apply best practices for GPU-accelerated robotics processing"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-isaac-ros-framework",children:"Introduction to Isaac ROS Framework"}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS is a collection of GPU-accelerated perception and navigation packages designed to run on NVIDIA Jetson and discrete GPUs. It provides high-performance implementations of common robotics algorithms that leverage NVIDIA's CUDA, TensorRT, and other acceleration technologies."}),"\n",(0,s.jsx)(n.p,{children:"For humanoid robotics, Isaac ROS offers:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU-accelerated perception"}),": Real-time processing of sensor data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Visual SLAM"}),": Accurate localization and mapping"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Navigation"}),": GPU-accelerated path planning and obstacle avoidance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deep learning inference"}),": Optimized neural network execution"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-sensor fusion"}),": Efficient integration of various sensor modalities"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros-architecture",children:"Isaac ROS Architecture"}),"\n",(0,s.jsx)(n.p,{children:"The Isaac ROS framework consists of several key components:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   ROS 2 Nodes   \u2502    \u2502  GPU Compute    \u2502    \u2502  Accelerated    \u2502\n\u2502   (CPU)         \u2502    \u2502  (CUDA/RT)      \u2502    \u2502  Algorithms     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Perception      \u2502\u2500\u2500\u2500\u25b6\u2502 Stereo Disparity\u2502\u2500\u2500\u2500\u25b6\u2502 Depth Maps      \u2502\n\u2502 Navigation      \u2502    \u2502 Feature Extract \u2502    \u2502 Point Clouds    \u2502\n\u2502 SLAM            \u2502    \u2502 Image Rectify   \u2502    \u2502 Trajectories    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h2,{id:"isaac-ros-installation-and-setup",children:"Isaac ROS Installation and Setup"}),"\n",(0,s.jsx)(n.h3,{id:"system-requirements",children:"System Requirements"}),"\n",(0,s.jsx)(n.p,{children:"Before installing Isaac ROS, ensure your system meets the requirements:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU"}),": NVIDIA GPU with Compute Capability 6.0+ (RTX 2060 or better recommended)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Driver"}),": NVIDIA Driver 470+ with CUDA support"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CUDA"}),": CUDA 11.8+ and cuDNN 8.0+"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2"}),": Humble Hawksbill"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"OS"}),": Ubuntu 20.04/22.04 LTS"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"installation-process",children:"Installation Process"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Add NVIDIA package repositories\nsudo apt update\nsudo apt install -y software-properties-common\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb\nsudo dpkg -i cuda-keyring_1.0-1_all.deb\nsudo apt update\n\n# Install CUDA and cuDNN\nsudo apt install -y cuda-toolkit-11-8 libcudnn8 libcudnn8-dev\n\n# Install Isaac ROS packages\nsudo apt install ros-humble-isaac-ros-common\nsudo apt install ros-humble-isaac-ros-perception\nsudo apt install ros-humble-isaac-ros-nav2\nsudo apt install ros-humble-isaac-ros-buffers\nsudo apt install ros-humble-isaac-ros-gems\n"})}),"\n",(0,s.jsx)(n.h3,{id:"verification-and-testing",children:"Verification and Testing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Test Isaac ROS installation\nros2 run isaac_ros_test test_image_pipeline\n\n# Check available Isaac ROS packages\napt list --installed | grep isaac-ros\n"})}),"\n",(0,s.jsx)(n.h2,{id:"visual-slam-vslam-with-isaac-ros",children:"Visual SLAM (VSLAM) with Isaac ROS"}),"\n",(0,s.jsx)(n.h3,{id:"overview-of-isaac-ros-visual-slam",children:"Overview of Isaac ROS Visual SLAM"}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS Visual SLAM provides GPU-accelerated visual-inertial odometry and mapping. It leverages NVIDIA's hardware acceleration to achieve real-time performance for humanoid navigation."}),"\n",(0,s.jsx)(n.h3,{id:"basic-vslam-setup",children:"Basic VSLAM Setup"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, Imu\nfrom geometry_msgs.msg import PoseStamped\nfrom nav_msgs.msg import Odometry\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\n\nclass IsaacVSLAMNode(Node):\n    def __init__(self):\n        super().__init__('isaac_vsalm_node')\n\n        # Create subscribers for stereo camera and IMU\n        self.left_image_sub = self.create_subscription(\n            Image, '/camera/left/image_rect_color', self.left_image_callback, 10)\n        self.right_image_sub = self.create_subscription(\n            Image, '/camera/right/image_rect_color', self.right_image_callback, 10)\n        self.left_info_sub = self.create_subscription(\n            CameraInfo, '/camera/left/camera_info', self.left_info_callback, 10)\n        self.right_info_sub = self.create_subscription(\n            CameraInfo, '/camera/right/camera_info', self.right_info_callback, 10)\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10)\n\n        # Create publishers for pose and odometry\n        self.pose_pub = self.create_publisher(PoseStamped, '/visual_slam/pose', 10)\n        self.odom_pub = self.create_publisher(Odometry, '/visual_slam/odometry', 10)\n\n        self.bridge = CvBridge()\n        self.latest_left = None\n        self.latest_right = None\n        self.camera_info_left = None\n        self.camera_info_right = None\n        self.latest_imu = None\n\n        # Isaac ROS VSLAM parameters\n        self.baseline = 0.1  # Stereo baseline in meters\n        self.focal_length = 320.0  # Approximate focal length\n\n    def left_image_callback(self, msg):\n        self.latest_left = msg\n\n    def right_image_callback(self, msg):\n        self.latest_right = msg\n\n    def left_info_callback(self, msg):\n        self.camera_info_left = msg\n\n    def right_info_callback(self, msg):\n        self.camera_info_right = msg\n\n    def imu_callback(self, msg):\n        self.latest_imu = msg\n\n    def process_stereo_pair(self):\n        if not all([self.latest_left, self.latest_right, self.camera_info_left, self.camera_info_right]):\n            return None\n\n        # Convert ROS images to OpenCV\n        left_cv = self.bridge.imgmsg_to_cv2(self.latest_left, \"bgr8\")\n        right_cv = self.bridge.imgmsg_to_cv2(self.latest_right, \"bgr8\")\n\n        # Create stereo matcher (using OpenCV as example - Isaac ROS uses optimized CUDA implementation)\n        stereo = cv2.StereoSGBM_create(\n            minDisparity=0,\n            numDisparities=128,\n            blockSize=5,\n            P1=8 * 3 * 5 * 5,\n            P2=32 * 3 * 5 * 5,\n            disp12MaxDiff=1,\n            uniquenessRatio=15,\n            speckleWindowSize=0,\n            speckleRange=2,\n            preFilterCap=63,\n            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n        )\n\n        # Compute disparity (Isaac ROS does this on GPU)\n        disparity = stereo.compute(left_cv, right_cv).astype(np.float32) / 16.0\n\n        # Convert disparity to depth\n        depth = (self.focal_length * self.baseline) / (disparity + 1e-6)\n\n        return depth\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vsalm_node = IsaacVSLAMNode()\n\n    try:\n        rclpy.spin(vsalm_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vsalm_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros-vslam-launch-configuration",children:"Isaac ROS VSLAM Launch Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"\x3c!-- launch/isaac_vslam.launch.py --\x3e\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Stereo rectification\n        Node(\n            package='isaac_ros_image_proc',\n            executable='rectify_node',\n            name='left_rectify_node',\n            parameters=[{\n                'use_sensor_data_qos': True\n            }],\n            remappings=[\n                ('image_raw', '/camera/left/image_raw'),\n                ('camera_info', '/camera/left/camera_info'),\n                ('image_rect', '/camera/left/image_rect_color')\n            ]\n        ),\n\n        Node(\n            package='isaac_ros_image_proc',\n            executable='rectify_node',\n            name='right_rectify_node',\n            parameters=[{\n                'use_sensor_data_qos': True\n            }],\n            remappings=[\n                ('image_raw', '/camera/right/image_raw'),\n                ('camera_info', '/camera/right/camera_info'),\n                ('image_rect', '/camera/right/image_rect_color')\n            ]\n        ),\n\n        # Isaac ROS Stereo Disparity (GPU accelerated)\n        Node(\n            package='isaac_ros_stereo_image_proc',\n            executable='disparity_node',\n            name='disparity_node',\n            parameters=[{\n                'use_sensor_data_qos': True,\n                'StereoDisparityNode.gpu_allocator_type': 0  # 0 for CUDA\n            }],\n            remappings=[\n                ('left/image_rect', '/camera/left/image_rect_color'),\n                ('right/image_rect', '/camera/right/image_rect_color'),\n                ('left/camera_info', '/camera/left/camera_info'),\n                ('right/camera_info', '/camera/right/camera_info'),\n                ('disparity', '/disparity')\n            ]\n        ),\n\n        # Isaac ROS Point Cloud Generation\n        Node(\n            package='isaac_ros_stereo_image_proc',\n            executable='pointcloud_node',\n            name='pointcloud_node',\n            parameters=[{\n                'use_sensor_data_qos': True,\n                'PointCloududeNode.gpu_allocator_type': 0\n            }],\n            remappings=[\n                ('left/camera_info', '/camera/left/camera_info'),\n                ('right/camera_info', '/camera/right/camera_info'),\n                ('disparity', '/disparity'),\n                ('points', '/points')\n            ]\n        ),\n\n        # Isaac ROS Visual SLAM\n        Node(\n            package='isaac_ros_visual_slam',\n            executable='visual_slam_node',\n            name='visual_slam_node',\n            parameters=[{\n                'use_sim_time': True,\n                'enable_debug_mode': False,\n                'use_vio_initialization': True,\n                'enable_slam_visualization': True,\n                'enable_landmarks_view': True,\n                'enable_observations_view': True,\n                'map_frame': 'map',\n                'odom_frame': 'odom',\n                'base_frame': 'base_link',\n                'input_viz_points_topic_name': 'visual_slam_points',\n                'publish_odom_tf': True\n            }],\n            remappings=[\n                ('/visual_slam/imu', '/imu/data'),\n                ('/visual_slam/left/camera_info', '/camera/left/camera_info'),\n                ('/visual_slam/left/image', '/camera/left/image_rect_color'),\n                ('/visual_slam/right/camera_info', '/camera/right/camera_info'),\n                ('/visual_slam/right/image', '/camera/right/image_rect_color'),\n                ('/visual_slam/tracked_pose', '/visual_slam/pose'),\n                ('/visual_slam/visual_odometry', '/visual_slam/odometry'),\n                ('/visual_slam/path', '/visual_slam/path'),\n                ('/visual_slam/landmarks', '/visual_slam/landmarks'),\n                ('/visual_slam/keyframes', '/visual_slam/keyframes'),\n                ('/visual_slam/mapped_points', '/visual_slam/mapped_points')\n            ]\n        )\n    ])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"isaac-ros-navigation-and-perception-pipelines",children:"Isaac ROS Navigation and Perception Pipelines"}),"\n",(0,s.jsx)(n.h3,{id:"gpu-accelerated-navigation-stack",children:"GPU-Accelerated Navigation Stack"}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS provides GPU-accelerated navigation components that work with Nav2:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# config/isaac_nav2_params.yaml\namcl:\n  ros__parameters:\n    use_sim_time: True\n    alpha1: 0.2\n    alpha2: 0.2\n    alpha3: 0.2\n    alpha4: 0.2\n    alpha5: 0.2\n    base_frame_id: "base_link"\n    beam_skip_distance: 0.5\n    beam_skip_error_threshold: 0.9\n    beam_skip_threshold: 0.3\n    do_beamskip: false\n    global_frame_id: "map"\n    lambda_short: 0.1\n    laser_likelihood_max_dist: 2.0\n    laser_max_range: 100.0\n    laser_min_range: -1.0\n    laser_model_type: "likelihood_field"\n    max_beams: 60\n    max_particles: 2000\n    min_particles: 500\n    odom_frame_id: "odom"\n    pf_err: 0.05\n    pf_z: 0.5\n    recovery_alpha_fast: 0.0\n    recovery_alpha_slow: 0.0\n    resample_interval: 1\n    robot_model_type: "nav2_amcl::DifferentialMotionModel"\n    save_pose_rate: 0.5\n    sigma_hit: 0.2\n    tf_broadcast: true\n    transform_tolerance: 1.0\n    update_min_a: 0.2\n    update_min_d: 0.25\n    z_hit: 0.5\n    z_max: 0.05\n    z_rand: 0.5\n    z_short: 0.05\n\nbt_navigator:\n  ros__parameters:\n    use_sim_time: True\n    global_frame: map\n    robot_base_frame: base_link\n    odom_topic: /odom\n    bt_loop_duration: 10\n    default_server_timeout: 20\n    enable_groot_monitoring: True\n    groot_zmq_publisher_port: 1666\n    groot_zmq_server_port: 1667\n    default_nav_to_pose_bt_xml: "navigate_to_pose_w_replanning_and_recovery.xml"\n\ncontroller_server:\n  ros__parameters:\n    use_sim_time: True\n    controller_frequency: 20.0\n    min_x_velocity_threshold: 0.001\n    min_y_velocity_threshold: 0.5\n    min_theta_velocity_threshold: 0.001\n    progress_checker_plugin: "progress_checker"\n    goal_checker_plugin: "goal_checker"\n    controller_plugins: ["FollowPath"]\n\n    # Isaac ROS MPPI Controller (GPU accelerated)\n    FollowPath:\n      plugin: "nav2_mppi_controller::MPPIController"\n      time_steps: 50\n      control_horizon: 1.0\n      model_dt: 0.05\n      vx_std: 0.2\n      vy_std: 0.1\n      wz_std: 0.3\n      iteration_count: 100\n      temperature: 0.3\n      track_target_heading: True\n      transform_tolerance: 0.3\n      xy_goal_tolerance: 0.25\n      yaw_goal_tolerance: 0.2\n      state_reset: True\n      publish_cost_grid_pc: False\n      progress_checker: "progress_checker"\n      goal_checker: "goal_checker"\n      # Isaac ROS acceleration\n      use_gpu: True\n      gpu_id: 0\n\nlocal_costmap:\n  ros__parameters:\n    use_sim_time: True\n    update_frequency: 5.0\n    publish_frequency: 2.0\n    global_frame: odom\n    robot_base_frame: base_link\n    rolling_window: true\n    width: 6\n    height: 6\n    resolution: 0.05\n    robot_radius: 0.3\n    plugins: ["voxel_layer", "inflation_layer"]\n    inflation_layer:\n      plugin: "nav2_costmap_2d::InflationLayer"\n      cost_scaling_factor: 3.0\n      inflation_radius: 0.55\n    voxel_layer:\n      plugin: "nav2_costmap_2d::VoxelLayer"\n      enabled: True\n      publish_voxel_map: False\n      origin_z: 0.0\n      z_resolution: 0.2\n      z_voxels: 10\n      max_obstacle_height: 2.0\n      mark_threshold: 0\n      observation_sources: pointcloud\n      pointcloud:\n        topic: /points\n        max_obstacle_height: 2.0\n        clearing: True\n        marking: True\n        data_type: "PointCloud2"\n        queue_size: 10\n        expected_update_rate: 0.0\n        observation_persistence: 0.0\n        max_obstacle_range: 3.0\n        min_obstacle_range: 0.1\n\nglobal_costmap:\n  ros__parameters:\n    use_sim_time: True\n    global_frame: map\n    robot_base_frame: base_link\n    update_frequency: 1.0\n    publish_frequency: 1.0\n    static_map: true\n    rolling_window: false\n    width: 20\n    height: 20\n    resolution: 0.05\n    robot_radius: 0.3\n    plugins: ["static_layer", "obstacle_layer", "inflation_layer"]\n    obstacle_layer:\n      plugin: "nav2_costmap_2d::ObstacleLayer"\n      enabled: True\n      observation_sources: pointcloud\n      pointcloud:\n        topic: /points\n        max_obstacle_height: 2.0\n        clearing: True\n        marking: True\n        data_type: "PointCloud2"\n        queue_size: 10\n        expected_update_rate: 0.0\n        observation_persistence: 0.0\n        max_obstacle_range: 3.0\n        min_obstacle_range: 0.1\n    static_layer:\n      plugin: "nav2_costmap_2d::StaticLayer"\n      map_subscribe_transient_local: True\n    inflation_layer:\n      plugin: "nav2_costmap_2d::InflationLayer"\n      cost_scaling_factor: 3.0\n      inflation_radius: 0.55\n'})}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros-perception-pipeline",children:"Isaac ROS Perception Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom vision_msgs.msg import Detection2DArray\nfrom geometry_msgs.msg import Point\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\nimport torch\nimport torchvision.transforms as transforms\n\nclass IsaacPerceptionPipeline(Node):\n    def __init__(self):\n        super().__init__('isaac_perception_pipeline')\n\n        # Create subscribers for camera input\n        self.image_sub = self.create_subscription(\n            Image, '/camera/rgb/image_raw', self.image_callback, 10)\n        self.info_sub = self.create_subscription(\n            CameraInfo, '/camera/rgb/camera_info', self.info_callback, 10)\n\n        # Create publisher for detections\n        self.detection_pub = self.create_publisher(\n            Detection2DArray, '/perception/detections', 10)\n\n        self.bridge = CvBridge()\n        self.camera_info = None\n\n        # Load Isaac ROS optimized detection model (TensorRT)\n        self.load_detection_model()\n\n    def load_detection_model(self):\n        \"\"\"Load optimized detection model for GPU inference\"\"\"\n        try:\n            # This would typically load a TensorRT optimized model\n            # For demonstration, using a placeholder\n            self.get_logger().info(\"Loading Isaac ROS optimized detection model...\")\n            # In practice, this would load a TensorRT engine\n            self.detection_model = None  # Placeholder\n        except Exception as e:\n            self.get_logger().error(f\"Failed to load detection model: {e}\")\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera image with GPU acceleration\"\"\"\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n\n            # Perform GPU-accelerated detection\n            detections = self.detect_objects_gpu(cv_image)\n\n            # Publish detections\n            self.publish_detections(detections, msg.header)\n\n        except Exception as e:\n            self.get_logger().error(f\"Error processing image: {e}\")\n\n    def detect_objects_gpu(self, image):\n        \"\"\"GPU-accelerated object detection\"\"\"\n        # This would use Isaac ROS optimized detection\n        # For demonstration, using OpenCV as a placeholder\n        detections = []\n\n        # Convert image for processing\n        # In Isaac ROS, this would use CUDA/TensorRT optimized operations\n        height, width = image.shape[:2]\n\n        # Example detection results\n        for i in range(3):  # Simulated detections\n            detection = {\n                'bbox': [i * 100, i * 50, 100, 80],  # [x, y, width, height]\n                'class': 'object',\n                'confidence': 0.8 + (i * 0.05),\n                'center': [i * 100 + 50, i * 50 + 40]\n            }\n            detections.append(detection)\n\n        return detections\n\n    def publish_detections(self, detections, header):\n        \"\"\"Publish detection results\"\"\"\n        detection_msg = Detection2DArray()\n        detection_msg.header = header\n\n        for detection in detections:\n            # Create detection message\n            vision_detection = Detection2D()\n            vision_detection.header = header\n\n            # Bounding box\n            bbox = vision_detection.bbox\n            bbox.center.position.x = detection['center'][0]\n            bbox.center.position.y = detection['center'][1]\n            bbox.size_x = detection['bbox'][2]\n            bbox.size_y = detection['bbox'][3]\n\n            # Results\n            result = vision_detection.results.add()\n            result.hypothesis.class_id = detection['class']\n            result.hypothesis.score = detection['confidence']\n\n            detection_msg.detections.append(vision_detection)\n\n        self.detection_pub.publish(detection_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    perception_pipeline = IsaacPerceptionPipeline()\n\n    try:\n        rclpy.spin(perception_pipeline)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        perception_pipeline.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"cuda-and-tensorrt-optimization",children:"CUDA and TensorRT Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"gpu-memory-management",children:"GPU Memory Management"}),"\n",(0,s.jsx)(n.p,{children:"Efficient GPU memory management is crucial for Isaac ROS performance:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# GPU memory management utilities\nimport rclpy\nfrom rclpy.node import Node\nimport numpy as np\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nfrom pycuda.compiler import SourceModule\n\nclass GPUMemoryManager:\n    def __init__(self):\n        # Initialize CUDA context\n        self.ctx = cuda.Device(0).make_context()\n        self.memory_pool = {}\n        self.allocated_memory = 0\n        self.max_memory = 0  # Will be set based on GPU capacity\n\n    def allocate_tensor(self, shape, dtype=np.float32):\n        """Allocate GPU memory for tensor"""\n        # Calculate memory size\n        element_size = np.dtype(dtype).itemsize\n        size_bytes = np.prod(shape) * element_size\n\n        # Allocate GPU memory\n        gpu_mem = cuda.mem_alloc(size_bytes)\n        self.allocated_memory += size_bytes\n\n        # Store reference\n        tensor_key = f"tensor_{id(gpu_mem)}"\n        self.memory_pool[tensor_key] = {\n            \'ptr\': gpu_mem,\n            \'shape\': shape,\n            \'dtype\': dtype,\n            \'size_bytes\': size_bytes\n        }\n\n        return gpu_mem, tensor_key\n\n    def free_tensor(self, tensor_key):\n        """Free GPU memory for tensor"""\n        if tensor_key in self.memory_pool:\n            tensor_info = self.memory_pool[tensor_key]\n            tensor_info[\'ptr\'].free()\n            self.allocated_memory -= tensor_info[\'size_bytes\']\n            del self.memory_pool[tensor_key]\n\n    def get_memory_usage(self):\n        """Get current GPU memory usage"""\n        return {\n            \'allocated\': self.allocated_memory,\n            \'pool_size\': len(self.memory_pool),\n            \'usage_percent\': (self.allocated_memory / self.max_memory) * 100 if self.max_memory > 0 else 0\n        }\n'})}),"\n",(0,s.jsx)(n.h3,{id:"tensorrt-integration-example",children:"TensorRT Integration Example"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# TensorRT integration for Isaac ROS\nimport tensorrt as trt\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nimport numpy as np\n\nclass TensorRTInference:\n    def __init__(self, engine_path):\n        self.engine_path = engine_path\n        self.runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))\n        self.engine = self.load_engine()\n        self.context = self.engine.create_execution_context()\n\n        # Allocate buffers\n        self.input_buffer = None\n        self.output_buffer = None\n        self.cuda_input = None\n        self.cuda_output = None\n        self.allocate_buffers()\n\n    def load_engine(self):\n        """Load TensorRT engine from file"""\n        with open(self.engine_path, \'rb\') as f:\n            serialized_engine = f.read()\n        return self.runtime.deserialize_cuda_engine(serialized_engine)\n\n    def allocate_buffers(self):\n        """Allocate input and output buffers for inference"""\n        for binding in self.engine:\n            if self.engine.binding_is_input(binding):\n                input_shape = self.engine.get_binding_shape(binding)\n                input_size = trt.volume(input_shape) * self.engine.max_batch_size * np.dtype(np.float32).itemsize\n                self.cuda_input = cuda.mem_alloc(input_size)\n                self.input_buffer = np.empty(input_shape, dtype=np.float32)\n            else:\n                output_shape = self.engine.get_binding_shape(binding)\n                output_size = trt.volume(output_shape) * self.engine.max_batch_size * np.dtype(np.float32).itemsize\n                self.cuda_output = cuda.mem_alloc(output_size)\n                self.output_buffer = np.empty(output_shape, dtype=np.float32)\n\n    def infer(self, input_data):\n        """Perform inference using TensorRT"""\n        # Copy input data to GPU\n        cuda.memcpy_htod(self.cuda_input, input_data.astype(np.float32))\n\n        # Run inference\n        bindings = [int(self.cuda_input), int(self.cuda_output)]\n        self.context.execute_v2(bindings)\n\n        # Copy output data from GPU\n        cuda.memcpy_dtoh(self.output_buffer, self.cuda_output)\n\n        return self.output_buffer\n\n# Isaac ROS node using TensorRT\nclass IsaacTensorRTNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_tensorrt_node\')\n\n        # Initialize TensorRT inference\n        self.tensorrt_inference = TensorRTInference(\'/path/to/model.engine\')\n\n        # Create subscribers and publishers\n        self.subscriber = self.create_subscription(\n            Image, \'/camera/rgb/image_raw\', self.image_callback, 10)\n        self.publisher = self.create_publisher(\n            Detection2DArray, \'/tensorrt_detections\', 10)\n\n        self.bridge = CvBridge()\n\n    def image_callback(self, msg):\n        """Process image using TensorRT acceleration"""\n        try:\n            # Convert ROS image to numpy array\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n\n            # Preprocess image for TensorRT model\n            processed_image = self.preprocess_image(cv_image)\n\n            # Run inference\n            results = self.tensorrt_inference.infer(processed_image)\n\n            # Process results and publish\n            detections = self.process_results(results)\n            self.publish_detections(detections, msg.header)\n\n        except Exception as e:\n            self.get_logger().error(f"TensorRT inference error: {e}")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"isaac-ros-hardware-acceleration-techniques",children:"Isaac ROS Hardware Acceleration Techniques"}),"\n",(0,s.jsx)(n.h3,{id:"cuda-kernel-optimization",children:"CUDA Kernel Optimization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Example CUDA kernel for image processing\nmod = SourceModule("""\n__global__ void image_processing_kernel(float *input, float *output, int width, int height)\n{\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int idy = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (idx < width && idy < height) {\n        int index = idy * width + idx;\n\n        // Example processing: simple brightness adjustment\n        float brightness_factor = 1.2;\n        output[index] = input[index] * brightness_factor;\n    }\n}\n""")\n\nimage_processing_kernel = mod.get_function("image_processing_kernel")\n\ndef cuda_image_processing(input_image):\n    """Process image using CUDA kernel"""\n    height, width = input_image.shape[:2]\n\n    # Allocate GPU memory\n    input_gpu = cuda.mem_alloc(input_image.nbytes)\n    output_gpu = cuda.mem_alloc(input_image.nbytes)\n\n    # Copy input to GPU\n    cuda.memcpy_htod(input_gpu, input_image.astype(np.float32))\n\n    # Define block and grid dimensions\n    block_size = (16, 16, 1)\n    grid_size = ((width + block_size[0] - 1) // block_size[0],\n                 (height + block_size[1] - 1) // block_size[1])\n\n    # Execute kernel\n    image_processing_kernel(\n        input_gpu, output_gpu, np.int32(width), np.int32(height),\n        block=block_size, grid=grid_size\n    )\n\n    # Copy result back to CPU\n    output_image = np.empty_like(input_image, dtype=np.float32)\n    cuda.memcpy_dtoh(output_image, output_gpu)\n\n    # Cleanup\n    input_gpu.free()\n    output_gpu.free()\n\n    return output_image\n'})}),"\n",(0,s.jsx)(n.h3,{id:"multi-stream-processing",children:"Multi-Stream Processing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class MultiStreamProcessor:\n    def __init__(self, num_streams=4):\n        self.num_streams = num_streams\n        self.streams = [cuda.Stream() for _ in range(num_streams)]\n        self.current_stream = 0\n\n    def process_frame_async(self, frame_data):\n        """Process frame asynchronously using multiple CUDA streams"""\n        stream = self.streams[self.current_stream]\n\n        # Allocate GPU memory with stream\n        gpu_data = cuda.mem_alloc_async(frame_data.nbytes, stream)\n        cuda.memcpy_htod_async(gpu_data, frame_data, stream)\n\n        # Process with kernel using stream\n        # ... processing operations ...\n\n        # Copy result back asynchronously\n        result = np.empty_like(frame_data)\n        cuda.memcpy_dtoh_async(result, gpu_data, stream)\n\n        # Synchronize stream\n        stream.synchronize()\n\n        # Move to next stream\n        self.current_stream = (self.current_stream + 1) % self.num_streams\n\n        return result\n'})}),"\n",(0,s.jsx)(n.h2,{id:"isaac-ros-for-humanoid-robotics-applications",children:"Isaac ROS for Humanoid Robotics Applications"}),"\n",(0,s.jsx)(n.h3,{id:"humanoid-specific-perception",children:"Humanoid-Specific Perception"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class HumanoidPerceptionPipeline:\n    def __init__(self, node):\n        self.node = node\n        self.human_detector = self.initialize_human_detector()\n        self.gesture_recognizer = self.initialize_gesture_recognizer()\n        self.environment_mapper = self.initialize_environment_mapper()\n\n    def initialize_human_detector(self):\n        """Initialize human detection optimized for Isaac ROS"""\n        # This would load an Isaac ROS optimized human detection model\n        return {\n            \'model\': \'isaac_ros_human_detection\',\n            \'confidence_threshold\': 0.7,\n            \'max_detection_distance\': 10.0\n        }\n\n    def detect_humans(self, image_data):\n        """Detect humans in environment using GPU acceleration"""\n        # Isaac ROS optimized human detection\n        # Returns list of human detections with poses\n        detections = []\n\n        # Example processing (in practice, this would use Isaac ROS optimized functions)\n        # This could detect humans, estimate poses, recognize gestures\n        return detections\n\n    def recognize_gestures(self, image_data, human_poses):\n        """Recognize human gestures using Isaac ROS pipeline"""\n        # Isaac ROS optimized gesture recognition\n        gestures = []\n\n        # Process each detected human\n        for pose in human_poses:\n            # Recognize gestures using GPU-accelerated models\n            gesture = self.process_gesture(image_data, pose)\n            if gesture:\n                gestures.append(gesture)\n\n        return gestures\n\n    def process_gesture(self, image_data, human_pose):\n        """Process individual gesture recognition"""\n        # Extract human region\n        # Apply gesture recognition model\n        # Return recognized gesture\n        return None  # Placeholder\n'})}),"\n",(0,s.jsx)(n.h3,{id:"humanoid-navigation-with-isaac-ros",children:"Humanoid Navigation with Isaac ROS"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class HumanoidNavigation:\n    def __init__(self, node):\n        self.node = node\n        self.local_planner = self.initialize_local_planner()\n        self.global_planner = self.initialize_global_planner()\n        self.footstep_planner = self.initialize_footstep_planner()\n\n    def initialize_local_planner(self):\n        """Initialize local planner for humanoid navigation"""\n        return {\n            \'type\': \'isaac_ros_mppi_controller\',\n            \'use_gpu\': True,\n            \'control_horizon\': 1.0,\n            \'time_steps\': 50\n        }\n\n    def plan_footsteps(self, path, robot_state):\n        """Plan footsteps for bipedal navigation"""\n        # Isaac ROS optimized footstep planning\n        # Takes into account humanoid kinematics and balance\n        footsteps = []\n\n        # Generate footstep plan based on path and robot state\n        # Consider balance, step constraints, and terrain\n        return footsteps\n\n    def navigate_with_avoidance(self, goal_pose, robot_state):\n        """Navigate to goal with dynamic obstacle avoidance"""\n        # Use Isaac ROS collision avoidance\n        # Adapt for humanoid-specific constraints\n        plan = self.generate_navigation_plan(goal_pose, robot_state)\n        return plan\n\n    def generate_navigation_plan(self, goal_pose, robot_state):\n        """Generate navigation plan considering humanoid constraints"""\n        # Plan path that accounts for humanoid morphology\n        # Step size limits, balance constraints, etc.\n        return []\n'})}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization-and-best-practices",children:"Performance Optimization and Best Practices"}),"\n",(0,s.jsx)(n.h3,{id:"gpu-utilization-monitoring",children:"GPU Utilization Monitoring"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import pynvml\n\nclass GPUResourceManager:\n    def __init__(self):\n        pynvml.nvmlInit()\n        self.device_count = pynvml.nvmlDeviceGetCount()\n\n    def get_gpu_utilization(self, device_id=0):\n        """Get current GPU utilization"""\n        handle = pynvml.nvmlDeviceGetHandleByIndex(device_id)\n        util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n        return util.gpu\n\n    def get_gpu_memory_info(self, device_id=0):\n        """Get GPU memory information"""\n        handle = pynvml.nvmlDeviceGetHandleByIndex(device_id)\n        memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n        return {\n            \'total\': memory_info.total,\n            \'used\': memory_info.used,\n            \'free\': memory_info.free,\n            \'utilization_percent\': (memory_info.used / memory_info.total) * 100\n        }\n\n    def optimize_for_gpu(self, node):\n        """Optimize node configuration for GPU usage"""\n        # Set appropriate QoS profiles\n        # Configure memory allocation\n        # Set GPU-specific parameters\n        pass\n'})}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros-best-practices",children:"Isaac ROS Best Practices"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory Management"}),": Always use CUDA memory pools and proper cleanup"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stream Processing"}),": Use multiple CUDA streams for parallel processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Batch Processing"}),": Process data in batches when possible"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Optimization"}),": Use TensorRT for neural network inference"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"QoS Configuration"}),": Use appropriate QoS settings for real-time performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Error Handling"}),": Implement proper error handling for GPU operations"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting-and-debugging",children:"Troubleshooting and Debugging"}),"\n",(0,s.jsx)(n.h3,{id:"common-isaac-ros-issues",children:"Common Isaac ROS Issues"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"GPU Memory Issues"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Check GPU memory usage\nnvidia-smi\n\n# Increase GPU memory limits if needed\necho \'options nvidia "NVreg_RegistryDwords=\\"PerfLevelSrc=0x2222\\""\' | sudo tee -a /etc/modprobe.d/nvidia.conf\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"CUDA Context Issues"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Ensure proper CUDA context management\nimport pycuda.driver as cuda\ncuda.init()\ndevice = cuda.Device(0)\nctx = device.make_context()\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Performance Bottlenecks"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Monitor GPU utilization with ",(0,s.jsx)(n.code,{children:"nvidia-smi"})]}),"\n",(0,s.jsx)(n.li,{children:"Use TensorRT for neural network acceleration"}),"\n",(0,s.jsx)(n.li,{children:"Optimize data transfers between CPU and GPU"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"This chapter covered Isaac ROS hardware-accelerated tools for humanoid robotics:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Isaac ROS framework architecture and installation"}),"\n",(0,s.jsx)(n.li,{children:"Visual SLAM implementation with GPU acceleration"}),"\n",(0,s.jsx)(n.li,{children:"Navigation and perception pipeline configuration"}),"\n",(0,s.jsx)(n.li,{children:"CUDA and TensorRT optimization techniques"}),"\n",(0,s.jsx)(n.li,{children:"Multi-stream processing for performance"}),"\n",(0,s.jsx)(n.li,{children:"Humanoid-specific perception and navigation"}),"\n",(0,s.jsx)(n.li,{children:"Performance optimization best practices"}),"\n",(0,s.jsx)(n.li,{children:"Troubleshooting common issues"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS provides powerful GPU-accelerated capabilities that significantly improve the performance of perception and navigation algorithms for humanoid robots. By leveraging NVIDIA's hardware acceleration technologies, developers can achieve real-time performance for complex robotics applications."})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,a){a.d(n,{R:()=>r,x:()=>o});var i=a(6540);const s={},t=i.createContext(s);function r(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);