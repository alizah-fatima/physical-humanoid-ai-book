"use strict";(globalThis.webpackChunkphysical_humanoid_ai_book=globalThis.webpackChunkphysical_humanoid_ai_book||[]).push([[681],{3097(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module3-nvidia-isaac/chapter1-isaac-sim-intro","title":"Chapter 1: Introduction to NVIDIA Isaac Sim","description":"Learning Objectives","source":"@site/docs/module3-nvidia-isaac/chapter1-isaac-sim-intro.md","sourceDirName":"module3-nvidia-isaac","slug":"/module3-nvidia-isaac/chapter1-isaac-sim-intro","permalink":"/physical-humanoid-ai-book/docs/module3-nvidia-isaac/chapter1-isaac-sim-intro","draft":false,"unlisted":false,"editUrl":"https://github.com/alizah-fatima/physical-humanoid-ai-book/tree/main/docs/module3-nvidia-isaac/chapter1-isaac-sim-intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Sensor Simulation and Unity Integration for High-Fidelity Rendering","permalink":"/physical-humanoid-ai-book/docs/module2-digital-twin/chapter3-sensor-unity-integration"},"next":{"title":"Chapter 2: Isaac ROS - Hardware-Accelerated Tools","permalink":"/physical-humanoid-ai-book/docs/module3-nvidia-isaac/chapter2-isaac-ros-tools"}}');var t=i(4848),r=i(8453);const s={sidebar_position:1},o="Chapter 1: Introduction to NVIDIA Isaac Sim",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to NVIDIA Isaac Sim",id:"introduction-to-nvidia-isaac-sim",level:2},{value:"Installation and Setup",id:"installation-and-setup",level:2},{value:"System Requirements",id:"system-requirements",level:3},{value:"Installation Process",id:"installation-process",level:3},{value:"Method 1: Omniverse Launcher (Recommended)",id:"method-1-omniverse-launcher-recommended",level:4},{value:"Method 2: Docker (Alternative)",id:"method-2-docker-alternative",level:4},{value:"Initial Configuration",id:"initial-configuration",level:3},{value:"Photorealistic Simulation Fundamentals",id:"photorealistic-simulation-fundamentals",level:2},{value:"USD (Universal Scene Description)",id:"usd-universal-scene-description",level:3},{value:"Rendering Pipeline",id:"rendering-pipeline",level:3},{value:"Lighting Configuration",id:"lighting-configuration",level:4},{value:"Material Properties",id:"material-properties",level:4},{value:"Synthetic Data Generation",id:"synthetic-data-generation",level:2},{value:"Camera Sensor Setup",id:"camera-sensor-setup",level:3},{value:"Ground Truth Data Generation",id:"ground-truth-data-generation",level:3},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"Advanced Perception Training Capabilities",id:"advanced-perception-training-capabilities",level:2},{value:"Sensor Simulation",id:"sensor-simulation",level:3},{value:"Training Data Pipeline",id:"training-data-pipeline",level:3},{value:"Isaac Sim-ROS Integration",id:"isaac-sim-ros-integration",level:2},{value:"Setting up the Bridge",id:"setting-up-the-bridge",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"GPU Resource Management",id:"gpu-resource-management",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"Rendering Issues",id:"rendering-issues",level:3},{value:"Physics Issues",id:"physics-issues",level:3},{value:"Sensor Issues",id:"sensor-issues",level:3},{value:"Summary",id:"summary",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-1-introduction-to-nvidia-isaac-sim",children:"Chapter 1: Introduction to NVIDIA Isaac Sim"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand the fundamentals of NVIDIA Isaac Sim platform"}),"\n",(0,t.jsx)(n.li,{children:"Set up and configure Isaac Sim for humanoid robotics simulation"}),"\n",(0,t.jsx)(n.li,{children:"Create photorealistic simulation environments"}),"\n",(0,t.jsx)(n.li,{children:"Generate synthetic data for perception training"}),"\n",(0,t.jsx)(n.li,{children:"Implement advanced perception and training capabilities"}),"\n",(0,t.jsx)(n.li,{children:"Integrate Isaac Sim with Isaac ROS ecosystem"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-nvidia-isaac-sim",children:"Introduction to NVIDIA Isaac Sim"}),"\n",(0,t.jsx)(n.p,{children:"NVIDIA Isaac Sim is a powerful, photorealistic simulation application built on NVIDIA's Omniverse platform. It provides a comprehensive environment for developing, testing, and training robotics applications with physically accurate simulation, advanced rendering capabilities, and seamless integration with the Isaac ROS ecosystem."}),"\n",(0,t.jsx)(n.p,{children:"For humanoid robotics, Isaac Sim offers:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Photorealistic rendering"}),": RTX-accelerated ray tracing for realistic lighting and materials"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Physically accurate simulation"}),": PhysX engine for realistic physics interactions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Synthetic data generation"}),": High-quality training data with ground truth annotations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Domain randomization"}),": Techniques to improve sim-to-real transfer"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Omniverse integration"}),": Collaborative development and asset sharing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPU acceleration"}),": Hardware-accelerated rendering and physics computation"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"installation-and-setup",children:"Installation and Setup"}),"\n",(0,t.jsx)(n.h3,{id:"system-requirements",children:"System Requirements"}),"\n",(0,t.jsx)(n.p,{children:"Before installing Isaac Sim, ensure your system meets the requirements:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPU"}),": NVIDIA GPU with Compute Capability 6.0 or higher (GTX 1060/RTX 2060 or better recommended)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"OS"}),": Ubuntu 20.04/22.04 LTS or Windows 10/11"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"RAM"}),": 16GB minimum (32GB+ recommended)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Storage"}),": 50GB+ free space"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Driver"}),": NVIDIA Driver 470+ with CUDA support"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2"}),": Humble Hawksbill or newer"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"installation-process",children:"Installation Process"}),"\n",(0,t.jsx)(n.h4,{id:"method-1-omniverse-launcher-recommended",children:"Method 1: Omniverse Launcher (Recommended)"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Download and install NVIDIA Omniverse Launcher from the NVIDIA Developer website"}),"\n",(0,t.jsx)(n.li,{children:"Sign in with your NVIDIA Developer account"}),"\n",(0,t.jsx)(n.li,{children:'Search for "Isaac Sim" in the app library'}),"\n",(0,t.jsx)(n.li,{children:'Click "Install" to download and install Isaac Sim'}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"method-2-docker-alternative",children:"Method 2: Docker (Alternative)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Pull the Isaac Sim Docker image\ndocker pull nvcr.io/nvidia/isaac-sim:4.0.0\n\n# Run Isaac Sim container\ndocker run --gpus all -it --rm \\\n  --network=host \\\n  --env "ACCEPT_EULA=Y" \\\n  --env "NVIDIA_VISIBLE_DEVICES=all" \\\n  --volume "$PWD:/workspace" \\\n  --volume "$HOME/.Xauthority:/root/.Xauthority:rw" \\\n  --volume "/tmp/.X11-unix:/tmp/.X11-unix:rw" \\\n  --volume "/dev/shm:/dev/shm" \\\n  --volume "/tmp/.docker.xauth:/tmp/.docker.xauth" \\\n  nvcr.io/nvidia/isaac-sim:4.0.0\n'})}),"\n",(0,t.jsx)(n.h3,{id:"initial-configuration",children:"Initial Configuration"}),"\n",(0,t.jsx)(n.p,{children:"After installation, configure Isaac Sim for humanoid robotics:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Launch Isaac Sim"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"isaac-sim\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Verify GPU acceleration"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Go to Window \u2192 Compute \u2192 RTX Renderer"}),"\n",(0,t.jsx)(n.li,{children:"Check that rendering is GPU-accelerated"}),"\n",(0,t.jsx)(n.li,{children:"Verify PhysX is enabled for physics simulation"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Install Isaac Sim Python API"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# In Isaac Sim's Python environment\npython -m pip install omni.isaac.core\npython -m pip install omni.isaac.sensor\npython -m pip install omni.isaac.motion_generation\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"photorealistic-simulation-fundamentals",children:"Photorealistic Simulation Fundamentals"}),"\n",(0,t.jsx)(n.h3,{id:"usd-universal-scene-description",children:"USD (Universal Scene Description)"}),"\n",(0,t.jsx)(n.p,{children:"Isaac Sim uses USD as its core scene description format. USD enables collaborative workflows and efficient asset sharing:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Basic USD scene creation\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\n\n# Create world instance\nworld = World(stage_units_in_meters=1.0)\n\n# Get Isaac Sim assets root\nassets_root_path = get_assets_root_path()\n\n# Add a humanoid robot to the scene\nhumanoid_path = assets_root_path + "/Isaac/Robots/Humanoid/humanoid_instanceable.usd"\nadd_reference_to_stage(usd_path=humanoid_path, prim_path="/World/Humanoid")\n\n# Add ground plane\nadd_reference_to_stage(\n    usd_path=assets_root_path + "/Isaac/Environments/Simple_Room/simple_room.usd",\n    prim_path="/World/Room"\n)\n\n# Reset world and run simulation\nworld.reset()\nfor i in range(100):\n    world.step(render=True)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"rendering-pipeline",children:"Rendering Pipeline"}),"\n",(0,t.jsx)(n.p,{children:"Isaac Sim leverages NVIDIA's RTX technology for photorealistic rendering:"}),"\n",(0,t.jsx)(n.h4,{id:"lighting-configuration",children:"Lighting Configuration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Create physically-based lighting\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom pxr import Gf\n\n# Add dome light (environment lighting)\ndome_light = get_prim_at_path("/World/DomeLight")\nif dome_light:\n    dome_light.GetAttribute("inputs:color").Set(Gf.Vec3f(0.5, 0.5, 0.5))\n    dome_light.GetAttribute("inputs:intensity").Set(3000)\n\n# Add directional light\ndirectional_light = get_prim_at_path("/World/DirectionalLight")\nif directional_light:\n    directional_light.GetAttribute("inputs:color").Set(Gf.Vec3f(1.0, 0.9, 0.8))\n    directional_light.GetAttribute("inputs:intensity").Set(1000)\n'})}),"\n",(0,t.jsx)(n.h4,{id:"material-properties",children:"Material Properties"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Apply physically-based materials\nfrom omni.isaac.core.utils.prims import create_primitive\nfrom omni.isaac.core.materials import OmniPBR\n\n# Create a primitive with PBR material\nsphere = create_primitive(\n    prim_path="/World/Sphere",\n    primitive_type="Sphere",\n    position=[1.0, 0.0, 0.5],\n    scale=[0.2, 0.2, 0.2],\n    orientation=[0.0, 0.0, 0.0, 1.0]\n)\n\n# Apply metallic material\nmetal_material = OmniPBR(\n    prim_path="/World/Looks/Metal",\n    color=(0.7, 0.7, 0.8),\n    metallic=0.9,\n    roughness=0.1\n)\nmetal_material.apply([sphere])\n'})}),"\n",(0,t.jsx)(n.h2,{id:"synthetic-data-generation",children:"Synthetic Data Generation"}),"\n",(0,t.jsx)(n.h3,{id:"camera-sensor-setup",children:"Camera Sensor Setup"}),"\n",(0,t.jsx)(n.p,{children:"For synthetic data generation, configure high-quality camera sensors:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Set up RGB and depth cameras\nfrom omni.isaac.sensor import Camera\nimport numpy as np\n\n# Create RGB camera\nrgb_camera = Camera(\n    prim_path="/World/Humanoid/Head/Camera",\n    frequency=30,\n    resolution=(640, 480)\n)\n\n# Set camera properties for photorealistic output\nrgb_camera.set_focal_length(24.0)\nrgb_camera.set_horizontal_aperture(20.955)\nrgb_camera.set_vertical_aperture(15.2908)\nrgb_camera.set_clipping_range(0.1, 1000.0)\n\n# Enable ground truth annotations\nrgb_camera.add_ground_truth_to_frame(\n    "rgb",\n    "/Isaac/Isaac_Sim_4.0/Isaac/Archive/Isaac/Isaac/Sensors/RGB_Camera/rgb_publisher.isaac.sdf"\n)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"ground-truth-data-generation",children:"Ground Truth Data Generation"}),"\n",(0,t.jsx)(n.p,{children:"Generate high-quality training data with ground truth annotations:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Synthetic data collection script\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.replicator.core import handle_reset_done\nimport numpy as np\nimport cv2\nimport os\n\nclass SyntheticDataCollector:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.assets_root_path = get_assets_root_path()\n\n        # Set up scene\n        self.setup_scene()\n\n        # Create output directory\n        self.output_dir = "/tmp/isaac_synthetic_data"\n        os.makedirs(self.output_dir, exist_ok=True)\n        os.makedirs(f"{self.output_dir}/rgb", exist_ok=True)\n        os.makedirs(f"{self.output_dir}/depth", exist_ok=True)\n        os.makedirs(f"{self.output_dir}/segmentation", exist_ok=True)\n\n        self.frame_count = 0\n\n    def setup_scene(self):\n        # Add humanoid robot\n        humanoid_path = self.assets_root_path + "/Isaac/Robots/Humanoid/humanoid_instanceable.usd"\n        add_reference_to_stage(usd_path=humanoid_path, prim_path="/World/Humanoid")\n\n        # Add camera\n        self.camera = Camera(\n            prim_path="/World/Humanoid/Head/Camera",\n            frequency=30,\n            resolution=(640, 480)\n        )\n\n        # Enable various ground truth outputs\n        self.camera.add_ground_truth_to_frame("distance_to_image_plane", "/Isaac/Producers/IsaacComputeDistanceToImagePlane")\n        self.camera.add_ground_truth_to_frame("semantic_segmentation", "/Isaac/Producers/IsaacComputeSemanticSegmentation")\n        self.camera.add_ground_truth_to_frame("bounding_box_2d_tight", "/Isaac/Producers/IsaacComputeBoundingBox2DTight")\n\n    def collect_data(self, num_frames=1000):\n        self.world.reset()\n\n        for i in range(num_frames):\n            # Randomize environment for domain randomization\n            self.randomize_environment()\n\n            # Step simulation\n            self.world.step(render=True)\n\n            # Capture data\n            if i % 10 == 0:  # Save every 10 frames\n                self.save_frame_data()\n\n            self.frame_count += 1\n\n    def randomize_environment(self):\n        # Apply domain randomization\n        # Randomize lighting\n        dome_light = get_prim_at_path("/World/DomeLight")\n        if dome_light:\n            intensity = np.random.uniform(1000, 5000)\n            dome_light.GetAttribute("inputs:intensity").Set(intensity)\n\n    def save_frame_data(self):\n        # Get camera data\n        rgb_data = self.camera.get_rgb()\n        depth_data = self.camera.get_depth()\n        seg_data = self.camera.get_semantic_segmentation()\n\n        # Save RGB image\n        rgb_filename = f"{self.output_dir}/rgb/frame_{self.frame_count:06d}.png"\n        cv2.imwrite(rgb_filename, cv2.cvtColor(rgb_data, cv2.COLOR_RGB2BGR))\n\n        # Save depth image\n        depth_filename = f"{self.output_dir}/depth/frame_{self.frame_count:06d}.png"\n        cv2.imwrite(depth_filename, (depth_data * 1000).astype(np.uint16))  # Scale for 16-bit\n\n        # Save segmentation\n        seg_filename = f"{self.output_dir}/segmentation/frame_{self.frame_count:06d}.png"\n        cv2.imwrite(seg_filename, seg_data)\n\n        print(f"Saved frame {self.frame_count}")\n\n# Usage\ncollector = SyntheticDataCollector()\ncollector.collect_data(1000)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,t.jsx)(n.p,{children:"Domain randomization improves sim-to-real transfer by training models on diverse visual conditions:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Domain randomization implementation\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom pxr import Gf\nimport numpy as np\n\nclass DomainRandomizer:\n    def __init__(self, world):\n        self.world = world\n        self.randomization_config = {\n            "lighting": {\n                "intensity_range": (1000, 5000),\n                "color_range": (0.5, 1.0)\n            },\n            "materials": {\n                "roughness_range": (0.0, 1.0),\n                "metallic_range": (0.0, 1.0)\n            },\n            "textures": {\n                "color_jitter": 0.1,\n                "brightness_jitter": 0.2\n            }\n        }\n\n    def apply_randomization(self):\n        # Randomize lighting\n        self.randomize_lighting()\n\n        # Randomize materials\n        self.randomize_materials()\n\n        # Randomize textures\n        self.randomize_textures()\n\n    def randomize_lighting(self):\n        dome_light = get_prim_at_path("/World/DomeLight")\n        if dome_light:\n            intensity = np.random.uniform(\n                self.randomization_config["lighting"]["intensity_range"][0],\n                self.randomization_config["lighting"]["intensity_range"][1]\n            )\n            dome_light.GetAttribute("inputs:intensity").Set(intensity)\n\n            # Randomize color\n            color = Gf.Vec3f(\n                np.random.uniform(0.5, 1.0),\n                np.random.uniform(0.5, 1.0),\n                np.random.uniform(0.5, 1.0)\n            )\n            dome_light.GetAttribute("inputs:color").Set(color)\n\n    def randomize_materials(self):\n        # Example: Randomize materials of specific objects\n        # In practice, you would iterate through all materials in the scene\n        pass\n\n    def randomize_textures(self):\n        # Apply texture randomization\n        # This would involve modifying texture properties\n        pass\n'})}),"\n",(0,t.jsx)(n.h2,{id:"advanced-perception-training-capabilities",children:"Advanced Perception Training Capabilities"}),"\n",(0,t.jsx)(n.h3,{id:"sensor-simulation",children:"Sensor Simulation"}),"\n",(0,t.jsx)(n.p,{children:"Isaac Sim provides realistic sensor simulation for perception training:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Multi-sensor setup for humanoid robot\nfrom omni.isaac.sensor import Camera, LidarRtx\nfrom omni.isaac.core.sensors import ImuSensor\n\nclass HumanoidSensorSuite:\n    def __init__(self, robot_prim_path):\n        self.robot_prim_path = robot_prim_path\n        self.sensors = {}\n\n        # Set up head-mounted RGB-D camera\n        self.setup_head_camera()\n\n        # Set up torso-mounted LiDAR\n        self.setup_lidar()\n\n        # Set up IMU in torso\n        self.setup_imu()\n\n    def setup_head_camera(self):\n        camera_path = f"{self.robot_prim_path}/Head/Camera"\n        self.sensors["camera"] = Camera(\n            prim_path=camera_path,\n            frequency=30,\n            resolution=(640, 480)\n        )\n\n        # Enable ground truth for training\n        self.sensors["camera"].add_ground_truth_to_frame(\n            "distance_to_image_plane",\n            "/Isaac/Producers/IsaacComputeDistanceToImagePlane"\n        )\n        self.sensors["camera"].add_ground_truth_to_frame(\n            "semantic_segmentation",\n            "/Isaac/Producers/IsaacComputeSemanticSegmentation"\n        )\n\n    def setup_lidar(self):\n        lidar_path = f"{self.robot_prim_path}/Torso/Lidar"\n        self.sensors["lidar"] = LidarRtx(\n            prim_path=lidar_path,\n            points_per_second=500000,\n            rotation_frequency=20,\n            channels=16,\n            horizontal_resolution=2,\n            vertical_resolution=2,\n            upper_fov=15,\n            lower_fov=-15,\n            max_range=25.0,\n            min_range=0.1,\n            enable_semantics=True\n        )\n\n    def setup_imu(self):\n        imu_path = f"{self.robot_prim_path}/Torso/Imu"\n        self.sensors["imu"] = ImuSensor(\n            prim_path=imu_path,\n            name="torso_imu",\n            frequency=100\n        )\n'})}),"\n",(0,t.jsx)(n.h3,{id:"training-data-pipeline",children:"Training Data Pipeline"}),"\n",(0,t.jsx)(n.p,{children:"Create a complete pipeline for generating training data:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Complete training data generation pipeline\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nimport numpy as np\nimport json\nimport os\nfrom datetime import datetime\n\nclass IsaacSimTrainingPipeline:\n    def __init__(self, output_dir=None):\n        self.world = World(stage_units_in_meters=1.0)\n        self.assets_root_path = get_assets_root_path()\n\n        # Create output directory\n        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n        self.output_dir = output_dir or f"/tmp/isaac_training_data_{timestamp}"\n        os.makedirs(self.output_dir, exist_ok=True)\n        os.makedirs(f"{self.output_dir}/images", exist_ok=True)\n        os.makedirs(f"{self.output_dir}/labels", exist_ok=True)\n\n        # Initialize components\n        self.setup_scene()\n        self.domain_randomizer = DomainRandomizer(self.world)\n\n        self.frame_count = 0\n        self.annotations = []\n\n    def setup_scene(self):\n        # Add humanoid robot\n        humanoid_path = self.assets_root_path + "/Isaac/Robots/Humanoid/humanoid_instanceable.usd"\n        add_reference_to_stage(usd_path=humanoid_path, prim_path="/World/Humanoid")\n\n        # Add camera for data collection\n        self.camera = Camera(\n            prim_path="/World/Humanoid/Head/Camera",\n            frequency=30,\n            resolution=(640, 480)\n        )\n\n        # Enable all ground truth data\n        self.camera.add_ground_truth_to_frame("rgb", "/Isaac/Producers/IsaacReadRGBSensor")\n        self.camera.add_ground_truth_to_frame("distance_to_image_plane", "/Isaac/Producers/IsaacComputeDistanceToImagePlane")\n        self.camera.add_ground_truth_to_frame("semantic_segmentation", "/Isaac/Producers/IsaacComputeSemanticSegmentation")\n        self.camera.add_ground_truth_to_frame("bounding_box_2d_tight", "/Isaac/Producers/IsaacComputeBoundingBox2DTight")\n\n    def generate_training_episode(self, episode_length=1000):\n        self.world.reset()\n\n        episode_annotations = []\n\n        for step in range(episode_length):\n            # Apply domain randomization\n            self.domain_randomizer.apply_randomization()\n\n            # Step simulation\n            self.world.step(render=True)\n\n            # Collect data every N steps\n            if step % 5 == 0:  # Collect every 5 steps\n                annotation = self.collect_frame_data()\n                episode_annotations.append(annotation)\n\n        return episode_annotations\n\n    def collect_frame_data(self):\n        # Get sensor data\n        rgb_data = self.camera.get_rgb()\n        depth_data = self.camera.get_depth()\n        seg_data = self.camera.get_semantic_segmentation()\n        bboxes = self.camera.get_ground_truth("bounding_box_2d_tight")\n\n        # Create annotation\n        annotation = {\n            "frame_id": self.frame_count,\n            "timestamp": self.world.current_time_step_index,\n            "rgb_path": f"images/frame_{self.frame_count:06d}.png",\n            "depth_path": f"images/depth_{self.frame_count:06d}.png",\n            "seg_path": f"images/seg_{self.frame_count:06d}.png",\n            "bounding_boxes": bboxes,\n            "camera_pose": self.camera.get_world_pose()\n        }\n\n        # Save image data\n        import cv2\n        cv2.imwrite(f"{self.output_dir}/images/frame_{self.frame_count:06d}.png",\n                   cv2.cvtColor(rgb_data, cv2.COLOR_RGB2BGR))\n        cv2.imwrite(f"{self.output_dir}/images/depth_{self.frame_count:06d}.png",\n                   (depth_data * 1000).astype(np.uint16))\n        cv2.imwrite(f"{self.output_dir}/images/seg_{self.frame_count:06d}.png", seg_data)\n\n        self.frame_count += 1\n        return annotation\n\n    def generate_dataset(self, num_episodes=10, episode_length=1000):\n        all_annotations = []\n\n        for episode in range(num_episodes):\n            print(f"Generating episode {episode + 1}/{num_episodes}")\n            episode_annotations = self.generate_training_episode(episode_length)\n            all_annotations.extend(episode_annotations)\n\n        # Save dataset metadata\n        metadata = {\n            "dataset_name": "Isaac_Sim_Humanoid_Training_Data",\n            "creation_date": datetime.now().isoformat(),\n            "num_episodes": num_episodes,\n            "episode_length": episode_length,\n            "total_frames": len(all_annotations),\n            "annotations": all_annotations\n        }\n\n        with open(f"{self.output_dir}/dataset.json", "w") as f:\n            json.dump(metadata, f, indent=2)\n\n        print(f"Dataset saved to {self.output_dir}")\n        return all_annotations\n\n# Usage example\npipeline = IsaacSimTrainingPipeline()\nannotations = pipeline.generate_dataset(num_episodes=5, episode_length=500)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"isaac-sim-ros-integration",children:"Isaac Sim-ROS Integration"}),"\n",(0,t.jsx)(n.h3,{id:"setting-up-the-bridge",children:"Setting up the Bridge"}),"\n",(0,t.jsx)(n.p,{children:"Connect Isaac Sim to ROS 2 for perception and control:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Isaac Sim to ROS bridge setup\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nimport omni\nimport carb\n\n# Enable ROS bridge extension\nomni.kit.commands.execute("RosBridgeCreateNode", target="127.0.0.1:8888")\n\n# Set up robot with ROS bridge\ndef setup_ros_bridge_robot():\n    world = World(stage_units_in_meters=1.0)\n    assets_root_path = get_assets_root_path()\n\n    # Add humanoid robot\n    humanoid_path = assets_root_path + "/Isaac/Robots/Humanoid/humanoid_instanceable.usd"\n    add_reference_to_stage(usd_path=humanoid_path, prim_path="/World/Humanoid")\n\n    # Apply ROS bridge components to robot\n    # This would typically be done through USD composition or via Python API\n    # depending on your specific robot configuration\n\n    world.reset()\n    return world\n\n# Example of publishing sensor data to ROS\ndef publish_sensor_data_to_ros(world):\n    # This would involve creating ROS publishers and publishing\n    # sensor data from Isaac Sim to ROS topics\n    pass\n'})}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"gpu-resource-management",children:"GPU Resource Management"}),"\n",(0,t.jsx)(n.p,{children:"Optimize Isaac Sim for best performance:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Performance optimization settings\ndef configure_performance_settings():\n    # Set rendering quality\n    carb.settings.get_settings().set("/rtx/antialiasing/enable", True)\n    carb.settings.get_settings().set("/rtx/ambientOcclusion/enable", True)\n    carb.settings.get_settings().set("/rtx/dlss/enable", True)  # If available\n\n    # Physics settings\n    carb.settings.get_settings().set("/physics/iterations", 10)\n    carb.settings.get_settings().set("/physics/substeps", 1)\n\n    # Memory management\n    carb.settings.get_settings().set("/persistent/app/viewport/maxTextureMemory", 2048)\n    carb.settings.get_settings().set("/persistent/app/viewport/maxGeometryMemory", 1024)\n\n# Scene optimization for performance\ndef optimize_scene_for_performance():\n    # Reduce complexity where possible\n    # Use Level of Detail (LOD) where appropriate\n    # Optimize materials for faster rendering\n    pass\n'})}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,t.jsx)(n.h3,{id:"rendering-issues",children:"Rendering Issues"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Black screen"}),": Check GPU drivers and RTX support"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Low frame rate"}),": Reduce scene complexity or rendering quality"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Artifacts"}),": Verify material properties and lighting setup"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"physics-issues",children:"Physics Issues"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robot falling through ground"}),": Check collision geometry and mass properties"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Unstable simulation"}),": Adjust physics solver parameters"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Jittering joints"}),": Verify joint limits and dynamics parameters"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"sensor-issues",children:"Sensor Issues"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"No sensor data"}),": Verify sensor placement and configuration"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Incorrect data"}),": Check coordinate frame conventions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Low quality"}),": Adjust sensor parameters and rendering settings"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"This chapter introduced the fundamentals of NVIDIA Isaac Sim for humanoid robotics:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Installation and setup procedures for Isaac Sim"}),"\n",(0,t.jsx)(n.li,{children:"Photorealistic rendering and USD scene management"}),"\n",(0,t.jsx)(n.li,{children:"Synthetic data generation with ground truth annotations"}),"\n",(0,t.jsx)(n.li,{children:"Domain randomization for improved sim-to-real transfer"}),"\n",(0,t.jsx)(n.li,{children:"Advanced perception training capabilities"}),"\n",(0,t.jsx)(n.li,{children:"Isaac Sim-ROS integration patterns"}),"\n",(0,t.jsx)(n.li,{children:"Performance optimization strategies"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Isaac Sim provides a powerful platform for developing and training humanoid robots in photorealistic, physically accurate simulations. The combination of advanced rendering, physics simulation, and synthetic data generation capabilities makes it an essential tool for modern robotics development and research."})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453(e,n,i){i.d(n,{R:()=>s,x:()=>o});var a=i(6540);const t={},r=a.createContext(t);function s(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);