"use strict";(globalThis.webpackChunkphysical_humanoid_ai_book=globalThis.webpackChunkphysical_humanoid_ai_book||[]).push([[40],{1556(n,e,t){t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>r,metadata:()=>o,toc:()=>p});const o=JSON.parse('{"id":"module3-nvidia-isaac/chapter3-nav2-bipedal-planning","title":"Chapter 3: Nav2 and Path Planning for Bipedal Humanoid Movement","description":"Learning Objectives","source":"@site/docs/module3-nvidia-isaac/chapter3-nav2-bipedal-planning.md","sourceDirName":"module3-nvidia-isaac","slug":"/module3-nvidia-isaac/chapter3-nav2-bipedal-planning","permalink":"/docs/module3-nvidia-isaac/chapter3-nav2-bipedal-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/alizah-fatima/physical-humanoid-ai-book/tree/main/docs/module3-nvidia-isaac/chapter3-nav2-bipedal-planning.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Isaac ROS - Hardware-Accelerated Tools","permalink":"/docs/module3-nvidia-isaac/chapter2-isaac-ros-tools"},"next":{"title":"Chapter 1: Introduction to Vision-Language-Action Models","permalink":"/docs/module4-vla/chapter1-vla-intro"}}');var a=t(4848),i=t(8453);const r={sidebar_position:3},s="Chapter 3: Nav2 and Path Planning for Bipedal Humanoid Movement",l={},p=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Humanoid Navigation Challenges",id:"introduction-to-humanoid-navigation-challenges",level:2},{value:"Key Differences from Traditional Navigation",id:"key-differences-from-traditional-navigation",level:3},{value:"Nav2 Framework Overview",id:"nav2-framework-overview",level:3},{value:"Nav2 Adaptation for Humanoid Robots",id:"nav2-adaptation-for-humanoid-robots",level:2},{value:"Humanoid-Specific Configuration",id:"humanoid-specific-configuration",level:3},{value:"Humanoid-Specific Behavior Tree",id:"humanoid-specific-behavior-tree",level:3},{value:"Bipedal Path Planning Algorithms",id:"bipedal-path-planning-algorithms",level:2},{value:"Footstep Planning Fundamentals",id:"footstep-planning-fundamentals",level:3},{value:"Center of Mass Trajectory Planning",id:"center-of-mass-trajectory-planning",level:3},{value:"Advanced Path Planning for Humanoid Constraints",id:"advanced-path-planning-for-humanoid-constraints",level:3},{value:"Sim-to-Real Transfer Techniques",id:"sim-to-real-transfer-techniques",level:2},{value:"Domain Randomization for Navigation",id:"domain-randomization-for-navigation",level:3},{value:"System Identification for Real Robot Tuning",id:"system-identification-for-real-robot-tuning",level:3},{value:"Transfer Learning Approaches",id:"transfer-learning-approaches",level:3},{value:"Isaac Sim Integration for Humanoid Navigation",id:"isaac-sim-integration-for-humanoid-navigation",level:2},{value:"Creating Navigation Training Environments",id:"creating-navigation-training-environments",level:3},{value:"Validation and Testing Strategies",id:"validation-and-testing-strategies",level:2},{value:"Simulation Validation",id:"simulation-validation",level:3},{value:"Deployment and Real-World Testing",id:"deployment-and-real-world-testing",level:2},{value:"Real Robot Integration",id:"real-robot-integration",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Real-time Performance Considerations",id:"real-time-performance-considerations",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"Navigation Issues and Solutions",id:"navigation-issues-and-solutions",level:3},{value:"Summary",id:"summary",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"chapter-3-nav2-and-path-planning-for-bipedal-humanoid-movement",children:"Chapter 3: Nav2 and Path Planning for Bipedal Humanoid Movement"})}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Adapt Nav2 framework for bipedal humanoid navigation"}),"\n",(0,a.jsx)(e.li,{children:"Implement path planning algorithms considering humanoid constraints"}),"\n",(0,a.jsx)(e.li,{children:"Design footstep planning for stable bipedal movement"}),"\n",(0,a.jsx)(e.li,{children:"Execute sim-to-real transfer techniques for humanoid navigation"}),"\n",(0,a.jsx)(e.li,{children:"Optimize navigation for complex humanoid environments"}),"\n",(0,a.jsx)(e.li,{children:"Validate and test humanoid navigation systems"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"introduction-to-humanoid-navigation-challenges",children:"Introduction to Humanoid Navigation Challenges"}),"\n",(0,a.jsx)(e.p,{children:"Humanoid navigation presents unique challenges compared to wheeled or tracked robots due to the inherent complexity of bipedal locomotion. Unlike traditional mobile robots, humanoid robots must maintain balance while navigating, which introduces additional constraints and considerations."}),"\n",(0,a.jsx)(e.h3,{id:"key-differences-from-traditional-navigation",children:"Key Differences from Traditional Navigation"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Balance Requirements"}),": Humanoid robots must maintain center of mass within support polygon"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Step Constraints"}),": Limited step size, height, and placement capabilities"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Dynamic Stability"}),": Movement requires continuous balance control"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Gait Patterns"}),": Different walking patterns affect navigation planning"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Terrain Adaptation"}),": Must handle various surface types and obstacles"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"nav2-framework-overview",children:"Nav2 Framework Overview"}),"\n",(0,a.jsx)(e.p,{children:"Navigation2 (Nav2) is the standard navigation framework for ROS 2, but it requires adaptation for humanoid robots. The standard Nav2 stack includes:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Global Planner"}),": Path planning from start to goal"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Local Planner"}),": Local obstacle avoidance and trajectory following"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Controller"}),": Low-level control commands"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Costmap"}),": Environment representation with obstacles"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Recovery Behaviors"}),": Actions when navigation fails"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"For humanoid robots, each component needs to be adapted to consider bipedal constraints."}),"\n",(0,a.jsx)(e.h2,{id:"nav2-adaptation-for-humanoid-robots",children:"Nav2 Adaptation for Humanoid Robots"}),"\n",(0,a.jsx)(e.h3,{id:"humanoid-specific-configuration",children:"Humanoid-Specific Configuration"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-yaml",children:'# config/humanoid_nav2_params.yaml\namcl:\n  ros__parameters:\n    use_sim_time: True\n    alpha1: 0.2\n    alpha2: 0.2\n    alpha3: 0.2\n    alpha4: 0.2\n    alpha5: 0.2\n    base_frame_id: "base_link"\n    beam_skip_distance: 0.5\n    beam_skip_error_threshold: 0.9\n    beam_skip_threshold: 0.3\n    do_beamskip: false\n    global_frame_id: "map"\n    lambda_short: 0.1\n    laser_likelihood_max_dist: 2.0\n    laser_max_range: 100.0\n    laser_min_range: -1.0\n    laser_model_type: "likelihood_field"\n    max_beams: 60\n    max_particles: 2000\n    min_particles: 500\n    odom_frame_id: "odom"\n    pf_err: 0.05\n    pf_z: 0.5\n    recovery_alpha_fast: 0.0\n    recovery_alpha_slow: 0.0\n    resample_interval: 1\n    robot_model_type: "nav2_amcl::DifferentialMotionModel"\n    save_pose_rate: 0.5\n    sigma_hit: 0.2\n    tf_broadcast: true\n    transform_tolerance: 1.0\n    update_min_a: 0.2\n    update_min_d: 0.25\n    z_hit: 0.5\n    z_max: 0.05\n    z_rand: 0.5\n    z_short: 0.05\n\nbt_navigator:\n  ros__parameters:\n    use_sim_time: True\n    global_frame: map\n    robot_base_frame: base_link\n    odom_topic: /odom\n    bt_loop_duration: 10\n    default_server_timeout: 20\n    enable_groot_monitoring: True\n    groot_zmq_publisher_port: 1666\n    groot_zmq_server_port: 1667\n    default_nav_to_pose_bt_xml: "humanoid_nav_to_pose.xml"\n\ncontroller_server:\n  ros__parameters:\n    use_sim_time: True\n    controller_frequency: 20.0  # Lower frequency for humanoid stability\n    min_x_velocity_threshold: 0.001\n    min_y_velocity_threshold: 0.5\n    min_theta_velocity_threshold: 0.001\n    progress_checker_plugin: "progress_checker"\n    goal_checker_plugin: "goal_checker"\n    controller_plugins: ["HumanoidController"]\n\n    # Humanoid-specific controller\n    HumanoidController:\n      plugin: "humanoid_mppi_controller::HumanoidMPPIController"\n      time_steps: 50\n      control_horizon: 1.0\n      model_dt: 0.05\n      # Humanoid-specific parameters\n      vx_std: 0.2\n      vy_std: 0.1\n      wz_std: 0.3\n      iteration_count: 100\n      temperature: 0.3\n      track_target_heading: True\n      transform_tolerance: 0.3\n      # Larger tolerances for humanoid stability\n      xy_goal_tolerance: 0.3\n      yaw_goal_tolerance: 0.3\n      state_reset: True\n      publish_cost_grid_pc: False\n      progress_checker: "progress_checker"\n      goal_checker: "goal_checker"\n      # Humanoid-specific parameters\n      max_step_length: 0.3      # Maximum step length in meters\n      max_step_height: 0.1      # Maximum step height\n      step_duration: 0.8        # Time per step in seconds\n      balance_margin: 0.1       # Safety margin for balance\n\nlocal_costmap:\n  ros__parameters:\n    use_sim_time: True\n    update_frequency: 5.0\n    publish_frequency: 2.0\n    global_frame: odom\n    robot_base_frame: base_link\n    rolling_window: true\n    width: 8.0    # Larger window for humanoid planning\n    height: 8.0\n    resolution: 0.05  # Higher resolution for step planning\n    robot_radius: 0.4 # Larger radius for humanoid safety\n    plugins: ["voxel_layer", "inflation_layer"]\n    inflation_layer:\n      plugin: "nav2_costmap_2d::InflationLayer"\n      cost_scaling_factor: 3.0\n      inflation_radius: 0.8  # Larger inflation for humanoid safety\n    voxel_layer:\n      plugin: "nav2_costmap_2d::VoxelLayer"\n      enabled: True\n      publish_voxel_map: False\n      origin_z: 0.0\n      z_resolution: 0.2\n      z_voxels: 10\n      max_obstacle_height: 2.0\n      mark_threshold: 0\n      observation_sources: pointcloud laser_scan\n      pointcloud:\n        topic: /points\n        max_obstacle_height: 2.0\n        clearing: True\n        marking: True\n        data_type: "PointCloud2"\n        queue_size: 10\n        expected_update_rate: 0.0\n        observation_persistence: 0.0\n        max_obstacle_range: 5.0  # Humanoid-specific range\n        min_obstacle_range: 0.1\n      laser_scan:\n        topic: /scan\n        max_obstacle_range: 5.0  # Extended range for humanoid planning\n        clearing: True\n        marking: True\n        data_type: "LaserScan"\n        raytrace_range: 6.0\n        obstacle_range: 5.0\n\nglobal_costmap:\n  ros__parameters:\n    use_sim_time: True\n    global_frame: map\n    robot_base_frame: base_link\n    update_frequency: 1.0\n    publish_frequency: 1.0\n    static_map: true\n    rolling_window: false\n    width: 40.0    # Much larger for humanoid navigation\n    height: 40.0\n    resolution: 0.05 # Higher resolution for detailed planning\n    robot_radius: 0.4\n    plugins: ["static_layer", "obstacle_layer", "inflation_layer"]\n    obstacle_layer:\n      plugin: "nav2_costmap_2d::ObstacleLayer"\n      enabled: True\n      observation_sources: pointcloud laser_scan\n      pointcloud:\n        topic: /points\n        max_obstacle_height: 2.0\n        clearing: True\n        marking: True\n        data_type: "PointCloud2"\n        queue_size: 10\n        expected_update_rate: 0.0\n        observation_persistence: 0.0\n        max_obstacle_range: 5.0\n        min_obstacle_range: 0.1\n      laser_scan:\n        topic: /scan\n        max_obstacle_range: 5.0\n        clearing: True\n        marking: True\n        data_type: "LaserScan"\n        raytrace_range: 6.0\n        obstacle_range: 5.0\n    static_layer:\n      plugin: "nav2_costmap_2d::StaticLayer"\n      map_subscribe_transient_local: True\n    inflation_layer:\n      plugin: "nav2_costmap_2d::InflationLayer"\n      cost_scaling_factor: 3.0\n      inflation_radius: 0.8\n'})}),"\n",(0,a.jsx)(e.h3,{id:"humanoid-specific-behavior-tree",children:"Humanoid-Specific Behavior Tree"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'\x3c!-- humanoid_nav_to_pose.xml --\x3e\n<root main_tree_to_execute="MainTree">\n    <BehaviorTree ID="MainTree">\n        <PipelineSequence name="NavigateWithReplanning">\n            <RateController hz="1.0">\n                <RecoveryNode number_of_retries="6" name="ComputeAndFollowPath">\n                    <PipelineSequence name="ComputeAndFollowPath">\n                        <RecoveryNode number_of_retries="2" name="ComputePath">\n                            <RecoveryNode number_of_retries="2" name="SmoothPath">\n                                <ComputePathToPose goal="{goal}" path="{path}" planner_id="GridBased"/>\n                                <SmoothPath input_path="{path}" output_path="{smoothed_path}" smoother_id="SimpleSmoother"/>\n                            </RecoveryNode>\n                            <FollowPath path="{smoothed_path}" controller_id="HumanoidController"/>\n                        </RecoveryNode>\n                    </PipelineSequence>\n                    <RecoveryNode number_of_retries="2" name="HumanoidSpin">\n                        <Spin spin_dist="1.57"/>\n                    </RecoveryNode>\n                </RecoveryNode>\n            </RateController>\n            <ReactiveSequence>\n                <GoalUpdated/>\n                <ClearEntireCostmap name="ClearGlobalCostmap-Context" service_name="global_costmap/clear_entirely_global_costmap"/>\n            </ReactiveSequence>\n        </PipelineSequence>\n    </BehaviorTree>\n</root>\n'})}),"\n",(0,a.jsx)(e.h2,{id:"bipedal-path-planning-algorithms",children:"Bipedal Path Planning Algorithms"}),"\n",(0,a.jsx)(e.h3,{id:"footstep-planning-fundamentals",children:"Footstep Planning Fundamentals"}),"\n",(0,a.jsx)(e.p,{children:"Footstep planning is critical for humanoid navigation. Unlike wheeled robots that can move continuously, humanoid robots must plan each foot placement to maintain stability."}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped, Point\nfrom nav_msgs.msg import Path\nfrom visualization_msgs.msg import Marker, MarkerArray\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass FootstepPlanner(Node):\n    def __init__(self):\n        super().__init__(\'footstep_planner\')\n\n        # Create publishers for visualization\n        self.footstep_pub = self.create_publisher(MarkerArray, \'/footsteps\', 10)\n        self.path_pub = self.create_publisher(Path, \'/bipedal_path\', 10)\n\n        # Humanoid-specific parameters\n        self.step_length = 0.3  # meters\n        self.step_width = 0.2   # meters (distance between feet)\n        self.max_step_height = 0.1\n        self.step_duration = 0.8  # seconds per step\n        self.support_polygon_margin = 0.05  # safety margin\n\n    def plan_footsteps(self, global_path, start_pose):\n        """\n        Plan footstep sequence from global path\n        """\n        footsteps = []\n\n        # Start with current position\n        current_left_foot = self.calculate_initial_foot_position(start_pose, \'left\')\n        current_right_foot = self.calculate_initial_foot_position(start_pose, \'right\')\n\n        # Convert global path to footstep sequence\n        for i in range(len(global_path.poses) - 1):\n            # Calculate next step position based on path direction\n            current_pose = global_path.poses[i]\n            next_pose = global_path.poses[i + 1]\n\n            # Calculate step direction\n            dx = next_pose.pose.position.x - current_pose.pose.position.x\n            dy = next_pose.pose.position.y - current_pose.pose.position.y\n            step_direction = np.arctan2(dy, dx)\n\n            # Generate footsteps based on gait pattern\n            left_step, right_step = self.generate_step_pair(\n                current_left_foot, current_right_foot, step_direction\n            )\n\n            footsteps.append(left_step)\n            footsteps.append(right_step)\n\n            # Update current foot positions\n            current_left_foot = left_step\n            current_right_foot = right_step\n\n        return footsteps\n\n    def calculate_initial_foot_position(self, robot_pose, foot_type):\n        """\n        Calculate initial foot position based on robot pose\n        """\n        # For initial position, place feet in a stable stance\n        offset_x = 0.0\n        if foot_type == \'left\':\n            offset_y = self.step_width / 2.0\n        else:  # right\n            offset_y = -self.step_width / 2.0\n\n        # Apply offset in robot\'s coordinate frame\n        rotation = R.from_quat([\n            robot_pose.pose.orientation.x,\n            robot_pose.pose.orientation.y,\n            robot_pose.pose.orientation.z,\n            robot_pose.pose.orientation.w\n        ])\n\n        offset_local = np.array([offset_x, offset_y, 0.0])\n        offset_world = rotation.apply(offset_local)\n\n        foot_pose = PoseStamped()\n        foot_pose.header = robot_pose.header\n        foot_pose.pose.position.x = robot_pose.pose.position.x + offset_world[0]\n        foot_pose.pose.position.y = robot_pose.pose.position.y + offset_world[1]\n        foot_pose.pose.position.z = robot_pose.pose.position.z  # Ground level\n\n        return foot_pose\n\n    def generate_step_pair(self, left_foot, right_foot, direction):\n        """\n        Generate a pair of footsteps based on gait pattern\n        """\n        # Simple alternating gait: left, right, left, right...\n        # Calculate next step position in the direction of movement\n        step_distance = self.step_length\n\n        # Calculate new positions\n        dx = step_distance * np.cos(direction)\n        dy = step_distance * np.sin(direction)\n\n        # For alternating gait, each foot takes a step\n        new_left = PoseStamped()\n        new_left.header = left_foot.header\n        new_left.pose.position.x = left_foot.pose.position.x + dx\n        new_left.pose.position.y = left_foot.pose.position.y + dy\n        new_left.pose.position.z = left_foot.pose.position.z\n\n        new_right = PoseStamped()\n        new_right.header = right_foot.header\n        new_right.pose.position.x = right_foot.pose.position.x + dx\n        new_right.pose.position.y = right_foot.pose.position.y + dy\n        new_right.pose.position.z = right_foot.pose.position.z\n\n        # Add some offset to maintain balance\n        # In a real implementation, this would consider support polygon\n        return new_left, new_right\n\n    def validate_footstep(self, foot_pose, costmap):\n        """\n        Validate that a footstep is safe and stable\n        """\n        # Check if position is in collision-free area\n        # Check if step maintains balance\n        # Check if terrain is suitable for stepping\n        return True  # Simplified for example\n\n    def visualize_footsteps(self, footsteps):\n        """\n        Visualize footsteps in RViz\n        """\n        marker_array = MarkerArray()\n\n        for i, footstep in enumerate(footsteps):\n            marker = Marker()\n            marker.header = footstep.header\n            marker.ns = "footsteps"\n            marker.id = i\n            marker.type = Marker.CUBE\n            marker.action = Marker.ADD\n\n            marker.pose = footstep.pose\n            marker.scale.x = 0.15  # Foot size\n            marker.scale.y = 0.08\n            marker.scale.z = 0.01\n\n            if i % 2 == 0:\n                marker.color.r = 1.0  # Left foot in red\n                marker.color.g = 0.0\n                marker.color.b = 0.0\n            else:\n                marker.color.r = 0.0  # Right foot in blue\n                marker.color.g = 0.0\n                marker.color.b = 1.0\n            marker.color.a = 1.0\n\n            marker_array.markers.append(marker)\n\n        self.footstep_pub.publish(marker_array)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    footstep_planner = FootstepPlanner()\n\n    try:\n        rclpy.spin(footstep_planner)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        footstep_planner.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(e.h3,{id:"center-of-mass-trajectory-planning",children:"Center of Mass Trajectory Planning"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class CoMTrajectoryPlanner:\n    def __init__(self):\n        self.footstep_planner = FootstepPlanner()\n        self.zmp_reference = []  # Zero Moment Point reference trajectory\n        self.com_trajectory = []  # Center of Mass trajectory\n\n    def generate_com_trajectory(self, footsteps):\n        """\n        Generate Center of Mass trajectory from footstep plan\n        """\n        # Use inverted pendulum model for CoM planning\n        # The CoM trajectory should keep the Zero Moment Point (ZMP)\n        # within the support polygon defined by feet\n\n        com_trajectory = []\n\n        # For each footstep, calculate CoM position that maintains balance\n        for i in range(len(footsteps)):\n            # Calculate support polygon from current and next foot positions\n            support_polygon = self.calculate_support_polygon(footsteps, i)\n\n            # Plan CoM trajectory to stay within support polygon\n            com_point = self.plan_com_point(support_polygon, i)\n            com_trajectory.append(com_point)\n\n        return com_trajectory\n\n    def calculate_support_polygon(self, footsteps, step_index):\n        """\n        Calculate support polygon based on foot positions\n        """\n        # Support polygon is the convex hull of contact points\n        # For biped, it\'s the area between feet\n        if step_index < len(footsteps) - 1:\n            # Double support phase (both feet on ground)\n            left_foot = footsteps[step_index]\n            right_foot = footsteps[step_index + 1]\n\n            # Create polygon from foot positions\n            polygon = [\n                (left_foot.pose.position.x, left_foot.pose.position.y),\n                (right_foot.pose.position.x, right_foot.pose.position.y)\n            ]\n        else:\n            # Single support phase (one foot on ground)\n            support_foot = footsteps[step_index]\n            polygon = [(support_foot.pose.position.x, support_foot.pose.position.y)]\n\n        return polygon\n\n    def plan_com_point(self, support_polygon, step_index):\n        """\n        Plan CoM point that maintains stability\n        """\n        # Calculate CoM position that keeps ZMP within support polygon\n        # This is a simplified version - real implementation would use\n        # more sophisticated balance control algorithms\n\n        # For now, keep CoM roughly centered over support polygon\n        com_x = sum([p[0] for p in support_polygon]) / len(support_polygon)\n        com_y = sum([p[1] for p in support_polygon]) / len(support_polygon)\n        com_z = 0.8  # Typical CoM height for humanoid\n\n        return (com_x, com_y, com_z)\n\n    def generate_zmp_trajectory(self, com_trajectory, dt=0.01):\n        """\n        Generate ZMP trajectory from CoM trajectory\n        """\n        zmp_trajectory = []\n\n        for i in range(len(com_trajectory)):\n            com = com_trajectory[i]\n\n            # Calculate ZMP based on inverted pendulum model\n            # ZMP_x = CoM_x - (CoM_z - support_z) * CoM_ddot_x / gravity\n            # ZMP_y = CoM_y - (CoM_z - support_z) * CoM_ddot_y / gravity\n\n            # Simplified: ZMP close to CoM projection\n            zmp_x = com[0]\n            zmp_y = com[1]\n\n            zmp_trajectory.append((zmp_x, zmp_y))\n\n        return zmp_trajectory\n'})}),"\n",(0,a.jsx)(e.h3,{id:"advanced-path-planning-for-humanoid-constraints",children:"Advanced Path Planning for Humanoid Constraints"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class HumanoidPathPlanner:\n    def __init__(self):\n        self.step_constraints = {\n            \'max_step_length\': 0.3,\n            \'max_step_width\': 0.4,\n            \'max_step_height\': 0.1,\n            \'min_step_length\': 0.05,\n            \'foot_size\': (0.15, 0.08),  # length, width\n        }\n        self.balance_constraints = {\n            \'max_com_offset\': 0.1,\n            \'support_polygon_margin\': 0.05,\n        }\n\n    def plan_path_with_constraints(self, start, goal, costmap):\n        """\n        Plan path considering humanoid-specific constraints\n        """\n        # Use A* or Dijkstra with humanoid-specific cost function\n        # Consider step size limits, balance requirements, and terrain\n        path = self.humanoid_astar(start, goal, costmap)\n        return path\n\n    def humanoid_astar(self, start, goal, costmap):\n        """\n        A* algorithm adapted for humanoid navigation\n        """\n        import heapq\n\n        # Priority queue: (cost, position, g_score)\n        open_set = [(0, start, 0)]\n        closed_set = set()\n        came_from = {}\n        g_score = {start: 0}\n\n        while open_set:\n            current_cost, current, current_g = heapq.heappop(open_set)\n\n            if current == goal:\n                # Reconstruct path\n                path = [current]\n                while current in came_from:\n                    current = came_from[current]\n                    path.append(current)\n                return path[::-1]\n\n            closed_set.add(current)\n\n            # Get neighbors considering humanoid step constraints\n            neighbors = self.get_humanoid_neighbors(current, costmap)\n\n            for neighbor in neighbors:\n                if neighbor in closed_set:\n                    continue\n\n                # Calculate tentative g_score\n                tentative_g = current_g + self.calculate_step_cost(current, neighbor)\n\n                if neighbor not in [item[1] for item in open_set] or tentative_g < g_score.get(neighbor, float(\'inf\')):\n                    came_from[neighbor] = current\n                    g_score[neighbor] = tentative_g\n                    f_score = tentative_g + self.heuristic(neighbor, goal)\n\n                    heapq.heappush(open_set, (f_score, neighbor, tentative_g))\n\n        return []  # No path found\n\n    def get_humanoid_neighbors(self, position, costmap):\n        """\n        Get valid neighboring positions considering humanoid constraints\n        """\n        neighbors = []\n\n        # Generate possible step positions within constraints\n        for step_x in np.arange(-self.step_constraints[\'max_step_length\'],\n                                self.step_constraints[\'max_step_length\'], 0.1):\n            for step_y in np.arange(-self.step_constraints[\'max_step_width\']/2,\n                                    self.step_constraints[\'max_step_width\']/2, 0.1):\n                neighbor_x = position[0] + step_x\n                neighbor_y = position[1] + step_y\n\n                neighbor = (neighbor_x, neighbor_y)\n\n                # Check if step is valid\n                if self.is_valid_step(position, neighbor, costmap):\n                    neighbors.append(neighbor)\n\n        return neighbors\n\n    def is_valid_step(self, from_pos, to_pos, costmap):\n        """\n        Check if a step is valid considering all constraints\n        """\n        # Check step size constraints\n        step_dist = np.sqrt((to_pos[0] - from_pos[0])**2 + (to_pos[1] - from_pos[1])**2)\n        if step_dist > self.step_constraints[\'max_step_length\']:\n            return False\n\n        # Check collision with obstacles\n        if self.is_collision(to_pos, costmap):\n            return False\n\n        # Check terrain suitability\n        if not self.is_traversable_terrain(to_pos, costmap):\n            return False\n\n        return True\n\n    def calculate_step_cost(self, from_pos, to_pos):\n        """\n        Calculate cost of taking a step from one position to another\n        """\n        # Base cost is distance\n        distance = np.sqrt((to_pos[0] - from_pos[0])**2 + (to_pos[1] - from_pos[1])**2)\n\n        # Add penalties for difficult terrain\n        terrain_penalty = self.get_terrain_penalty(to_pos)\n\n        return distance + terrain_penalty\n\n    def heuristic(self, pos, goal):\n        """\n        Heuristic function for A* (Euclidean distance)\n        """\n        return np.sqrt((pos[0] - goal[0])**2 + (pos[1] - goal[1])**2)\n\n    def is_collision(self, position, costmap):\n        """\n        Check if position is in collision\n        """\n        # Check costmap for obstacle\n        costmap_value = self.get_costmap_value(position, costmap)\n        return costmap_value >= 50  # Threshold for obstacle\n\n    def is_traversable_terrain(self, position, costmap):\n        """\n        Check if terrain is suitable for humanoid stepping\n        """\n        # Check if terrain is flat enough, not too steep, etc.\n        return True  # Simplified\n\n    def get_terrain_penalty(self, position):\n        """\n        Get penalty for traversing terrain at position\n        """\n        # Higher penalty for rough terrain, stairs, etc.\n        return 0.0  # Simplified\n'})}),"\n",(0,a.jsx)(e.h2,{id:"sim-to-real-transfer-techniques",children:"Sim-to-Real Transfer Techniques"}),"\n",(0,a.jsx)(e.h3,{id:"domain-randomization-for-navigation",children:"Domain Randomization for Navigation"}),"\n",(0,a.jsx)(e.p,{children:"Domain randomization helps improve sim-to-real transfer by training navigation systems on diverse environments:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class DomainRandomizer:\n    def __init__(self):\n        self.randomization_params = {\n            'lighting': {\n                'intensity_range': (1000, 5000),\n                'color_temperature_range': (3000, 8000)\n            },\n            'textures': {\n                'roughness_range': (0.0, 1.0),\n                'metallic_range': (0.0, 1.0),\n                'normal_map_strength_range': (0.0, 1.0)\n            },\n            'materials': {\n                'friction_range': (0.1, 1.0),\n                'restitution_range': (0.0, 0.5)\n            },\n            'geometry': {\n                'scale_variation': 0.1,\n                'position_jitter': 0.05\n            }\n        }\n\n    def randomize_scene(self, scene_data):\n        \"\"\"\n        Apply domain randomization to scene\n        \"\"\"\n        randomized_scene = scene_data.copy()\n\n        # Randomize lighting\n        randomized_scene = self.randomize_lighting(randomized_scene)\n\n        # Randomize textures\n        randomized_scene = self.randomize_textures(randomized_scene)\n\n        # Randomize materials\n        randomized_scene = self.randomize_materials(randomized_scene)\n\n        # Randomize geometry\n        randomized_scene = self.randomize_geometry(randomized_scene)\n\n        return randomized_scene\n\n    def randomize_lighting(self, scene):\n        \"\"\"\n        Randomize lighting conditions\n        \"\"\"\n        intensity = np.random.uniform(\n            self.randomization_params['lighting']['intensity_range'][0],\n            self.randomization_params['lighting']['intensity_range'][1]\n        )\n\n        # Apply to all lights in scene\n        for light in scene['lights']:\n            light['intensity'] = intensity\n\n        return scene\n\n    def randomize_textures(self, scene):\n        \"\"\"\n        Randomize texture properties\n        \"\"\"\n        for obj in scene['objects']:\n            if 'texture' in obj:\n                obj['texture']['roughness'] = np.random.uniform(\n                    self.randomization_params['textures']['roughness_range'][0],\n                    self.randomization_params['textures']['roughness_range'][1]\n                )\n\n        return scene\n\n    def randomize_materials(self, scene):\n        \"\"\"\n        Randomize material properties\n        \"\"\"\n        for obj in scene['objects']:\n            if 'material' in obj:\n                obj['material']['friction'] = np.random.uniform(\n                    self.randomization_params['materials']['friction_range'][0],\n                    self.randomization_params['materials']['friction_range'][1]\n                )\n\n        return scene\n\n    def randomize_geometry(self, scene):\n        \"\"\"\n        Randomize geometric properties\n        \"\"\"\n        for obj in scene['objects']:\n            # Add small random variations to position and scale\n            pos_jitter = np.random.uniform(\n                -self.randomization_params['geometry']['position_jitter'],\n                self.randomization_params['geometry']['position_jitter'],\n                3\n            )\n            obj['position'] = np.array(obj['position']) + pos_jitter\n\n        return scene\n"})}),"\n",(0,a.jsx)(e.h3,{id:"system-identification-for-real-robot-tuning",children:"System Identification for Real Robot Tuning"}),"\n",(0,a.jsx)(e.p,{children:"System identification helps adapt simulation parameters to match real robot behavior:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class SystemIdentifier:\n    def __init__(self):\n        self.model_type = \'bipedal_dynamics\'\n        self.parameters = {\n            \'mass\': 50.0,  # kg\n            \'com_height\': 0.8,  # m\n            \'step_length_max\': 0.3,  # m\n            \'step_duration\': 0.8,  # s\n            \'balance_stiffness\': 100.0,\n            \'control_gain\': 1.0\n        }\n\n    def collect_system_data(self, robot, trajectory):\n        """\n        Collect input-output data for system identification\n        """\n        inputs = []\n        outputs = []\n\n        # Execute trajectory and collect data\n        for cmd in trajectory:\n            # Send command to robot\n            robot.send_command(cmd)\n\n            # Wait and measure response\n            robot.wait_for_response()\n\n            # Collect input (command) and output (actual pose, sensor data)\n            input_data = self.get_command_data(cmd)\n            output_data = self.get_sensor_data(robot)\n\n            inputs.append(input_data)\n            outputs.append(output_data)\n\n        return inputs, outputs\n\n    def estimate_parameters(self, inputs, outputs):\n        """\n        Estimate system parameters from input-output data\n        """\n        # Use least squares or other system identification method\n        # This is a simplified example\n        A = np.array(inputs)\n        B = np.array(outputs)\n\n        # Solve: params = (A^T * A)^(-1) * A^T * B\n        try:\n            estimated_params = np.linalg.lstsq(A, B, rcond=None)[0]\n            return estimated_params\n        except np.linalg.LinAlgError:\n            # Handle singular matrix case\n            return self.parameters  # Return default if estimation fails\n\n    def update_simulation_model(self, estimated_params):\n        """\n        Update simulation model with estimated parameters\n        """\n        # Update physics parameters in simulation\n        # This would involve modifying simulation configuration\n        updated_params = self.parameters.copy()\n\n        # Apply estimated parameters\n        for i, param_name in enumerate(self.parameters.keys()):\n            if i < len(estimated_params):\n                updated_params[param_name] = estimated_params[i]\n\n        return updated_params\n\n    def validate_model(self, simulation_model, real_robot_data):\n        """\n        Validate that simulation model matches real robot behavior\n        """\n        # Compare simulation output with real robot output\n        # for the same inputs\n        simulation_output = self.run_simulation(simulation_model, real_robot_data[\'inputs\'])\n        real_output = real_robot_data[\'outputs\']\n\n        # Calculate error metrics\n        error = np.mean(np.abs(simulation_output - real_output))\n\n        return error < 0.1  # Return True if error is acceptable\n\n    def run_simulation(self, model, inputs):\n        """\n        Run simulation with given model and inputs\n        """\n        # This would run the actual simulation\n        # Return simulation outputs\n        pass\n'})}),"\n",(0,a.jsx)(e.h3,{id:"transfer-learning-approaches",children:"Transfer Learning Approaches"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class TransferLearner:\n    def __init__(self):\n        self.simulation_policy = None\n        self.real_robot_policy = None\n        self.transfer_method = \'domain_adaptation\'\n\n    def pretrain_in_simulation(self, env, policy_network):\n        """\n        Pre-train navigation policy in simulation\n        """\n        import torch\n        import torch.nn as nn\n        import torch.optim as optim\n\n        # Train policy in simulation environment\n        optimizer = optim.Adam(policy_network.parameters(), lr=0.001)\n        criterion = nn.MSELoss()\n\n        for episode in range(1000):  # Training episodes\n            # Run episode in simulation\n            state = env.reset()\n            total_reward = 0\n\n            for step in range(100):  # Steps per episode\n                # Get action from policy\n                action = policy_network(torch.tensor(state, dtype=torch.float32))\n\n                # Execute action\n                next_state, reward, done, _ = env.step(action.detach().numpy())\n\n                # Update policy\n                loss = criterion(action, torch.tensor([0.5, 0.5]))  # Simplified\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                state = next_state\n                total_reward += reward\n\n                if done:\n                    break\n\n            if episode % 100 == 0:\n                print(f"Episode {episode}, Average Reward: {total_reward}")\n\n        return policy_network\n\n    def adapt_to_real_robot(self, pretrained_policy, real_data):\n        """\n        Adapt simulation-trained policy to real robot\n        """\n        # Fine-tune policy with real robot data\n        # This could involve domain adaptation techniques\n        adapted_policy = pretrained_policy\n\n        # Adjust for real robot dynamics\n        # Update network weights based on real data\n        return adapted_policy\n\n    def validate_transfer(self, adapted_policy, real_env):\n        """\n        Validate that transferred policy works on real robot\n        """\n        success_rate = 0\n        total_trials = 0\n\n        for trial in range(10):  # Validation trials\n            state = real_env.reset()\n            success = False\n\n            for step in range(200):  # Max steps per trial\n                action = adapted_policy(state)\n                next_state, reward, done, info = real_env.step(action)\n\n                if info.get(\'reached_goal\', False):\n                    success = True\n                    break\n                elif done:\n                    break\n\n                state = next_state\n\n            if success:\n                success_rate += 1\n            total_trials += 1\n\n        return success_rate / total_trials if total_trials > 0 else 0\n'})}),"\n",(0,a.jsx)(e.h2,{id:"isaac-sim-integration-for-humanoid-navigation",children:"Isaac Sim Integration for Humanoid Navigation"}),"\n",(0,a.jsx)(e.h3,{id:"creating-navigation-training-environments",children:"Creating Navigation Training Environments"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Isaac Sim script for creating navigation training environments\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.utils.prims import create_primitive\nfrom omni.isaac.core.materials import OmniPBR\nimport numpy as np\n\nclass HumanoidNavigationTrainer:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.assets_root_path = get_assets_root_path()\n        self.setup_environment()\n\n    def setup_environment(self):\n        """\n        Set up navigation training environment in Isaac Sim\n        """\n        # Add humanoid robot\n        humanoid_path = self.assets_root_path + "/Isaac/Robots/Humanoid/humanoid_instanceable.usd"\n        add_reference_to_stage(usd_path=humanoid_path, prim_path="/World/Humanoid")\n\n        # Create navigation course with obstacles\n        self.create_navigation_course()\n\n        # Add sensors for navigation\n        self.add_navigation_sensors()\n\n        # Enable physics\n        self.world.reset()\n\n    def create_navigation_course(self):\n        """\n        Create a navigation course with various obstacles\n        """\n        # Create ground plane\n        create_primitive(\n            prim_path="/World/ground",\n            primitive_type="Plane",\n            position=[0, 0, 0],\n            scale=[20, 20, 1],\n            orientation=[0.0, 0.0, 0.0, 1.0]\n        )\n\n        # Add obstacles\n        obstacles = [\n            {"type": "box", "pos": [2, 0, 0.5], "size": [0.5, 2, 1]},\n            {"type": "box", "pos": [-2, 3, 0.5], "size": [2, 0.5, 1]},\n            {"type": "cylinder", "pos": [0, -3, 1], "size": [0.5, 2]},\n        ]\n\n        for i, obs in enumerate(obstacles):\n            if obs["type"] == "box":\n                create_primitive(\n                    prim_path=f"/World/obstacle_{i}",\n                    primitive_type="Cube",\n                    position=obs["pos"],\n                    scale=obs["size"]\n                )\n            elif obs["type"] == "cylinder":\n                create_primitive(\n                    prim_path=f"/World/obstacle_{i}",\n                    primitive_type="Cylinder",\n                    position=obs["pos"],\n                    scale=[obs["size"][0], obs["size"][0], obs["size"][1]]\n                )\n\n    def add_navigation_sensors(self):\n        """\n        Add sensors needed for navigation\n        """\n        from omni.isaac.sensor import Camera, LidarRtx\n        from omni.isaac.core.sensors import ImuSensor\n\n        # Add RGB-D camera for perception\n        self.camera = Camera(\n            prim_path="/World/Humanoid/Head/Camera",\n            frequency=30,\n            resolution=(640, 480)\n        )\n\n        # Add LiDAR for obstacle detection\n        self.lidar = LidarRtx(\n            prim_path="/World/Humanoid/Torso/Lidar",\n            points_per_second=500000,\n            rotation_frequency=20,\n            channels=16,\n            horizontal_resolution=2,\n            vertical_resolution=2,\n            upper_fov=15,\n            lower_fov=-15,\n            max_range=25.0,\n            min_range=0.1\n        )\n\n        # Add IMU for balance feedback\n        self.imu = ImuSensor(\n            prim_path="/World/Humanoid/Torso/Imu",\n            name="torso_imu",\n            frequency=100\n        )\n\n    def run_navigation_training(self, num_episodes=1000):\n        """\n        Run navigation training episodes\n        """\n        for episode in range(num_episodes):\n            # Reset environment\n            self.world.reset()\n\n            # Apply randomization for domain randomization\n            self.randomize_environment()\n\n            # Run navigation episode\n            self.run_navigation_episode()\n\n            if episode % 100 == 0:\n                print(f"Completed episode {episode}")\n\n    def randomize_environment(self):\n        """\n        Apply domain randomization to environment\n        """\n        # Randomize lighting\n        from omni.isaac.core.utils.prims import get_prim_at_path\n        from pxr import Gf\n\n        dome_light = get_prim_at_path("/World/DomeLight")\n        if dome_light:\n            intensity = np.random.uniform(1000, 5000)\n            dome_light.GetAttribute("inputs:intensity").Set(intensity)\n\n    def run_navigation_episode(self):\n        """\n        Run a single navigation episode\n        """\n        # This would implement the actual navigation algorithm\n        # and collect training data\n        pass\n'})}),"\n",(0,a.jsx)(e.h2,{id:"validation-and-testing-strategies",children:"Validation and Testing Strategies"}),"\n",(0,a.jsx)(e.h3,{id:"simulation-validation",children:"Simulation Validation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class NavigationValidator:\n    def __init__(self, simulation_env, real_robot=None):\n        self.sim_env = simulation_env\n        self.real_robot = real_robot\n        self.metrics = {}\n\n    def validate_in_simulation(self):\n        \"\"\"\n        Validate navigation performance in simulation\n        \"\"\"\n        metrics = {\n            'success_rate': 0,\n            'path_efficiency': 0,\n            'collision_rate': 0,\n            'com_stability': 0,\n            'computation_time': 0\n        }\n\n        total_trials = 100\n        successful_trials = 0\n        total_path_length = 0\n        total_optimal_length = 0\n        collision_count = 0\n        stability_score = 0\n\n        for trial in range(total_trials):\n            # Run navigation trial\n            result = self.run_navigation_trial()\n\n            if result['success']:\n                successful_trials += 1\n                total_path_length += result['path_length']\n                total_optimal_length += result['optimal_length']\n\n            if result['collision']:\n                collision_count += 1\n\n            stability_score += result['stability_score']\n\n        metrics['success_rate'] = successful_trials / total_trials\n        metrics['path_efficiency'] = total_optimal_length / total_path_length if total_path_length > 0 else 0\n        metrics['collision_rate'] = collision_count / total_trials\n        metrics['com_stability'] = stability_score / total_trials\n\n        self.metrics['simulation'] = metrics\n        return metrics\n\n    def run_navigation_trial(self):\n        \"\"\"\n        Run a single navigation trial\n        \"\"\"\n        # Reset environment\n        self.sim_env.reset()\n\n        # Set random start and goal positions\n        start_pos = self.generate_random_position()\n        goal_pos = self.generate_random_goal_position(start_pos)\n\n        # Run navigation\n        path, success, collision = self.execute_navigation(start_pos, goal_pos)\n\n        # Calculate metrics\n        optimal_length = self.calculate_optimal_distance(start_pos, goal_pos)\n        path_length = self.calculate_path_length(path)\n        stability_score = self.calculate_balance_stability(path)\n\n        return {\n            'success': success,\n            'collision': collision,\n            'path_length': path_length,\n            'optimal_length': optimal_length,\n            'stability_score': stability_score\n        }\n\n    def validate_sim_to_real_transfer(self):\n        \"\"\"\n        Validate sim-to-real transfer performance\n        \"\"\"\n        if not self.real_robot:\n            print(\"No real robot available for validation\")\n            return None\n\n        # Compare simulation and real robot performance\n        sim_metrics = self.metrics.get('simulation', {})\n        real_metrics = self.validate_on_real_robot()\n\n        transfer_gap = {}\n        for metric in sim_metrics:\n            if metric in real_metrics:\n                gap = abs(sim_metrics[metric] - real_metrics[metric])\n                transfer_gap[metric] = gap\n\n        self.metrics['transfer_gap'] = transfer_gap\n        return transfer_gap\n\n    def validate_on_real_robot(self):\n        \"\"\"\n        Validate navigation on real robot\n        \"\"\"\n        # This would run actual tests on the real robot\n        # Collecting the same metrics as in simulation\n        pass\n\n    def generate_random_position(self):\n        \"\"\"\n        Generate random starting position\n        \"\"\"\n        x = np.random.uniform(-10, 10)\n        y = np.random.uniform(-10, 10)\n        return (x, y)\n\n    def generate_random_goal_position(self, start_pos):\n        \"\"\"\n        Generate random goal position different from start\n        \"\"\"\n        while True:\n            goal_pos = self.generate_random_position()\n            distance = np.sqrt((goal_pos[0] - start_pos[0])**2 + (goal_pos[1] - start_pos[1])**2)\n            if distance > 2.0:  # Minimum distance from start\n                return goal_pos\n"})}),"\n",(0,a.jsx)(e.h2,{id:"deployment-and-real-world-testing",children:"Deployment and Real-World Testing"}),"\n",(0,a.jsx)(e.h3,{id:"real-robot-integration",children:"Real Robot Integration"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class RealRobotDeployer:\n    def __init__(self, robot_interface):\n        self.robot = robot_interface\n        self.nav_system = None\n        self.safety_monitor = SafetyMonitor()\n\n    def deploy_navigation_system(self, config_file):\n        """\n        Deploy navigation system to real robot\n        """\n        # Load configuration\n        config = self.load_config(config_file)\n\n        # Initialize navigation stack\n        self.nav_system = self.initialize_navigation_stack(config)\n\n        # Set up safety systems\n        self.safety_monitor.start_monitoring()\n\n        print("Navigation system deployed successfully")\n\n    def load_config(self, config_file):\n        """\n        Load navigation configuration\n        """\n        import yaml\n        with open(config_file, \'r\') as f:\n            return yaml.safe_load(f)\n\n    def initialize_navigation_stack(self, config):\n        """\n        Initialize navigation stack with proper parameters\n        """\n        # Initialize Nav2 with humanoid-specific parameters\n        # Set up proper TF frames\n        # Configure sensors\n        pass\n\n    def run_safety_checks(self):\n        """\n        Run pre-navigation safety checks\n        """\n        checks = [\n            self.check_battery_level(),\n            self.check_sensor_health(),\n            self.verify_calibration(),\n            self.validate_environment(),\n        ]\n\n        return all(checks)\n\n    def check_battery_level(self):\n        """\n        Check robot battery level\n        """\n        battery_level = self.robot.get_battery_level()\n        return battery_level > 0.2  # At least 20% battery\n\n    def check_sensor_health(self):\n        """\n        Check if all navigation sensors are healthy\n        """\n        sensors = [\'lidar\', \'camera\', \'imu\', \'joint_encoders\']\n        for sensor in sensors:\n            if not self.robot.is_sensor_healthy(sensor):\n                return False\n        return True\n\n    def verify_calibration(self):\n        """\n        Verify sensor and robot calibration\n        """\n        # Check if IMU is calibrated\n        # Check if cameras are calibrated\n        # Check if joint encoders are accurate\n        pass\n\n    def navigate_with_monitoring(self, goal):\n        """\n        Navigate to goal with continuous safety monitoring\n        """\n        if not self.run_safety_checks():\n            print("Safety checks failed, aborting navigation")\n            return False\n\n        try:\n            # Start navigation\n            self.nav_system.navigate_to_goal(goal)\n\n            # Monitor during navigation\n            while not self.nav_system.is_goal_reached():\n                if self.safety_monitor.is_unsafe():\n                    self.emergency_stop()\n                    return False\n\n                # Continue monitoring\n                self.safety_monitor.update()\n\n            return True\n\n        except Exception as e:\n            print(f"Navigation error: {e}")\n            self.emergency_stop()\n            return False\n\n    def emergency_stop(self):\n        """\n        Emergency stop for safety\n        """\n        self.robot.stop_motion()\n        self.nav_system.cancel_all_goals()\n        print("Emergency stop executed")\n\nclass SafetyMonitor:\n    def __init__(self):\n        self.unsafe_conditions = []\n        self.monitoring_active = False\n\n    def start_monitoring(self):\n        """\n        Start safety monitoring\n        """\n        self.monitoring_active = True\n        # Start monitoring threads/processes\n        pass\n\n    def is_unsafe(self):\n        """\n        Check if current state is unsafe\n        """\n        # Check for various unsafe conditions\n        conditions = [\n            self.is_balance_unstable(),\n            self.is_collision_imminent(),\n            self.is_motor_overheating(),\n            self.is_battery_critical(),\n        ]\n\n        return any(conditions)\n\n    def is_balance_unstable(self):\n        """\n        Check if robot balance is unstable\n        """\n        # Check IMU data for balance issues\n        # Check CoM position relative to support polygon\n        pass\n\n    def is_collision_imminent(self):\n        """\n        Check if collision is imminent\n        """\n        # Check proximity sensors\n        # Check planned path for obstacles\n        pass\n\n    def update(self):\n        """\n        Update safety monitoring\n        """\n        if self.monitoring_active:\n            # Update all safety checks\n            pass\n'})}),"\n",(0,a.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsx)(e.h3,{id:"real-time-performance-considerations",children:"Real-time Performance Considerations"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class PerformanceOptimizer:\n    def __init__(self):\n        self.profiling_enabled = True\n        self.performance_targets = {\n            \'path_planning_rate\': 1.0,      # Hz\n            \'control_rate\': 100.0,          # Hz\n            \'sensor_processing_rate\': 30.0, # Hz\n            \'max_cpu_usage\': 80.0,          # Percentage\n            \'max_memory_usage\': 80.0,       # Percentage\n        }\n\n    def optimize_for_real_time(self, node):\n        """\n        Optimize navigation node for real-time performance\n        """\n        # Use real-time scheduling\n        import os\n        import sched\n        import time\n\n        # Set appropriate QoS profiles\n        # Use dedicated threads for different components\n        # Optimize data structures for speed\n        pass\n\n    def profile_performance(self, component_name, func, *args, **kwargs):\n        """\n        Profile performance of a component\n        """\n        import time\n        start_time = time.perf_counter()\n\n        result = func(*args, **kwargs)\n\n        end_time = time.perf_counter()\n        execution_time = end_time - start_time\n\n        print(f"{component_name} execution time: {execution_time:.4f}s")\n\n        return result, execution_time\n\n    def optimize_footstep_planning(self):\n        """\n        Optimize footstep planning for real-time performance\n        """\n        # Use lookup tables for common calculations\n        # Pre-compute stable footstep positions\n        # Use efficient data structures\n        pass\n'})}),"\n",(0,a.jsx)(e.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,a.jsx)(e.h3,{id:"navigation-issues-and-solutions",children:"Navigation Issues and Solutions"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Balance Instability"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Check IMU calibration\nros2 run imu_tools imu_calib\n\n# Verify CoM estimation\nros2 topic echo /humanoid/com_state\n"})}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Path Planning Failures"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Check costmap configuration\nros2 param list | grep costmap\n\n# Visualize costmap in RViz\nros2 run rviz2 rviz2\n"})}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Footstep Planning Issues"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Check footstep planner status\nros2 service call /footstep_planner/get_state lifecycle_msgs/srv/GetState\n\n# Verify robot kinematics\nros2 run tf2_tools view_frames\n"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"This chapter covered Nav2 and path planning for bipedal humanoid movement:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Nav2 framework adaptation for humanoid robots with specific configurations"}),"\n",(0,a.jsx)(e.li,{children:"Bipedal path planning algorithms including footstep planning"}),"\n",(0,a.jsx)(e.li,{children:"Center of Mass trajectory planning for balance maintenance"}),"\n",(0,a.jsx)(e.li,{children:"Sim-to-real transfer techniques using domain randomization and system identification"}),"\n",(0,a.jsx)(e.li,{children:"Isaac Sim integration for navigation training"}),"\n",(0,a.jsx)(e.li,{children:"Validation and testing strategies for both simulation and real robots"}),"\n",(0,a.jsx)(e.li,{children:"Deployment considerations and safety monitoring"}),"\n",(0,a.jsx)(e.li,{children:"Performance optimization for real-time operation"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"The key to successful humanoid navigation lies in properly accounting for bipedal constraints, maintaining balance during movement, and ensuring smooth sim-to-real transfer. By adapting traditional navigation approaches to consider humanoid-specific requirements, robots can navigate complex environments while maintaining stability and safety."})]})}function d(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(c,{...n})}):c(n)}},8453(n,e,t){t.d(e,{R:()=>r,x:()=>s});var o=t(6540);const a={},i=o.createContext(a);function r(n){const e=o.useContext(i);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:r(n.components),o.createElement(i.Provider,{value:e},n.children)}}}]);